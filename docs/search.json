[
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#s.1-set-the-working-directory-and-create-a-folder-for-data.",
    "href": "notebooks/EC_GLM/EC_GLM.html#s.1-set-the-working-directory-and-create-a-folder-for-data.",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "S.1 Set the working directory and create a folder for data.",
    "text": "S.1 Set the working directory and create a folder for data.\nSave the Quarto Markdown file (.QMD) to a folder of your choice, and then set the path to your folder as your working directory.\n\n# Set the workspace to the current working directory\n# Uncomment and replace the path below with your own working directory if needed:\n# setwd(\"/Users/zhaoxiang/Documents/tmp/EC_GLM_notebook\") \n\nworkspace &lt;- getwd()  # Get the current working directory and store it in 'workspace'\n\n# Increase the plot size by adjusting the options for plot dimensions in the notebook output\noptions(repr.plot.width = 16, repr.plot.height = 8)  # Sets width to 16 and height to 8 for larger plots\n\nIdeally, you would use the renv package to create an isolated environment for installing all the required R packages used in this notebook. However, since installing renv and its dependencies can be time-consuming, we recommend trying this after the workshop.\n\n# # Ensure \"renv\" package is installed\n# if (!requireNamespace(\"renv\", quietly = TRUE)) {\n#   install.packages(\"renv\")\n# }\n# \n# # Check if renv has been initialized in the project\n# if (!file.exists(\"renv/activate.R\")) {\n#   message(\"renv has not been initiated in this project. Initializing now...\")\n#   renv::init()  # Initialize renv if not already set up\n# } else {\n#   source(\"renv/activate.R\")  # Activate the renv environment\n#   message(\"renv is activated.\")\n# }\n# \n# # Check for the existence of renv.lock and restore the environment\n# if (file.exists(\"renv.lock\")) {\n#   message(\"Restoring renv environment from renv.lock...\")\n#   renv::restore()\n# } else {\n#   message(\"No renv.lock file found in the current directory. Skipping restore.\")\n# }"
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#s.2-install-and-load-essential-libraries.",
    "href": "notebooks/EC_GLM/EC_GLM.html#s.2-install-and-load-essential-libraries.",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "S.2 Install and load essential libraries.",
    "text": "S.2 Install and load essential libraries.\nInstall and load R packages.\n\n# Set CRAN mirror\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\n# List of packages to check, install if needed, and load\npackages &lt;- c(\"dplyr\", \"terra\", \"sf\", \"googledrive\", \"ggplot2\", \"corrplot\", \"pROC\", \"dismo\", \"spatstat.geom\", \"patchwork\", \"biomod2\", \"leaflet\", \"car\", \"gridExtra\", \"htmltools\", \"RColorBrewer\")\n\n# Function to display a cat message\ncat_message &lt;- function(pkg, message_type) {\n  if (message_type == \"installed\") {\n    cat(paste0(pkg, \" has been installed successfully!\\n\"))\n  } else if (message_type == \"loading\") {\n    cat(paste0(pkg, \" is already installed and has been loaded!\\n\"))\n  }\n}\n\n# Install missing packages and load them\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n    cat_message(pkg, \"installed\")\n  } else {\n    cat_message(pkg, \"loading\")\n  }\n  library(pkg, character.only = TRUE)\n}\n\ndplyr is already installed and has been loaded!\n\n\nterra is already installed and has been loaded!\n\n\nsf is already installed and has been loaded!\n\n\ngoogledrive is already installed and has been loaded!\nggplot2 is already installed and has been loaded!\ncorrplot is already installed and has been loaded!\n\n\npROC is already installed and has been loaded!\n\n\ndismo is already installed and has been loaded!\n\n\nspatstat.geom is already installed and has been loaded!\n\n\npatchwork is already installed and has been loaded!\n\n\nbiomod2 is already installed and has been loaded!\n\n\nleaflet is already installed and has been loaded!\ncar is already installed and has been loaded!\n\n\ngridExtra is already installed and has been loaded!\n\n\nhtmltools is already installed and has been loaded!\nRColorBrewer is already installed and has been loaded!\n\n# If you are using renv, you can snapshot the renv after loading all the packages.\n\n#renv::snapshot()"
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#s.3-download-case-study-datasets",
    "href": "notebooks/EC_GLM/EC_GLM.html#s.3-download-case-study-datasets",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "S.3 Download case study datasets",
    "text": "S.3 Download case study datasets\nWe have prepared the following data and uploaded them to our Google Drive for your use:\n\nSpecies occurrence data: Shapefile format (.shp)\nEnvironmental variables: Stacked Raster format (.tif)\nStudy area boundary: Shapefile format (.shp)\n\n\n# De-authenticate Google Drive to access public files\ndrive_deauth()\n\n# Define Google Drive file ID and the path for downloading\nzip_file_id &lt;- \"1twKlNokB7t33QH2KesH7BHYffp5kvoiP\" # Replace with the actual file ID of the zipped file\ndatafolder_path &lt;- file.path(workspace)\n\n# Create a local path for the zipped file\nzip_file_path &lt;- file.path(datafolder_path, \"mountain_ash_centralhighlands_data.zip\")\n\n# Function to download a file with progress messages\ndownload_zip_file &lt;- function(file_id, file_path) {\n  cat(\"Downloading zipped file...\\n\")\n  drive_download(as_id(file_id), path = file_path, overwrite = TRUE)\n  cat(\"Downloaded zipped file to:\", file_path, \"\\n\")\n}\n\n# Create local directory if it doesn't exist\nif (!dir.exists(datafolder_path)) {\n  dir.create(datafolder_path, recursive = TRUE)\n}\n\n# Download the zipped file\ncat(\"Starting to download the zipped file...\\n\")\n\nStarting to download the zipped file...\n\ndownload_zip_file(zip_file_id, zip_file_path)\n\nDownloading zipped file...\n\n\nFile downloaded:\n\n\n• 'mountain_ash_centralhighlands-20241204T022054Z-001.zip'\n  &lt;id: 1twKlNokB7t33QH2KesH7BHYffp5kvoiP&gt;\n\n\nSaved locally as:\n\n\n• '/Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/EC_GLM/mountain_ash_centralhighlands_data.zip'\n\n\nDownloaded zipped file to: /Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/EC_GLM/mountain_ash_centralhighlands_data.zip \n\n# Unzip the downloaded file\ncat(\"Unzipping the file...\\n\")\n\nUnzipping the file...\n\nunzip(zip_file_path, exdir = datafolder_path)\ncat(\"Unzipped files to folder:\", datafolder_path, \"\\n\")\n\nUnzipped files to folder: /Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/EC_GLM"
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#taxon-location-data-and-scale",
    "href": "notebooks/EC_GLM/EC_GLM.html#taxon-location-data-and-scale",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "1.1 Taxon, location, data and scale",
    "text": "1.1 Taxon, location, data and scale\nTaxon: Mountain Ash (Eucalyptus regnans)\n\nPhotographer: Reiner Richter, ALA\nMountain Ash (Eucalyptus regnans), is a remarkably tall and straight tree native to Victoria and Tasmania. This species thrives in cool, temperate rainforests characterized by high rainfall, deep, well-drained soils, mild temperatures, and high humidity. It is typically found at altitudes ranging from 200 to 1,000 meters above sea level (Burns et al., 2015).\nThe Mountain Ash faces two main forms of disturbance: bushfires, which are its primary natural disturbance, and logging, which represents the primary human-induced threat to its habitat (Burns et al., 2015; Nevill et al., 2010).\nLocation: the Central Highlands (study area) in the south part of Victoria\nSpatial and temporal scales: small (spatial) and static (temporal)\n\n# Load your shapefiles\ncentral_highlands &lt;- st_read(\"mountain_ash_centralhighlands/central_highlands.shp\")\n\nReading layer `central_highlands' from data source \n  `/Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/EC_GLM/mountain_ash_centralhighlands/central_highlands.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 144.9398 ymin: -38.20964 xmax: 146.4563 ymax: -36.97746\nGeodetic CRS:  GDA94\n\n# Custom CSS for a smaller legend box\ncustom_css &lt;- tags$style(HTML(\"\n  .leaflet-control .legend {\n    font-size: 8px !important; /* Reduce font size */\n    padding: 4px; /* Reduce padding inside the legend box */\n    line-height: 1; /* Reduce spacing between lines */\n    width: auto; /* Automatically size the legend box */\n    height: auto; /* Automatically size the legend box */\n  }\n  .leaflet-control .legend i {\n    width: 10px; /* Smaller legend icons */\n    height: 10px;\n  }\n\"))\n\n# Render the map\nleaflet() %&gt;%\n  addProviderTiles(providers$Esri.WorldImagery) %&gt;%\n  # Add the Central Highlands layer with a distinct color\n  addPolygons(\n    data = central_highlands,\n    color = \"lightblue\",         # Border color of Central Highlands polygon\n    weight = 1,                  # Border width\n    fillColor = \"lightblue\",     # Fill color of Central Highlands\n    fillOpacity = 0.3,           # Transparency for fill\n    group = \"Central Highlands\"\n  ) %&gt;%\n  setView(lng = 145.7, lat = -37.5, zoom = 7) %&gt;% # Set the view to desired location\n  addLegend(\n    position = \"bottomright\",\n    colors = c(\"lightblue\"),\n    labels = c(\"Central Highlands\"),\n    opacity = 0.7\n  ) %&gt;%\n  # Add the custom CSS to modify the legend font size\n  htmlwidgets::prependContent(custom_css)"
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#model-objective",
    "href": "notebooks/EC_GLM/EC_GLM.html#model-objective",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "1.2 Model objective",
    "text": "1.2 Model objective\nExplanation: To conduct detailed analyses of species–environment relationships and test specific hypotheses about the main factors driving species distributions.\nMapping/interpolating: To use the estimated species-environment relationships to map the distribution of the targeted species in the same geographic area.\nPrediction in new area: To forecast or project the estimated species–environment relationships to a different geographic area. (Exercise, data provided)"
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#biodiversity-data",
    "href": "notebooks/EC_GLM/EC_GLM.html#biodiversity-data",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "2.1 Biodiversity data",
    "text": "2.1 Biodiversity data\nUnderstanding your species is essential. This includes knowing their common names (which may include multiple names) and scientific name to ensure you collect the most comprehensive records available in open-access biodiversity data portals, such as the Atlas of Living Australia (ALA) or the Global Biodiversity Information Facility (GBIF).\nFor this exercise, we have prepared a species occurrence data file in CSV format, which was downloaded from ALA. To make it accessible, we have stored this file in the EcoCommons Public Google Drive for you to download and use conveniently.\n\n# Read the shapefile for mountain ash occurrence point dataset\nmountain_ash_centralhighlands &lt;- st_read(\"mountain_ash_centralhighlands/mountain_ash_centralhighlands.shp\")\n\nReading layer `mountain_ash_centralhighlands' from data source \n  `/Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/EC_GLM/mountain_ash_centralhighlands/mountain_ash_centralhighlands.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3933 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 145.0258 ymin: -38.2 xmax: 146.4333 ymax: -37.22625\nGeodetic CRS:  GDA94\n\n# Filter the data to include only PRESENT points\nmountain_ash_present &lt;- mountain_ash_centralhighlands %&gt;%\n  dplyr::filter(occrrnS == \"1\")\n\nleaflet() %&gt;%\n  addProviderTiles(providers$Esri.WorldImagery) %&gt;%\n  # Add the Central Highlands layer with a distinct color\n  addPolygons(\n    data = central_highlands,\n    color = \"lightblue\",         # Border color of Central Highlands polygon\n    weight = 1,                  # Border width\n    fillColor = \"lightblue\",     # Fill color of Central Highlands\n    fillOpacity = 0.3,           # Transparency for fill\n    group = \"Central Highlands\"\n  ) %&gt;%\n  # Add Mountain Ash presence points\n  addCircleMarkers(\n    data = mountain_ash_present,\n    color = \"#11aa96\",\n    radius = 1,\n    weight = 0.5,\n    opacity = 1,\n    fillOpacity = 1,\n    group = \"Mountain Ash Presence Records\"\n  ) %&gt;%\n  setView(lng = 145.7, lat = -37.5, zoom = 7) %&gt;% # Adjust longitude, latitude, and zoom as needed\n  # Add layer controls for easier toggling\n  addLayersControl(\n    overlayGroups = c(\"Central Highlands\", \"Mountain Ash Presence Records\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) %&gt;%\n  # Add a compact legend with a clean style\n  addControl(\n    html = \"\n    &lt;div style='background-color: white; padding: 4px; font-size: 8px; border: none; box-shadow: none;'&gt;\n      &lt;div style='display: flex; align-items: center; margin-bottom: 3px;'&gt;\n        &lt;div style='background: lightblue; width: 10px; height: 10px; margin-right: 5px; opacity: 0.7;'&gt;&lt;/div&gt;\n        Central Highlands\n      &lt;/div&gt;\n      &lt;div style='display: flex; align-items: center;'&gt;\n        &lt;div style='background: #11aa96; width: 10px; height: 10px; margin-right: 5px;'&gt;&lt;/div&gt;\n        Mountain Ash Presence Records\n      &lt;/div&gt;\n    &lt;/div&gt;\n    \",\n    position = \"bottomright\"\n  )"
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#pseudo-absence-data",
    "href": "notebooks/EC_GLM/EC_GLM.html#pseudo-absence-data",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "2.2 Pseudo Absence Data",
    "text": "2.2 Pseudo Absence Data\nSpecies distribution models typically require both presence and absence data to predict the distribution of a species. However, true absence data—locations where the species is confirmed not to occur—are often unavailable for various reasons. In such cases, pseudo-absence data are used to fill this gap.\nWe generated pseudo-absence data using the EcoCommons platform with the following configuration:\n\nDispersal Kernel\n\nKey point: Mountain Ash seed dispersal is likely within 150 meters, based on von Takach Dukai (2019).\nImplication: This distance should be factored into pseudo-absence generation to avoid selecting pseudo-absence points too close to presence points, which could bias the model by including areas that the species might still occupy but haven’t been sampled.\n\nAbsence-Presence Ratio\n\nKey point: The ratio of absence to presence is set to 1.\nImplication: For each presence point, one pseudo-absence point should be generated. This balanced ratio ensures the model isn’t skewed by an overabundance of either class and aids in robust statistical comparisons.\n\nPseudo-Absence Strategy\n\nKey point: Disk strategy with a range of 1000 - 5000 meters.\nImplication:\n\nThe disk strategy selects pseudo-absences outside a certain buffer zone from presence points.\nThe range of 1000–5000 meters should be carefully reviewed since it must balance avoiding areas within the species’ potential dispersal kernel (150 meters) and including areas beyond the likely range of colonization.\n\n\n\n\n# Filter the data to include only ABSENT points\nmountain_ash_absent &lt;- mountain_ash_centralhighlands %&gt;%\n  dplyr::filter(occrrnS == \"0\")\n\nleaflet() %&gt;%\n  addProviderTiles(providers$Esri.WorldImagery) %&gt;%\n # Add the Central Highlands layer with a distinct color\n  addPolygons(data = central_highlands,\n              color = \"lightblue\",         # Border color of Central Highlands polygon\n              weight = 1,            # Border width\n              fillColor = \"lightblue\",  # Fill color of Central Highlands\n              fillOpacity = 0.3,     # Transparency for fill\n              group = \"Central Highlands\") %&gt;%\n  \n    # Add Mountain Ash presence points\n  addCircleMarkers(data = mountain_ash_present,\n                   color = \"#11aa96\",\n                   radius = 1,\n                   weight = 0.5,\n                   opacity = 1,\n                   fillOpacity = 1,\n                   group = \"Mountain Ash Presence Records\") %&gt;%\n  \n\n      # Add Mountain Ash absent points\n  addCircleMarkers(data = mountain_ash_absent,\n                   color = \"#f6aa70\",\n                   radius = 1,\n                   weight = 0.5,\n                   opacity = 1,\n                   fillOpacity = 1,\n                   group = \"Mountain Ash Pseudo_absent Records\") %&gt;%\n  \n  setView(lng = 145.7, lat = -37.5, zoom = 7)  %&gt;% # Adjust longitude, latitude, and zoom as needed \n  \n    # Add layer controls for easier toggling\n  addLayersControl(\n    overlayGroups = c(\"Central Highlands\", \"Mountain Ash Presence Records\", \"Mountain Ash Pseudo_absent Records\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) %&gt;%\n  \n  # Add a legend for the layers\n  addControl(\n    html = \"\n    &lt;div style='background-color: white; padding: 10px; border-radius: 5px;'&gt;\n      &lt;strong&gt;Legend&lt;/strong&gt;&lt;br&gt;\n      &lt;i style='background: lightblue; width: 18px; height: 18px; display: inline-block; margin-right: 8px; opacity: 0.7;'&gt;&lt;/i&gt;\n      Central Highlands&lt;br&gt;\n      &lt;i style='background: #11aa96; width: 10px; height: 10px; border-radius: 50%; display: inline-block; margin-right: 8px;'&gt;&lt;/i&gt;\n      Mountain Ash Presence Records&lt;br&gt;\n      &lt;i style='background: #f6aa70; width: 10px; height: 10px; border-radius: 50%; display: inline-block; margin-right: 8px;'&gt;&lt;/i&gt;\n      Mountain Ash Pseudo-absent Records\n    &lt;/div&gt;\n    \",\n    position = \"bottomright\"\n  )"
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#environmental-data",
    "href": "notebooks/EC_GLM/EC_GLM.html#environmental-data",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "2.3 Environmental Data",
    "text": "2.3 Environmental Data\nWhen selecting environmental variables for a model, it is important to avoid indiscriminately including any available data. Instead, select variables thoughtfully, guided by ecological knowledge and the specific hypotheses being tested.\nAs previously mentioned, Mountain Ash thrives in cool, temperate rainforests characterized by high rainfall, deep, well-drained soils, mild temperatures, and high humidity. This species is typically found at altitudes ranging from 200 to 1,000 meters above sea level (Burns et al., 2015). Mountain Ash habitats face two primary forms of disturbance: bushfires, which are the main natural disturbance, and logging, which constitutes the primary human-induced threat (Burns et al., 2015; Nevill et al., 2010).\n\n\n\n\n\n\n\n\n\nEnvironmental Variables Categories\nVariables\nData Type\nSource\n\n\n\n\nTemperature and Radiation\nBioclim 01: Annual mean temperature\nBioclim 04: Temperature Seasonality (standard deviation *100)\nBioclim 06: Min Temperature of Coldest Month\nBioclim 10: Mean Temperature of Warmest Quarter\nBioclim 20: Annual mean radiation\nContinuous\n1976-2005, CSIRO via EcoCommons\n\n\nHumidity\nBioclim 12: Annual Precipitation\nBioclim 18: Precipitation of Warmest Quarter\nBioclim 19: Precipitation of Coldest Quarter\nBioclim 28: Annual mean moisture index\nBioclim 34: Mean moisture index of warmest quarter\nBioclim 35: Mean moisture index of coldest quarter\nContinuous\n1976-2005, CSIRO via EcoCommons\n\n\nTopography\nDigital Elevation Model\nContinuous\nGeoscience Australia via EcoCommons\n\n\nSoil\nAustralian Soil Classification\nCategorical\nTern via EcoCommons\n\n\nDisturbance\nFires\nLogging\nContinuous\nVic DEECA spatial data and resources\n\n\n\n\n# Load the stacked raster layers\nenv_var_stack &lt;- rast(\"mountain_ash_centralhighlands/central_highlands_15envvar.tif\")\n\n# Define the custom names for the raster layers\nlayer_names &lt;- c(\n  \"Annual_Mean_Temp\",\n  \"Temp_Seasonality\",\n  \"Min_Temp_Coldest_Month\",\n  \"Mean_Temp_Warmest_Quarter\",\n  \"Annual_Mean_Radiation\",\n  \"Annual_Precipitation\",\n  \"Precip_Warmest_Quarter\",\n  \"Precip_Coldest_Quarter\",\n  \"Annual_Mean_Moisture\",\n  \"Moisture_Warmest_Quarter\",\n  \"Moisture_Coldest_Quarter\",\n  \"Elevation\",\n  \"Soil_Type\",\n  \"Fires\",\n  \"Logging\"\n)\n\n\n# Assign the custom names to the raster layers\nnames(env_var_stack) &lt;- layer_names\n\n# We want to make sure that soil type raster layer is factor.\nenv_var_stack[[\"Soil_Type\"]] &lt;- as.factor(env_var_stack[[\"Soil_Type\"]])\n\n# Check if the names are assigned correctly\nprint(names(env_var_stack))\n\n [1] \"Annual_Mean_Temp\"          \"Temp_Seasonality\"         \n [3] \"Min_Temp_Coldest_Month\"    \"Mean_Temp_Warmest_Quarter\"\n [5] \"Annual_Mean_Radiation\"     \"Annual_Precipitation\"     \n [7] \"Precip_Warmest_Quarter\"    \"Precip_Coldest_Quarter\"   \n [9] \"Annual_Mean_Moisture\"      \"Moisture_Warmest_Quarter\" \n[11] \"Moisture_Coldest_Quarter\"  \"Elevation\"                \n[13] \"Soil_Type\"                 \"Fires\"                    \n[15] \"Logging\"                  \n\n\n\n# Custom titles for the layers\nlayer_titles &lt;- c(\n  \"Bioclim 01: Annual Mean Temperature\",\n  \"Bioclim 04: Temperature Seasonality (standard deviation *100)\",\n  \"Bioclim 06: Min Temperature of Coldest Month\",\n  \"Bioclim 10: Mean Temperature of Warmest Quarter\",\n  \"Bioclim 20: Annual mean radiation\"\n)\n\n# Indices of the layers to plot\nlayers_to_plot &lt;- c(1, 2, 3, 4, 8)\n\n# Set layout for plotting (3 rows, 2 columns for 5 plots)\npar(mfrow = c(3, 2)) \n\n# Plot each specified layer with its custom title and smaller title size\nfor (i in seq_along(layers_to_plot)) {\n  layer_index &lt;- layers_to_plot[i]\n  plot(env_var_stack[[layer_index]], main = layer_titles[i], cex.main = 0.8) # Adjust cex.main for title size\n}\n\n\n\n\n\n\n\n\n\n# Custom titles for the specified layers\nlayer_titles &lt;- c(\n  \"Bioclim 12: Annual Precipitation\",\n  \"Bioclim 18: Precipitation of Warmest Quarter\",\n  \"Bioclim 19: Precipitation of Coldest Quarter\",\n  \"Bioclim 28: Annual Mean Moisture Index\",\n  \"Bioclim 34: Mean Moisture Index of Warmest Quarter\",\n  \"Bioclim 35: Mean Moisture Index of Coldest Quarter\"\n)\n\n# Indices of the layers to plot\nlayers_to_plot &lt;- c(5, 6, 7, 9, 10, 11)\n\n# Set layout for plotting (3 rows, 2 columns for 5 plots)\npar(mfrow = c(3, 2)) \n\n# Plot each specified layer with its custom title and smaller title size\nfor (i in seq_along(layers_to_plot)) {\n  layer_index &lt;- layers_to_plot[i]\n  plot(env_var_stack[[layer_index]], main = layer_titles[i], cex.main = 0.8) # Adjust cex.main for title size\n}\n\n\n\n\n\n\n\n\n\n# Custom titles for the specified layers\nlayer_titles &lt;- c(\n  \"Digital Elevation Model\",\n  \"Australian Soil Classification\",\n  \"Logging\",\n  \"Fires\"\n)\n\n# Indices of the layers to plot\nlayers_to_plot &lt;- c(12:15)\n\n# Set layout for plotting (3 rows, 2 columns for 5 plots)\npar(mfrow = c(2, 2)) \n\n# Plot each specified layer with its custom title and smaller title size\nfor (i in seq_along(layers_to_plot)) {\n  layer_index &lt;- layers_to_plot[i]\n  plot(env_var_stack[[layer_index]], main = layer_titles[i], cex.main = 0.8) # Adjust cex.main for title size\n}\n\n\n\n\n\n\n\n\nSoil Type Classification: 3 - Dermosol, 4 - Chromosol, 5 - Ferrosol, 7 - Tenosol, 8 - Kandosol,12 - Calcarosol, 13 - Organosol, and 14 - Anthroposol."
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#combine-species-occurrence-data-and-environmental-variables",
    "href": "notebooks/EC_GLM/EC_GLM.html#combine-species-occurrence-data-and-environmental-variables",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "2.4 Combine species occurrence data and environmental variables",
    "text": "2.4 Combine species occurrence data and environmental variables\nWe will create a data frame that combines each presence/absence record of Mountain Ash with data from our 15 environmental variables.\n\n# Convert species occurrence data to terra-compatible SpatVector\noccurrence_vect &lt;- vect(mountain_ash_centralhighlands)\n\n# Extract raster values at species occurrence points\nextracted_values &lt;- terra::extract(env_var_stack, occurrence_vect)\n\n# Combine occurrence data with extracted raster values\n# (Keep geometry or drop it based on your needs)\noccurrence_data &lt;- cbind(as.data.frame(mountain_ash_centralhighlands), extracted_values)\n\n# Remove rows with any NA values in predictors\noccurrence_data &lt;- na.omit(occurrence_data)\n\n# we want to make sure the data type of soil types is factor.\noccurrence_data$Soil_Type &lt;- as.factor(occurrence_data$Soil_Type)\n\nhead(occurrence_data)\n\n  occrrnS                   geometry ID Annual_Mean_Temp Temp_Seasonality\n1       1 POINT (145.7339 -37.67417)  1        10.369396         1.424792\n2       1  POINT (146.1373 -37.7804)  2         8.822482         1.449565\n3       1 POINT (145.3817 -37.88166)  3        12.967784         1.280466\n4       1 POINT (146.1797 -37.84517)  4        10.826058         1.390526\n5       1 POINT (145.3362 -37.88002)  5        11.825127         1.278574\n6       1 POINT (145.9281 -37.83222)  6        10.293063         1.386414\n  Min_Temp_Coldest_Month Mean_Temp_Warmest_Quarter Annual_Mean_Radiation\n1              2.0631714                  15.60523              1578.439\n2              0.4309082                  14.10276              1577.427\n3              4.2351027                  17.71625              1133.458\n4              1.5544521                  15.95073              1583.145\n5              3.9153066                  16.52677              1266.487\n6              1.6763960                  15.38035              1609.709\n  Annual_Precipitation Precip_Warmest_Quarter Precip_Coldest_Quarter\n1             250.6433               530.1663               13.70784\n2             268.0575               503.1617               13.63218\n3             203.8828               331.7352               14.27531\n4             275.0496               479.6527               13.56016\n5             230.7354               364.2117               13.97706\n6             267.1724               513.3069               13.54408\n  Annual_Mean_Moisture Moisture_Warmest_Quarter Moisture_Coldest_Quarter\n1            0.9548221                0.8892441                1.0000000\n2            0.9821509                0.9541187                1.0000000\n3            0.8531148                0.6036963                0.9996078\n4            0.9700990                0.9209611                1.0000000\n5            0.9097116                0.7599657                1.0000000\n6            0.9698939                0.9229909                1.0000000\n  Elevation Soil_Type Fires Logging\n1  834.7029         4     0       0\n2 1058.5144         4     0       3\n3  314.3798         4     0       0\n4  658.4626         7     0       9\n5  544.9786         4     0       0\n6  902.7314         4     0       7"
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#multicollinearity-and-variable-selection",
    "href": "notebooks/EC_GLM/EC_GLM.html#multicollinearity-and-variable-selection",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "3.1 Multicollinearity and Variable Selection",
    "text": "3.1 Multicollinearity and Variable Selection\nTesting for collinearity among continuous variables is an important step in many modeling processes, particularly in species distribution modeling and other regression-based analyses. Codllinearity occurs when two or more predictor variables in a dataset are highly correlated, which can lead to unstable estimates of regression coefficients and make it difficult to interpret the results.\nThere are two common methods for testing collinearity among continuous variables.\nCorrelation matrix.\n\n\nlayer_names &lt;- c(\n  \"Annual_Mean_Temp\",\n  \"Temp_Seasonality\",\n  \"Min_Temp_Coldest_Month\",\n  \"Mean_Temp_Warmest_Quarter\",\n  \"Annual_Mean_Radiation\",\n  \"Annual_Precipitation\",\n  \"Precip_Warmest_Quarter\",\n  \"Precip_Coldest_Quarter\",\n  \"Annual_Mean_Moisture\",\n  \"Moisture_Warmest_Quarter\",\n  \"Moisture_Coldest_Quarter\",\n  \"Elevation\",\n  \"Fires\",\n  \"Logging\"\n)\n\n\n# Select columns by their names\ncor_data &lt;- occurrence_data[, layer_names]\n\n# Check the structure of the numeric data\nstr(cor_data)\n\n'data.frame':   3932 obs. of  14 variables:\n $ Annual_Mean_Temp         : num  10.37 8.82 12.97 10.83 11.83 ...\n $ Temp_Seasonality         : num  1.42 1.45 1.28 1.39 1.28 ...\n $ Min_Temp_Coldest_Month   : num  2.063 0.431 4.235 1.554 3.915 ...\n $ Mean_Temp_Warmest_Quarter: num  15.6 14.1 17.7 16 16.5 ...\n $ Annual_Mean_Radiation    : num  1578 1577 1133 1583 1266 ...\n $ Annual_Precipitation     : num  251 268 204 275 231 ...\n $ Precip_Warmest_Quarter   : num  530 503 332 480 364 ...\n $ Precip_Coldest_Quarter   : num  13.7 13.6 14.3 13.6 14 ...\n $ Annual_Mean_Moisture     : num  0.955 0.982 0.853 0.97 0.91 ...\n $ Moisture_Warmest_Quarter : num  0.889 0.954 0.604 0.921 0.76 ...\n $ Moisture_Coldest_Quarter : num  1 1 1 1 1 ...\n $ Elevation                : num  835 1059 314 658 545 ...\n $ Fires                    : num  0 0 0 0 0 0 5 0 0 0 ...\n $ Logging                  : num  0 3 0 9 0 7 0 4 8 4 ...\n\n# Calculate the correlation matrix for the numeric columns\ncor_matrix &lt;- cor(cor_data, use = \"complete.obs\", method = \"pearson\")\n\n\ncorrplot(cor_matrix,\n         method = \"color\",            # Use colored squares for correlation\n         type = \"upper\",              # Show upper triangle only\n         order = \"hclust\",            # Reorder variables hierarchically\n         addCoef.col = \"black\",       # Show correlation coefficients in black\n         number.cex = 0.5,            # Reduce the size of correlation labels\n         tl.col = \"black\",            # Text label color\n         tl.srt = 30,                 # Rotate labels slightly for readability\n         tl.cex = 0.5,                # Reduce text size of variable labels (set smaller valu)\n         cl.cex = 0.8,                # Reduce text size of color legend\n         diag = FALSE,                # Hide diagonal\n         col = colorRampPalette(c(\"#11aa96\", \"#61c6fa\", \"#f6aa70\"))(200),\n         sig.level = 0.01, insig = \"blank\")\n\n\n\n\n\n\n\n\nIf you find corrplot is hard for you to make decisions, we can use Variance Inflation Factor (VIF). VIF is another statistical measure used to detect multicollinearity in a set of explanatory (independent) variables in a regression model.\nInterpretation:\n\nVIF = 1: No correlation\nVIF &gt; 1 and &lt;= 5: Moderate correlation; may not require corrective action.\nVIF &gt; 5: Indicates high correlation. Multicollinearity may be problematic, and further investigation is recommended.\nVIF &gt; 10: Strong multicollinearity. The variable is highly collinear with others, and steps should be taken to address this.\n\n\n# Fit a GLM for species distribution\nglm_model &lt;- glm(\n  occrrnS ~ Annual_Mean_Temp + Temp_Seasonality + Min_Temp_Coldest_Month +\n    Mean_Temp_Warmest_Quarter + Annual_Mean_Radiation + Annual_Precipitation +\n    Precip_Warmest_Quarter + Precip_Coldest_Quarter + Annual_Mean_Moisture +\n    Moisture_Warmest_Quarter + Moisture_Coldest_Quarter + Elevation + Fires + Logging,\n  data = occurrence_data,\n  family = binomial(link = \"logit\")  # Logistic regression\n)\n\n\n# Calculate VIF for the GLM\nvif_values &lt;- vif(glm_model)\n\n# Convert VIF values to a data frame with two columns\nvif_table &lt;- data.frame(\n  Variable = names(vif_values),  # Column 1: Variable names\n  VIF = round(vif_values, 2)     # Column 2: VIF values rounded to 2 decimals\n)\n\n# Rank the VIF table from high to low\nvif_table &lt;- vif_table[order(vif_table$VIF, decreasing = TRUE), ]\n\n# Print the ranked table\nprint(vif_table)\n\n                                           Variable      VIF\nAnnual_Mean_Temp                   Annual_Mean_Temp 30369.96\nMean_Temp_Warmest_Quarter Mean_Temp_Warmest_Quarter 21826.71\nAnnual_Mean_Radiation         Annual_Mean_Radiation  2863.99\nAnnual_Mean_Moisture           Annual_Mean_Moisture  1625.31\nTemp_Seasonality                   Temp_Seasonality  1527.84\nMoisture_Warmest_Quarter   Moisture_Warmest_Quarter  1241.81\nPrecip_Warmest_Quarter       Precip_Warmest_Quarter  1228.60\nPrecip_Coldest_Quarter       Precip_Coldest_Quarter   562.37\nAnnual_Precipitation           Annual_Precipitation   308.52\nElevation                                 Elevation    42.83\nMin_Temp_Coldest_Month       Min_Temp_Coldest_Month    24.07\nMoisture_Coldest_Quarter   Moisture_Coldest_Quarter     3.80\nLogging                                     Logging     1.27\nFires                                         Fires     1.03\n\n\nRules of thumb for VIF:\nVariables to Drop (Initial Recommendation):\n\nAnnual_Mean_Temp (High VIF + highly correlated with many others).\nMean_Temp_Warmest_Quarter (High VIF + redundant with Min_Temp_Coldest_Month).\nAnnual_Mean_Radiation (High VIF + redundant with Elevation).\nAnnual_Precipitation (Redundant with Annual_Mean_Moisture).\nPrecip_Warmest_Quarter (Redundant with Moisture_Warmest_Quarter).\n\nGetting rid of some highly correlated variables and run VIF again.\n\n# Fit a GLM for testing the VIF\nglm_model &lt;- glm(\n  occrrnS ~ Temp_Seasonality + Min_Temp_Coldest_Month +\n    Precip_Coldest_Quarter + Moisture_Coldest_Quarter + Elevation + Fires + Logging,\n  data = occurrence_data,\n  family = binomial(link = \"logit\")  # Logistic regression\n)\n\n# Calculate VIF for the GLM\nvif_values &lt;- vif(glm_model)\n\n# Convert VIF values to a data frame with two columns\nvif_table &lt;- data.frame(\n  Variable = names(vif_values),  # Column 1: Variable names\n  VIF = round(vif_values, 2)     # Column 2: VIF values rounded to 2 decimals\n)\n\n# Rank the VIF table from high to low\nvif_table &lt;- vif_table[order(vif_table$VIF, decreasing = TRUE), ]\n\n# Print the ranked table\nprint(vif_table)\n\n                                         Variable  VIF\nElevation                               Elevation 7.21\nMin_Temp_Coldest_Month     Min_Temp_Coldest_Month 5.43\nTemp_Seasonality                 Temp_Seasonality 4.33\nPrecip_Coldest_Quarter     Precip_Coldest_Quarter 3.41\nMoisture_Coldest_Quarter Moisture_Coldest_Quarter 1.53\nLogging                                   Logging 1.13\nFires                                       Fires 1.01\n\n\nNow, from our 14 continuous variables, we choose above 7 variables and 1 categorical variable Soil_Types to make our final model."
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#data-splitting",
    "href": "notebooks/EC_GLM/EC_GLM.html#data-splitting",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "3.2 Data splitting",
    "text": "3.2 Data splitting\nFor cross-validation purposes, we need to leave out some data as testing dataset. There are many strategies of splitting data for cross-validation, like random, k-fold, and leave-one-out etc. Here we will use the easiest one: random splitting.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Split the data into training (80%) and testing (20%)\ntrain_index &lt;- sample(1:nrow(occurrence_data), size = 0.8 * nrow(occurrence_data))\n\n# Create training and testing datasets\ntrain_data &lt;- occurrence_data[train_index, ]\ntest_data &lt;- occurrence_data[-train_index, ]\n\n# Check the split\ncat(\"Training Set:\", nrow(train_data), \"rows\\n\")\n\nTraining Set: 3145 rows\n\ncat(\"Testing Set:\", nrow(test_data), \"rows\\n\")\n\nTesting Set: 787 rows"
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#model-fitting-1",
    "href": "notebooks/EC_GLM/EC_GLM.html#model-fitting-1",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "3.3 Model fitting",
    "text": "3.3 Model fitting\nNull model: no explanatory variables or predictors are included.\nIt is always helpful to create a null model as a benchmark to assess how the inclusion of explanatory variables improves the model.\n\n# Let's make a null model as a benchmark\n\n# Fit a null model with only the intercept\nnull_model &lt;- glm(occrrnS ~ 1,\n                  data = train_data,\n                  family = binomial(link = \"logit\"))\n\nsummary(null_model)\n\n\nCall:\nglm(formula = occrrnS ~ 1, family = binomial(link = \"logit\"), \n    data = train_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.51541    0.03685   13.98   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4157.8  on 3144  degrees of freedom\nResidual deviance: 4157.8  on 3144  degrees of freedom\nAIC: 4159.8\n\nNumber of Fisher Scoring iterations: 4\n\n\nNow, we can fit a full model.\n\n# Fit the GLM on the training data\nglm_model &lt;- glm(\n  occrrnS ~ Temp_Seasonality + Min_Temp_Coldest_Month +\n    Precip_Coldest_Quarter + Moisture_Coldest_Quarter +\n    Elevation + Fires + Soil_Type + Logging,\n  data = train_data,\n  family = binomial(link = \"logit\")  # Logistic regression\n)\n\n# Summarize the model\nsummary(glm_model)\n\n\nCall:\nglm(formula = occrrnS ~ Temp_Seasonality + Min_Temp_Coldest_Month + \n    Precip_Coldest_Quarter + Moisture_Coldest_Quarter + Elevation + \n    Fires + Soil_Type + Logging, family = binomial(link = \"logit\"), \n    data = train_data)\n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               1.662e+01  3.074e+01   0.541   0.5887    \nTemp_Seasonality         -3.247e+00  1.315e+00  -2.470   0.0135 *  \nMin_Temp_Coldest_Month    1.551e+00  9.849e-02  15.750  &lt; 2e-16 ***\nPrecip_Coldest_Quarter   -4.519e+00  3.233e-01 -13.978  &lt; 2e-16 ***\nMoisture_Coldest_Quarter  4.487e+01  2.935e+01   1.529   0.1264    \nElevation                 2.484e-03  4.838e-04   5.133 2.85e-07 ***\nFires                     1.018e-02  9.933e-02   0.103   0.9184    \nSoil_Type4                1.949e+00  3.955e-01   4.928 8.31e-07 ***\nSoil_Type5               -1.770e-01  4.959e-01  -0.357   0.7211    \nSoil_Type7                1.621e+00  4.052e-01   4.002 6.29e-05 ***\nSoil_Type8               -1.347e+01  6.038e+02  -0.022   0.9822    \nSoil_Type12              -1.336e+01  4.723e+02  -0.028   0.9774    \nSoil_Type13              -1.554e+01  1.309e+03  -0.012   0.9905    \nSoil_Type14              -1.286e+01  6.500e+02  -0.020   0.9842    \nLogging                   1.378e-01  1.514e-02   9.102  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4157.8  on 3144  degrees of freedom\nResidual deviance: 2414.8  on 3130  degrees of freedom\nAIC: 2444.8\n\nNumber of Fisher Scoring iterations: 15"
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#summary-of-interpretation",
    "href": "notebooks/EC_GLM/EC_GLM.html#summary-of-interpretation",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "4.1 Summary of Interpretation",
    "text": "4.1 Summary of Interpretation\n\n# Let's compare the performance of our model to a null model\n\n# Compare null model with full model using the analysis of deviance (Likelihood Ratio Test)\nanova(null_model, glm_model, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: occrrnS ~ 1\nModel 2: occrrnS ~ Temp_Seasonality + Min_Temp_Coldest_Month + Precip_Coldest_Quarter + \n    Moisture_Coldest_Quarter + Elevation + Fires + Soil_Type + \n    Logging\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1      3144     4157.8                          \n2      3130     2414.8 14     1743 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Compare the AIC of the null model and the full model\nAIC(null_model, glm_model)\n\n           df      AIC\nnull_model  1 4159.769\nglm_model  15 2444.787\n\n# Get the null deviance and residual deviance from the full model\nnull_deviance &lt;- glm_model$null.deviance\nresidual_deviance &lt;- glm_model$deviance\n\n# Calculate the deviance explained\ndeviance_explained &lt;- (null_deviance - residual_deviance) / null_deviance\n\n# Print the deviance explained as a percentage\ndeviance_explained_percent &lt;- deviance_explained * 100\ncat(\"Deviance Explained:\", deviance_explained_percent, \"%\\n\")\n\nDeviance Explained: 41.92108 %\n\n\nThe Likelihood Ratio Test (ANOVA) shows that adding the predictors significantly improved the model’s fit compared to the null model, as indicated by the high deviance reduction and a p-value of 0.\nThe AIC for the full model is much lower than the null model, further indicating a better fit when balancing model complexity.\nThe Deviance Explained of 42 % suggests that the full model explains almost half of the variability in mountain ash presence/absence, indicating that while the predictors contribute useful information, there is still substantial unexplained variability that may require further investigation or additional predictors.\nVariable Importance metric.\nIt is a measure used to assess the relative importance of predictors (environmental variables) in the model.\n\n# Function to plot effect size graph\nplot_effect_size &lt;- function(glm_model) {\n  # Check if required libraries are installed\n  if (!requireNamespace(\"ggplot2\", quietly = TRUE)) {\n    stop(\"Please install the 'ggplot2' package to use this function.\")\n  }\n  library(ggplot2)\n\n  # Extract effect sizes (coefficients) from the model\n  coefs &lt;- summary(glm_model)$coefficients\n  effect_sizes &lt;- data.frame(\n    Variable = rownames(coefs)[-1],  # Exclude the intercept\n    Effect_Size = coefs[-1, \"Estimate\"],\n    Std_Error = coefs[-1, \"Std. Error\"]\n  )\n\n  # Sort by effect size\n  effect_sizes &lt;- effect_sizes[order(-abs(effect_sizes$Effect_Size)), ]\n\n  # Plot the effect sizes with error bars\n  ggplot(effect_sizes, aes(x = reorder(Variable, Effect_Size), y = Effect_Size)) +\n    geom_bar(stat = \"identity\", fill = \"#11aa96\") +\n    geom_errorbar(aes(ymin = Effect_Size - Std_Error, ymax = Effect_Size + Std_Error), width = 0.2) +\n    coord_flip() +\n    labs(\n      title = \"Effect Sizes of Variables\",\n      x = \"Variable\",\n      y = \"Effect Size (Coefficient Estimate)\"\n    ) +\n    theme_minimal()\n}\n\n# Example usage of effect size plot\nplot_effect_size(glm_model)"
  },
  {
    "objectID": "notebooks/EC_GLM/EC_GLM.html#cross-validation",
    "href": "notebooks/EC_GLM/EC_GLM.html#cross-validation",
    "title": "Species Distribution Analysis - Generalised Linear Model (GLM)",
    "section": "4.2 Cross validation",
    "text": "4.2 Cross validation\nNow, we use the testing data to evaluate the model performance.\n\n# Predict on the testing data\npredicted_probs &lt;- predict(glm_model, newdata = test_data, type = \"response\")\n\n# Create an ROC curve and compute AUC\nroc_curve &lt;- roc(test_data$occrrnS, predicted_probs)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nauc_value &lt;- auc(roc_curve)\n\n# Extract ROC data for ggplot\nroc_data &lt;- data.frame(\n  Sensitivity = roc_curve$sensitivities,\n  Specificity = 1 - roc_curve$specificities\n)\n\n# Plot the ROC curve using ggplot2\nggplot(roc_data, aes(x = Specificity, y = Sensitivity)) +\n  geom_line(color = \"#61cafa\", linewidth = 0.5) +                  # ROC curve\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray\") + # Diagonal line\n  labs(\n    title = paste(\"ROC Curve (AUC =\", round(auc_value, 2), \")\"),\n    x = \"1 - Specificity\",\n    y = \"Sensitivity\"\n  ) +\n  theme_minimal() +                                           # Minimal theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"), # Centered and bold title\n    axis.title = element_text(size = 12),                   # Axis label font size\n    axis.text = element_text(size = 10)                     # Axis tick font size\n  )\n\n\n\n\n\n\n\n\n\nplot_species_response &lt;- function(glm_model, predictors, data) {\n  # Check if required libraries are installed\n  if (!requireNamespace(\"ggplot2\", quietly = TRUE) || !requireNamespace(\"gridExtra\", quietly = TRUE)) {\n    stop(\"Please install the 'ggplot2' and 'gridExtra' packages to use this function.\")\n  }\n  library(ggplot2)\n  library(gridExtra)\n\n  # Create empty list to store response plots\n  response_plots &lt;- list()\n\n  # Loop through each predictor variable\n  for (predictor in predictors) {\n    # Create new data frame to vary predictor while keeping others constant\n    pred_range &lt;- seq(\n      min(data[[predictor]], na.rm = TRUE),\n      max(data[[predictor]], na.rm = TRUE),\n      length.out = 100\n    )\n    const_data &lt;- data[1, , drop = FALSE]  # Use first row to keep other predictors constant\n    response_data &lt;- const_data[rep(1, 100), ]  # Duplicate the row\n    response_data[[predictor]] &lt;- pred_range\n\n    # Predict probabilities\n    predicted_response &lt;- predict(glm_model, newdata = response_data, type = \"response\")\n\n    # Create data frame for plotting\n    plot_data &lt;- data.frame(\n      Predictor_Value = pred_range,\n      Predicted_Probability = predicted_response\n    )\n\n    # Add presence and absence data\n    presence_absence_data &lt;- data.frame(\n      Predictor_Value = data[[predictor]],\n      Presence_Absence = data$occrrnS\n    )\n\n    # Generate the response plot\n    p &lt;- ggplot() +\n      geom_line(data = plot_data, aes(x = Predictor_Value, y = Predicted_Probability), color = \"#61c6fa\", linewidth = 1) +\n      geom_point(data = presence_absence_data[presence_absence_data$Presence_Absence == 1, ], aes(x = Predictor_Value, y = Presence_Absence), color = \"#11aa96\", alpha = 0.6) +\n      geom_point(data = presence_absence_data[presence_absence_data$Presence_Absence == 0, ], aes(x = Predictor_Value, y = Presence_Absence), color = \"#f6aa70\", alpha = 0.6) +\n      labs(x = predictor, y = NULL) +\n      theme_minimal() +\n      theme(axis.title.y = element_blank())\n\n    # Store the plot in the list\n    response_plots[[predictor]] &lt;- p\n  }\n\n  # Arrange all plots in one combined plot with a single shared y-axis label\n  grid.arrange(\n    grobs = response_plots, \n    ncol = 3,\n    left = \"Predicted Probability / Presence-Absence\"\n  )\n}\n\n# Example usage:\npredictors &lt;- c(\"Temp_Seasonality\", \"Min_Temp_Coldest_Month\", \"Precip_Coldest_Quarter\", \"Moisture_Coldest_Quarter\", \"Elevation\", \"Fires\", \"Logging\")\nplot_species_response(glm_model, predictors, train_data)\n\n\n\n\n\n\n\n\n\n# Density and Histogram Plot Function\nplot_density_histogram &lt;- function(predicted_probs, actual_labels) {\n  # Combine data into a data frame\n  plot_data &lt;- data.frame(\n    Predicted_Probability = predicted_probs,\n    Presence_Absence = factor(actual_labels, levels = c(0, 1), labels = c(\"Absence\", \"Presence\"))\n  )\n  \n  # Plot density and histogram\n  ggplot(plot_data, aes(x = Predicted_Probability, fill = Presence_Absence)) +\n    geom_histogram(aes(y = after_stat(density)), bins = 10, alpha = 0.6, position = \"identity\") +  # Histogram with density\n    geom_density(alpha = 0.4) +  # Density curve\n    labs(\n      title = \"Density and Histogram of Predicted Probabilities\",\n      x = \"Predicted Probability\",\n      y = \"Density\",\n      fill = \"Presence/Absence\"\n    ) +\n    scale_fill_manual(values = c(\"#f6aa70\", \"#11aa96\")) +  # Custom colors\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n      axis.title = element_text(size = 12),\n      axis.text = element_text(size = 10),\n      legend.position = \"top\"\n    )\n}\n\n# Example usage:\n# Replace `predicted_probs` with your predicted probabilities and `test_data$occrrnS` with your actual labels\nplot_density_histogram(predicted_probs, test_data$occrrnS)"
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#environmental-variables",
    "href": "notebooks/data_prep/raster_preparation.html#environmental-variables",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "1. Environmental variables",
    "text": "1. Environmental variables\nIn Species Distribution Modeling (SDM), environmental variables are factors that influence where a species can live. These include:\n\nClimatic Variables: Temperature, precipitation.\nTopographic Variables: Elevation, slope.\nSoil Variables: Soil, pH, texture.\nVegetation Variables: Land cover type, canopy cover.\nHydrological Variables: Distance to water bodies, soil moisture.\nBiotic Variables: Presence of prey, competitors.\nAnthropogenic Variables: Land use, human impact.\n\nThese variables help explain and predict a species’ habitat suitability based on environmental conditions."
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#raster",
    "href": "notebooks/data_prep/raster_preparation.html#raster",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "2. Raster",
    "text": "2. Raster\nRaster data is a type of spatial data used to represent continuous surfaces, like environmental layers (e.g., elevation, temperature) in grids or cells. Each cell (pixel) has a value that represents information about that area.\nImportant Characteristics:\n\nResolution: Size of each cell, determining data detail (e.g., 10m x 10m cells).\nExtent: Geographic area covered by the raster.\nCoordinate Reference System (CRS): Defines the spatial reference of the raster for location accuracy.\nData Type: Can be continuous (e.g., temperature) or categorical (e.g., land cover type).\n\nThese characteristics impact how raster data is interpreted and used in analyses like SDM.\nTo extract values from raster data or perform spatial analyses, it is crucial that all rasters have the same resolution, extent, and coordinate reference system (CRS). This ensures that the data aligns correctly and avoids mismatches or errors during analysis.\nIdeally, you should define a single CRS to be used consistently across all spatial files in a project, including shapefiles and raster files. This uniformity simplifies processing and ensures that all spatial data layers are accurately overlaid and compared.\nRead “What is raster data?” by ArcMap for more detailed explanation."
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#categorical-and-continuous-data",
    "href": "notebooks/data_prep/raster_preparation.html#categorical-and-continuous-data",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "3. Categorical and Continuous data",
    "text": "3. Categorical and Continuous data\nIn environmental modeling, environmental variables can be classified as categorical or continuous:\nCategorical Data\n\nRepresents discrete classes or categories.\nExamples:\n\nLand Cover Type: Forest, grassland, urban.\nSoil Type: Sandy, clay, loam.\nVegetation Type: Different plant communities.\n\n\nContinuous Data\n\nRepresents data with a smooth gradient, measurable values.\nExamples:\n\nTemperature: Annual mean temperature.\nPrecipitation: Monthly or annual rainfall.\nElevation: Height above sea level in meters.\n\n\nCategorical data is useful for distinct classifications, while continuous data is used for variables that change gradually across the landscape. Both types are important for predicting species distributions."
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#s.1-set-the-working-directory-and-create-a-folder-for-data.",
    "href": "notebooks/data_prep/raster_preparation.html#s.1-set-the-working-directory-and-create-a-folder-for-data.",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "S.1 Set the working directory and create a folder for data.",
    "text": "S.1 Set the working directory and create a folder for data.\nSave the Quarto Markdown file (.QMD) to a folder of your choice, and then set the path to your folder as your working directory.\n\n# Set the workspace to the current working directory\n# Uncomment and replace the path below with your own working directory if needed:\n\n#setwd(\"/Users/zhaoxiang/Desktop/raster_preparation\")\n\nworkspace &lt;- getwd()  # Get the current working directory and store it in 'workspace'\n\n# Increase the plot size by adjusting the options for plot dimensions in the notebook output\noptions(repr.plot.width = 16, repr.plot.height = 8)  # Sets width to 16 and height to 8 for larger plots\n\nIdeally, you would use the renv package to create an isolated environment for installing all the required R packages used in this notebook. However, since installing renv and its dependencies can be time-consuming, we recommend trying this after the workshop.\n\n# # Ensure \"renv\" package is installed\n# if (!requireNamespace(\"renv\", quietly = TRUE)) {\n#   install.packages(\"renv\")\n# }\n# \n# # Check if renv has been initialized in the project\n# if (!file.exists(\"renv/activate.R\")) {\n#   message(\"renv has not been initiated in this project. Initializing now...\")\n#   renv::init()  # Initialize renv if not already set up\n# } else {\n#   source(\"renv/activate.R\")  # Activate the renv environment\n#   message(\"renv is activated.\")\n# }\n# \n# # Check for the existence of renv.lock and restore the environment\n# if (file.exists(\"renv.lock\")) {\n#   message(\"Restoring renv environment from renv.lock...\")\n#   renv::restore()\n# } else {\n#   message(\"No renv.lock file found in the current directory. Skipping restore.\")\n# }"
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#s.2-install-and-load-essential-libraries.",
    "href": "notebooks/data_prep/raster_preparation.html#s.2-install-and-load-essential-libraries.",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "S.2 Install and load essential libraries.",
    "text": "S.2 Install and load essential libraries.\nInstall and load R packages. Terra and sf are essential for working with raster and vector data, respectively. Other packages like dplyr, ggplot2, and leaflet are useful for data manipulation, visualization, and interactive mapping.\n\n# Set CRAN mirror\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\n# List of packages to check, install if needed, and load\npackages &lt;- c(\"dplyr\", \"terra\", \"sf\", \"googledrive\", \"ggplot2\", \"leaflet\", \"shiny\", \"htmltools\")\n\n# Function to display a cat message\ncat_message &lt;- function(pkg, message_type) {\n  if (message_type == \"installed\") {\n    cat(paste0(pkg, \" has been installed successfully!\\n\"))\n  } else if (message_type == \"loading\") {\n    cat(paste0(pkg, \" is already installed and has been loaded!\\n\"))\n  }\n}\n\n# Install missing packages and load them\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n    cat_message(pkg, \"installed\")\n  } else {\n    cat_message(pkg, \"loading\")\n  }\n  library(pkg, character.only = TRUE)\n}\n\ndplyr is already installed and has been loaded!\n\n\nterra is already installed and has been loaded!\n\n\nsf is already installed and has been loaded!\n\n\ngoogledrive is already installed and has been loaded!\nggplot2 is already installed and has been loaded!\nleaflet is already installed and has been loaded!\nshiny is already installed and has been loaded!\nhtmltools is already installed and has been loaded!\n\n# De-authenticate Google Drive to access public files\ndrive_deauth()\n\n# If you are using renv, you can snapshot the renv after loading all the packages.\n\n#renv::snapshot()"
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#s.3-download-the-case-study-datasets",
    "href": "notebooks/data_prep/raster_preparation.html#s.3-download-the-case-study-datasets",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "S.3 Download the Case Study Datasets",
    "text": "S.3 Download the Case Study Datasets\nEnvironmental variables are downloaded from both WorldClim and Google Drive, and stored in GeoTIFF format.\n\n# Define Google Drive file ID and the path for downloading\nzip_file_id &lt;- \"1TD4WBLo7uzNjtWS2DXvS1PVCUzetxfqn\" # Replace with the actual file ID of the zipped file\n\ndatafolder_path &lt;- file.path(workspace)\n\n# Create a local path for the zipped file\nzip_file_path &lt;- file.path(datafolder_path, \"raster_preparation.zip\")\n\n# Function to download a file with progress messages\ndownload_zip_file &lt;- function(file_id, file_path) {\n  cat(\"Downloading zipped file...\\n\")\n  drive_download(as_id(file_id), path = file_path, overwrite = TRUE)\n  cat(\"Downloaded zipped file to:\", file_path, \"\\n\")\n}\n\n# Create local directory if it doesn't exist\nif (!dir.exists(datafolder_path)) {\n  dir.create(datafolder_path, recursive = TRUE)\n}\n\n# Download the zipped file\ncat(\"Starting to download the zipped file...\\n\")\n\nStarting to download the zipped file...\n\ndownload_zip_file(zip_file_id, zip_file_path)\n\nDownloading zipped file...\n\n\nFile downloaded:\n\n\n• 'rasters_files.zip' &lt;id: 1TD4WBLo7uzNjtWS2DXvS1PVCUzetxfqn&gt;\n\n\nSaved locally as:\n\n\n• '/Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/data_prep/raster_preparation.zip'\n\n\nDownloaded zipped file to: /Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/data_prep/raster_preparation.zip \n\n# Unzip the downloaded file\ncat(\"Unzipping the file...\\n\")\n\nUnzipping the file...\n\nunzip(zip_file_path, exdir = datafolder_path)\ncat(\"Unzipped files to folder:\", datafolder_path, \"\\n\")\n\nUnzipped files to folder: /Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/data_prep"
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#study-area",
    "href": "notebooks/data_prep/raster_preparation.html#study-area",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "1.1 Study area",
    "text": "1.1 Study area\nBefore processing raster or any other spatial data, it’s important to understand your study area. This helps you determine the spatial extent of the environmental variables to collect, whether it’s nationwide data (e.g., all of Australia) or localised data for a smaller area.\n\n# Load your shapefile\ncentral_highlands &lt;- st_read(\"rasters_files/central_highlands.shp\")\n\nReading layer `central_highlands' from data source \n  `/Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/data_prep/rasters_files/central_highlands.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 144.9398 ymin: -38.20964 xmax: 146.4563 ymax: -36.97746\nGeodetic CRS:  GDA94\n\n#check the CRS of the vect of the study area\nprint(crs(central_highlands))\n\n[1] \"GEOGCRS[\\\"GDA94\\\",\\n    DATUM[\\\"Geocentric Datum of Australia 1994\\\",\\n        ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101,\\n            LENGTHUNIT[\\\"metre\\\",1]]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"Australia including Lord Howe Island, Macquarie Island, Ashmore and Cartier Islands, Christmas Island, Cocos (Keeling) Islands, Norfolk Island. All onshore and offshore.\\\"],\\n        BBOX[-60.55,93.41,-8.47,173.34]],\\n    ID[\\\"EPSG\\\",4283]]\"\n\n\nFrom above, we can find the CRS of the study area shapefile is EPSG:4283, the unit of it is ‘degree’. To know more about this CRS, visit https://epsg.io/4283.\n\n# Custom CSS for a smaller legend box\ncustom_css &lt;- tags$style(HTML(\"\n  .leaflet-control .legend {\n    font-size: 8px !important; /* Reduce font size */\n    padding: 4px; /* Reduce padding inside the legend box */\n    line-height: 1; /* Reduce spacing between lines */\n    width: auto; /* Automatically size the legend box */\n    height: auto; /* Automatically size the legend box */\n  }\n  .leaflet-control .legend i {\n    width: 10px; /* Smaller legend icons */\n    height: 10px;\n  }\n\"))\n\n# Render the map\nleaflet() %&gt;%\n  addProviderTiles(providers$Esri.WorldImagery) %&gt;%\n  # Add the Central Highlands layer with a distinct color\n  addPolygons(\n    data = central_highlands,\n    color = \"lightblue\",         # Border color of Central Highlands polygon\n    weight = 1,                  # Border width\n    fillColor = \"lightblue\",     # Fill color of Central Highlands\n    fillOpacity = 0.3,           # Transparency for fill\n    group = \"Central Highlands\"\n  ) %&gt;%\n  setView(lng = 145.7, lat = -37.5, zoom = 7) %&gt;% # Set the view to desired location\n  addLegend(\n    position = \"bottomright\",\n    colors = c(\"lightblue\"),\n    labels = c(\"Central Highlands\"),\n    opacity = 0.7\n  ) %&gt;%\n  # Add the custom CSS to modify the legend font size\n  htmlwidgets::prependContent(custom_css)\n\nWarning: sf layer has inconsistent datum (+proj=longlat +ellps=GRS80 +no_defs).\nNeed '+proj=longlat +datum=WGS84'\n\n\n\n\n\n\n\nThis light blue area called Central Highlands in Victoria, Australia is our study area."
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#load-rasters",
    "href": "notebooks/data_prep/raster_preparation.html#load-rasters",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "1.2 Load Rasters",
    "text": "1.2 Load Rasters\nWe prepared 15 environmental raster files for this practice.\n\n\n\n\n\n\n\n\n\n\nVariable name\nVariables\nData Type\nResolution\nSource\n\n\n\n\n1 - AusClim_bioclim_01_9s_1976-2005_vic\nBioclim 01: Annual mean temperature\nContinuous\n9 seconds (9s)\n1976-2005, CSIRO via EcoCommons\n\n\n2 - AusClim_bioclim_04_9s_1976-2005_vic\nBioclim 04: Temperature Seasonality (standard deviation *100)\nContinuous\n9 seconds (9s)\n1976-2005, CSIRO via EcoCommons\n\n\n3 - AusClim_bioclim_06_9s_1976-2005_vic\nBioclim 06: Min Temperature of Coldest Month\nContinuous\n9 seconds (9s)\n1976-2005, CSIRO via EcoCommons\n\n\n4 - AusClim_bioclim_10_9s_1976-2005_vic\nBioclim 10: Mean Temperature of Warmest Quarter\nContinuous\n9 seconds (9s)\n1976-2005, CSIRO via EcoCommons\n\n\n5 - AusClim_bioclim_12_9s_1976-2005_vic\nBioclim 12: Annual Precipitation\nContinuous\n9 seconds (9s)\n1976-2005, CSIRO via EcoCommons\n\n\n6 - AusClim_bioclim_18_9s_1976-2005_vic\nBioclim 18: Precipitation of Warmest Quarter\nContinuous\n9 seconds (9s)\n1976-2005, CSIRO via EcoCommons\n\n\n7 - AusClim_bioclim_19_9s_1976-2005_vic\nBioclim 19: Precipitation of Coldest Quarter\nContinuous\n9 seconds (9s)\n1976-2005, CSIRO via EcoCommons\n\n\n8 - AusClim_bioclim_20_9s_1976-2005_vic\nBioclim 20: Annual mean radiation\nContinuous\n9 seconds (9s)\n1976-2005, CSIRO via EcoCommons\n\n\n9 - AusClim_bioclim_28_9s_1976-2005_vic\nBioclim 28: Annual mean moisture index\nContinuous\n9 seconds (9s)\n1976-2005, CSIRO via EcoCommons\n\n\n10 - AusClim_bioclim_34_9s_1976-2005_vic\nBioclim 34: Mean moisture index of warmest quarter\nContinuous\n9 seconds (9s)\n1976-2005, CSIRO via EcoCommons\n\n\n11 - AusClim_bioclim_35_9s_1976-2005_vic\nBioclim 35: Mean moisture index of coldest quarter\nContinuous\n9 seconds (9s)\n1976-2005, CSIRO via EcoCommons\n\n\n12 - digital_elevation_model_3s_2009_3s_vic\nDigital Elevation Model\nContinuous\n3 seconds (3s)\nGeoscience Australia via EcoCommons\n\n\n13 - Fire_events_projection_area_vic\nFires\nContinuous\n1 km\nVic DEECA spatial data and resources\n\n\n14 - Logging_1km_vic\nLogging\nContinuous\n1 km\nVic DEECA spatial data and resources\n\n\n15 - nsg-2011-250m_asc_vic\nAustralian Soil Classification\nCategorical\n250 m\nTern via EcoCommons\n\n\n\nUsually, you are the person who collects raster files from different data platform or other researchers. Apart from the content of the raster, you should know the data type (categorical or continuous), resolution (units in degree or meter/kilometers), and Coordinate Reference System (CRS, which also determines the unit of the raster).\n\n# Define the folder path\ndatafolder_path &lt;- \"rasters_files\"\n\n# List all .tif files with full paths\nfile_paths &lt;- list.files(path = datafolder_path, pattern = \"\\\\.tif$\", full.names = TRUE)\n\n# Load all rasters and store in a list\nrasters &lt;- lapply(file_paths, rast)\n\n# Assign names to the raster list using file names without extensions\nnames(rasters) &lt;- tools::file_path_sans_ext(basename(file_paths))\n\n# Print numbers before each raster name\nfor (i in seq_along(names(rasters))) {\n  cat(i, \"-\", names(rasters)[i], \"\\n\")\n}\n\n1 - AusClim_bioclim_01_9s_1976-2005_vic \n2 - AusClim_bioclim_04_9s_1976-2005_vic \n3 - AusClim_bioclim_06_9s_1976-2005_vic \n4 - AusClim_bioclim_10_9s_1976-2005_vic \n5 - AusClim_bioclim_12_9s_1976-2005_vic \n6 - AusClim_bioclim_18_9s_1976-2005_vic \n7 - AusClim_bioclim_19_9s_1976-2005_vic \n8 - AusClim_bioclim_20_9s_1976-2005_vic \n9 - AusClim_bioclim_28_9s_1976-2005_vic \n10 - AusClim_bioclim_34_9s_1976-2005_vic \n11 - AusClim_bioclim_35_9s_1976-2005_vic \n12 - digital_elevation_model_3s_2009_3s_vic \n13 - Fire_events_projection_area_vic \n14 - Logging_1km_vic \n15 - nsg-2011-250m_asc_vic \n\n\nNow, let’s check the data type of each raster.\n\n# Check if each raster is categorical or continuous\nfor (i in seq_along(rasters)) {\n  if (is.factor(rasters[[i]])) {\n    cat(names(rasters[[i]]), \"is categorical.\\n\")\n  } else {\n    cat(names(rasters[[i]]), \"is continuous.\\n\")\n  }\n}\n\nAusClim_bioclim_01_9s_1976-2005 is continuous.\nAusClim_bioclim_04_9s_1976-2005 is continuous.\nAusClim_bioclim_06_9s_1976-2005 is continuous.\nAusClim_bioclim_10_9s_1976-2005 is continuous.\nAusClim_bioclim_12_9s_1976-2005 is continuous.\nAusClim_bioclim_18_9s_1976-2005 is continuous.\nAusClim_bioclim_19_9s_1976-2005 is continuous.\nAusClim_bioclim_20_9s_1976-2005 is continuous.\nAusClim_bioclim_28_9s_1976-2005 is continuous.\nAusClim_bioclim_34_9s_1976-2005 is continuous.\nAusClim_bioclim_35_9s_1976-2005 is continuous.\ndigital_elevation_model_3s_2009_3s is continuous.\nFire_events_projection_area is continuous.\nLogging_1km is continuous.\nnsg-2011-250m_asc is continuous.\n\n\nAustralian Soil Classification (nsg-2011-250m_asc_vic) should be categorical, however, it is shown as continuous. Why is that? Here are some possible reasons:\n\nIncorrect Data Type Interpretation:\n\nThe software or library you are using may be interpreting the soil classification values as continuous numbers instead of categories (factors or levels).\n\nRaster Encoding:\n\nCategorical data in raster format is typically stored with integer values representing categories. If the raster metadata does not correctly define these as categories, the software may interpret them as continuous.\n\nMissing Attribute Table:\n\nCategorical rasters often have an attribute table linking integer values to class names. If this table is missing or not recognized, the values may appear as continuous.\n\nData Export or Conversion Issue:\n\nIf the raster was exported or converted incorrectly (e.g., saved as a floating-point raster), it could cause the values to be treated as continuous.\n\n\nSometimes, after manipulation, a raster may lose its categorical (factor) properties due to how terra (the R package) or other spatial libraries interpret the file upon reloading. Rasters saved as categorical may be stored with integer values rather than explicit factor levels in the file, meaning they need to be redefined as factors when reloaded.\n\n# we can factorize Australian Soil Classification (nsg-2011-250m_asc_vic) to make sure it is categorical.\n\nrasters[[15]] &lt;- as.factor(rasters[[15]])\n\n# Check if each raster is categorical or continuous\nfor (i in seq_along(rasters)) {\n  if (is.factor(rasters[[i]])) {\n    cat(names(rasters[[i]]), \"is categorical.\\n\")\n  } else {\n    cat(names(rasters[[i]]), \"is continuous.\\n\")\n  }\n}\n\nAusClim_bioclim_01_9s_1976-2005 is continuous.\nAusClim_bioclim_04_9s_1976-2005 is continuous.\nAusClim_bioclim_06_9s_1976-2005 is continuous.\nAusClim_bioclim_10_9s_1976-2005 is continuous.\nAusClim_bioclim_12_9s_1976-2005 is continuous.\nAusClim_bioclim_18_9s_1976-2005 is continuous.\nAusClim_bioclim_19_9s_1976-2005 is continuous.\nAusClim_bioclim_20_9s_1976-2005 is continuous.\nAusClim_bioclim_28_9s_1976-2005 is continuous.\nAusClim_bioclim_34_9s_1976-2005 is continuous.\nAusClim_bioclim_35_9s_1976-2005 is continuous.\ndigital_elevation_model_3s_2009_3s is continuous.\nFire_events_projection_area is continuous.\nLogging_1km is continuous.\nnsg-2011-250m_asc is categorical.\n\n\nNow, it looks correct."
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#plot-rasters-to-understand-your-data",
    "href": "notebooks/data_prep/raster_preparation.html#plot-rasters-to-understand-your-data",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "1.3 Plot Rasters to Understand Your Data",
    "text": "1.3 Plot Rasters to Understand Your Data\n\nlayer_titles &lt;- c(\n   \"Bioclim 01: Annual Mean Temperature\",\n   \"Bioclim 04: Temperature Seasonality (standard deviation *100)\",\n   \"Bioclim 06: Min Temperature of Coldest Month\",\n   \"Bioclim 10: Mean Temperature of Warmest Quarter\",\n   \"Bioclim 20: Annual mean radiation\",\n   \"Bioclim 12: Annual Precipitation\",\n   \"Bioclim 18: Precipitation of Warmest Quarter\",\n   \"Bioclim 19: Precipitation of Coldest Quarter\",\n   \"Bioclim 28: Annual Mean Moisture Index\",\n   \"Bioclim 34: Mean Moisture Index of Warmest Quarter\",\n   \"Bioclim 35: Mean Moisture Index of Coldest Quarter\",\n   \"Digital Elevation Model\",\n   \"Logging\",\n   \"Fires\",\n   \"Australian Soil Classification\"\n )\n\nfor (i in seq_along(rasters)) {\n  plot(rasters[[i]], main = layer_titles[i], col = terrain.colors(20))  # Plot raster\n  plot(central_highlands, add = TRUE, border = \"red\", weight = 1,\n  col = NA, lwd = 1.5)         # Overlay shapefile\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is great that all raster layers are sufficiently larger (but not too large) than our study area. This means we won’t have any data gaps or overly large raster layers to manage."
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#check-crs-extent-and-resolution-of-each-raster",
    "href": "notebooks/data_prep/raster_preparation.html#check-crs-extent-and-resolution-of-each-raster",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "1.4 Check CRS, Extent, and Resolution of Each Raster",
    "text": "1.4 Check CRS, Extent, and Resolution of Each Raster\n\n# Iterate over the rasters to show resolution, extent, and CRS\nfor (i in seq_along(rasters)) {\n  cat(\"\\nRaster\", i, \":\\n\")\n  \n  # Show the resolution\n  res_val &lt;- res(rasters[[i]])\n  cat(\"Resolution (x, y):\", res_val[1], \",\", res_val[2], \"\\n\")\n  \n  # Show the extent - Modified to handle S4 object\n  ext_val &lt;- ext(rasters[[i]])\n  cat(\"Extent:\", as.character(ext_val), \"\\n\") # Convert ext_val to character\n  \n  # Show the CRS\n  crs_val &lt;- crs(rasters[[i]])\n  \n  # Extract all occurrences of ID[\"EPSG\",...] using gregexpr\n  id_matches &lt;- regmatches(crs_val, gregexpr(\"ID\\\\[\\\"EPSG\\\",[0-9]+\\\\]\", crs_val))\n  \n  # Select the last match\n  last_id &lt;- tail(unlist(id_matches), 1)\n  \n  # Print the result\n  cat(\"CRS:\", last_id, \"\\n\")\n}\n\n\nRaster 1 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\nRaster 2 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\nRaster 3 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\nRaster 4 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\nRaster 5 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\nRaster 6 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\nRaster 7 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\nRaster 8 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\nRaster 9 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\nRaster 10 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\nRaster 11 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\nRaster 12 :\nResolution (x, y): 0.0008333333 , 0.0008333333 \nExtent: ext(140.962083333851, 149.976250000663, -39.1595833336029, -33.9804166668527) \nCRS: ID[\"EPSG\",4283] \n\nRaster 13 :\nResolution (x, y): 0.00899273 , 0.008983703 \nExtent: ext(142.799993603565, 149.976192429999, -39.1591895299991, -36.500013311383) \nCRS: ID[\"EPSG\",4283] \n\nRaster 14 :\nResolution (x, y): 0.009 , 0.009 \nExtent: ext(142.79785887, 149.97985887, -39.1591895299999, -36.4951895299999) \nCRS: ID[\"EPSG\",4283] \n\nRaster 15 :\nResolution (x, y): 0.0025 , 0.0025 \nExtent: ext(140.9625, 149.975, -39.16, -33.98) \nCRS: ID[\"EPSG\",4283] \n\n\nCRS: All the raster layers have the same CRS, which is EPSG: 4283.\nExtent: Raster 1-11 and 15 have the same extent, the rest raster layers have different extents. All the extents of these raster files are sufficiently larger then the extent of the study area. So we don’t have to worry too much on this.\nResolution: Raster 1-11 and 15 have the same resolution, which is 0.0025 degree (9 seconds), the rest raster layers have different resolutions.\nWe need to unify extent and resolutions."
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#unify-crs-and-resolution",
    "href": "notebooks/data_prep/raster_preparation.html#unify-crs-and-resolution",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "1.5 Unify CRS and Resolution",
    "text": "1.5 Unify CRS and Resolution\nAs we know, the CRS of the shapefile of the study area and all the raster files is EPSG:4283 (unit in degree). What if I want to change the CRS of these files to EPSG: 28355 (GDA94 / MAG Zone 55, unit in meter), a more precise CRS for our study area (why? read this article by Paul Wintour).\n\n# Function to check CRS and reproject to EPSG: 28355\nreproject_if_needed &lt;- function(raster, is_categorical = FALSE) {\n  EPSG_28255 &lt;- \"EPSG:28355\"  # Define EPSG: 28355\n\n  # Check if the CRS is already EPSG:28355\n  if (!identical(crs(raster), EPSG_28255)) {\n    # Choose the appropriate method based on raster type\n    method &lt;- if (is_categorical) \"near\" else \"bilinear\" # see note below this chunk\n    message(\"Reprojecting raster to EPSG:28355 using method: \", method)\n\n    # Reproject raster to EPSG:28355\n    raster &lt;- project(raster, EPSG_28255, method = method)\n\n    # Convert to factor again if categorical to ensure category levels are preserved\n    if (is_categorical) {\n      raster &lt;- as.factor(raster)\n    }\n  }\n\n  return(raster)\n}\n\nDifferent interpolation methods are appropriate for different types of data:\n\n“near” (nearest neighbor interpolation): Used for categorical data (e.g., land cover types or classifications) because it preserves discrete class values.\n“bilinear” (bilinear interpolation): Used for continuous data (e.g., temperature, elevation) because it smooths values by averaging nearby points.\n\n\n# It is not a bad idea to check again whether your raster files are in the right data type.\n\n# Check if each raster is categorical or continuous\nfor (i in seq_along(rasters)) {\n  if (is.factor(rasters[[i]])) {\n    cat(names(rasters[[i]]), \"is categorical.\\n\")\n  } else {\n    cat(names(rasters[[i]]), \"is continuous.\\n\")\n  }\n}\n\nAusClim_bioclim_01_9s_1976-2005 is continuous.\nAusClim_bioclim_04_9s_1976-2005 is continuous.\nAusClim_bioclim_06_9s_1976-2005 is continuous.\nAusClim_bioclim_10_9s_1976-2005 is continuous.\nAusClim_bioclim_12_9s_1976-2005 is continuous.\nAusClim_bioclim_18_9s_1976-2005 is continuous.\nAusClim_bioclim_19_9s_1976-2005 is continuous.\nAusClim_bioclim_20_9s_1976-2005 is continuous.\nAusClim_bioclim_28_9s_1976-2005 is continuous.\nAusClim_bioclim_34_9s_1976-2005 is continuous.\nAusClim_bioclim_35_9s_1976-2005 is continuous.\ndigital_elevation_model_3s_2009_3s is continuous.\nFire_events_projection_area is continuous.\nLogging_1km is continuous.\nnsg-2011-250m_asc is categorical.\n\n# if not, factorize the categirical raster file manually\n# rasters[[15]] &lt;- as.factor(rasters[[15]])\n\n\n# Define the categorical raster indices\ncategorical_indices &lt;- c(15)  # Adjust if there are other categorical rasters\n\n# Apply the function to each raster in the list\nrasters_reprojected &lt;- lapply(seq_along(rasters), function(i) {\n  is_categorical &lt;- i %in% categorical_indices\n  reproject_if_needed(rasters[[i]], is_categorical = is_categorical)\n})\n\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nReprojecting raster to EPSG:28355 using method: bilinear\nReprojecting raster to EPSG:28355 using method: bilinear\n\n\nReprojecting raster to EPSG:28355 using method: near\n\n\nLet’s check the properties of all raster files.\n\n# Iterate over the reprojected rasters to show resolution, extent, and CRS\nfor (i in seq_along(rasters_reprojected)) {\n  cat(\"\\nRaster\", i, \":\\n\")\n  \n  # Show the resolution\n  res_val &lt;- res(rasters_reprojected[[i]])\n  cat(\"Resolution (x, y):\", res_val[1], \",\", res_val[2], \"\\n\")\n  \n  # Show the extent - Modified to handle S4 object\n  ext_val &lt;- ext(rasters_reprojected[[i]])\n  cat(\"Extent:\", as.character(ext_val), \"\\n\") # Convert ext_val to character\n  \n  # Show the CRS\n  crs_val &lt;- crs(rasters_reprojected[[i]])\n  \n  # Extract all occurrences of ID[\"EPSG\",...] using gregexpr\n  id_matches &lt;- regmatches(crs_val, gregexpr(\"ID\\\\[\\\"EPSG\\\",[0-9]+\\\\]\", crs_val))\n  \n  # Select the last match\n  last_id &lt;- tail(unlist(id_matches), 1)\n  \n  # Print the result\n  cat(\"CRS:\", last_id, \"\\n\")\n}\n\n\nRaster 1 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458765, 774777.333003385, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\nRaster 2 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458765, 774777.333003385, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\nRaster 3 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458765, 774777.333003385, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\nRaster 4 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458765, 774777.333003385, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\nRaster 5 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458765, 774777.333003385, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\nRaster 6 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458765, 774777.333003385, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\nRaster 7 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458765, 774777.333003385, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\nRaster 8 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458765, 774777.333003385, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\nRaster 9 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458765, 774777.333003385, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\nRaster 10 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458765, 774777.333003385, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\nRaster 11 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458765, 774777.333003385, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\nRaster 12 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 13 :\nResolution (x, y): 818.9801 , 818.9801 \nExtent: ext(123768.122970817, 766667.490018216, 5656743.63433381, 5960585.24602118) \nCRS: ID[\"EPSG\",28355] \n\nRaster 14 :\nResolution (x, y): 819.8114 , 819.8114 \nExtent: ext(123553.393091346, 767105.315260186, 5656970.16891723, 5961120.18563652) \nCRS: ID[\"EPSG\",28355] \n\nRaster 15 :\nResolution (x, y): 238.1606 , 238.1606 \nExtent: ext(-58070.4168458717, 774777.333003388, 5647993.85079617, 6240061.19598497) \nCRS: ID[\"EPSG\",28355] \n\n\nAs we can see, we unified the CRS of all raster files to EPSG:28355. We can also notice that the figures in resolution and extent have changed. That is because the unit of the CRS has changed from degree to meter."
  },
  {
    "objectID": "notebooks/data_prep/raster_preparation.html#resample-rasters-to-match-the-finest-resolution",
    "href": "notebooks/data_prep/raster_preparation.html#resample-rasters-to-match-the-finest-resolution",
    "title": "Species Distribution Analysis - Environmental Data Preparation (Raster)",
    "section": "1.6 Resample Rasters to Match the Finest Resolution",
    "text": "1.6 Resample Rasters to Match the Finest Resolution\nThere are two methods for unifying the resolutions of raster files: (1) resampling to the finest resolution or (2) resampling to the coarsest resolution. Resampling to the finest resolution preserves as much information as possible from the original raster files but increases the file size and processing time. On the other hand, resampling to the coarsest resolution significantly reduces the file size and processing time, though it may result in some loss of detail.\nIn EcoCommons, we recommend using the finest resolution when the objective of the study is well known, and coarser resolutions for preliminary / pilot study. The decision of fine vs coarse also depends on your study area, species and data availability. For example, when studying large, mobile animals such as whales or elephants, resampling all your raster files—from, say, 1 km, 5 km, 250 m, and 5 m to 5 m—would result in an unnecessary computational burden and waste of resources.\n\n# Determine the finest resolution among the reprojected rasters\nresolutions &lt;- sapply(rasters_reprojected, function(r) res(r)[1] * res(r)[2])\nfinest_index &lt;- which.min(resolutions)\nfinest_raster &lt;- rasters_reprojected[[finest_index]]\n\n# Resample each raster to match the finest resolution\nrasters_resampled &lt;- lapply(seq_along(rasters_reprojected), function(i) {\n  raster &lt;- rasters_reprojected[[i]]\n\n  # Determine resampling method based on the type of raster (categorical vs continuous)\n  if (i == 15) {  # the 15th raster is the categorical one\n    message(\"Resampling categorical raster to match the finest resolution using 'nearest' method.\")\n    raster &lt;- resample(raster, finest_raster, method = \"near\")  # Use 'near' for categorical data\n  } else {\n    message(\"Resampling continuous raster to match the finest resolution using 'bilinear' method.\")\n    raster &lt;- resample(raster, finest_raster, method = \"bilinear\")  # Use 'bilinear' for continuous data\n  }\n\n  return(raster)\n})\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling continuous raster to match the finest resolution using 'bilinear' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nResampling categorical raster to match the finest resolution using 'nearest' method.\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nWarning: [resample] detected values outside of the limits of datatype INT4S\n\n\n\n# Iterate over the reprojected rasters to show resolution, extent, and CRS\nfor (i in seq_along(rasters_resampled)) {\n  cat(\"\\nRaster\", i, \":\\n\")\n  \n  # Show the resolution\n  res_val &lt;- res(rasters_resampled[[i]])\n  cat(\"Resolution (x, y):\", res_val[1], \",\", res_val[2], \"\\n\")\n  \n  # Show the extent - Modified to handle S4 object\n  ext_val &lt;- ext(rasters_resampled[[i]])\n  cat(\"Extent:\", as.character(ext_val), \"\\n\") # Convert ext_val to character\n  \n  # Show the CRS\n  crs_val &lt;- crs(rasters_resampled[[i]])\n  \n  # Extract all occurrences of ID[\"EPSG\",...] using gregexpr\n  id_matches &lt;- regmatches(crs_val, gregexpr(\"ID\\\\[\\\"EPSG\\\",[0-9]+\\\\]\", crs_val))\n  \n  # Select the last match\n  last_id &lt;- tail(unlist(id_matches), 1)\n  \n  # Print the result\n  cat(\"CRS:\", last_id, \"\\n\")\n}\n\n\nRaster 1 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 2 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 3 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 4 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 5 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 6 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 7 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 8 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 9 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 10 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 11 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 12 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 13 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 14 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\nRaster 15 :\nResolution (x, y): 79.38445 , 79.38445 \nExtent: ext(-58106.2476755253, 774954.201024326, 5648124.89827636, 6240015.37907549) \nCRS: ID[\"EPSG\",28355] \n\n\nNow, the resolution has been unified to 79.4 meters."
  },
  {
    "objectID": "notebooks/sp/ecocommons-marxan-integration-poc.html",
    "href": "notebooks/sp/ecocommons-marxan-integration-poc.html",
    "title": "EcoCommons -> Marxan MaPP connection",
    "section": "",
    "text": "Author: Zhao Xiang, EcoCommons\nDate: 2024-10-02\n\n\n\n\n\n\n\nUsing the Species distribution modeling techniques provided by the EcoCommons Platform (www.ecocommons.org.au), we produced probability distribution maps for the three Queensland endangered species: koala, brush tailed rock-wallaby, and beach stone curlew.\nThen we adjusted the probability distribution maps of these three species with the planning units shapefile prepared by the Marxan MaPP, and ran four planning scenarios with a target of expanding the coverage of protected areas in QLD to 30%.\n\n\n\n\nSpecies records pulled from GBIF, ALA, EcoPlots, OBIS\nSpecies distribution modelling output: Species distribution Probability maps (This is the input tested in this project).\n\n\n\n\n\nShapefile of planning area and units.\nShapefile of cost.\nShapefile and csv of biodiversity features (Where EcoCommons can help!).\n\n\n\n\nMake sure you are in the directory you want\n\ngetwd()\n\n[1] \"/Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/sp\"\n\n# setwd(“/replace_this_with_your_actual_directory/ecocommons-marxan-integration-poc”)\n\n\n# Set CRAN mirror\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\n# List of packages to check, install if needed, and load\npackages &lt;- c(\"shiny\", \"sf\", \"terra\", \"ggplot2\", \"ggspatial\", \"raster\", \"dplyr\", \"httpuv\", \"rmarkdown\", \"knitr\", \"jsonlite\", \"reticulate\", \"htmltools\", \"pryr\")\n\n# Function to display a cat message\ncat_message &lt;- function(pkg, message_type) {\n  if (message_type == \"installed\") {\n    cat(paste0(pkg, \" has been installed successfully!\\n\"))\n  } else if (message_type == \"loading\") {\n    cat(paste0(pkg, \" is already installed and has been loaded!\\n\"))\n  }\n}\n\n# Install missing packages and load them\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n    cat_message(pkg, \"installed\")\n  } else {\n    cat_message(pkg, \"loading\")\n  }\n  library(pkg, character.only = TRUE)\n}\n\nshiny is already installed and has been loaded!\nsf is already installed and has been loaded!\n\n\nterra is already installed and has been loaded!\n\n\nggplot2 is already installed and has been loaded!\nggspatial is already installed and has been loaded!\nraster is already installed and has been loaded!\n\n\ndplyr is already installed and has been loaded!\n\n\nhttpuv is already installed and has been loaded!\nrmarkdown is already installed and has been loaded!\nknitr is already installed and has been loaded!\n\n\njsonlite is already installed and has been loaded!\n\n\n\nThe downloaded binary packages are in\n    /var/folders/r7/3wfwvlts0g52zbkjyt24s_nr0000gp/T//Rtmp4RTdHE/downloaded_packages\nreticulate has been installed successfully!\nhtmltools is already installed and has been loaded!\n\n\n\nThe downloaded binary packages are in\n    /var/folders/r7/3wfwvlts0g52zbkjyt24s_nr0000gp/T//Rtmp4RTdHE/downloaded_packages\npryr has been installed successfully!\n\n\n1. We get the QLD planning units from Marxan MaPP\n\nQLD_Unit &lt;- \"qld_3species_Marxan/QLD_plannningunits/cost-surface-template.shp\"  #This cost-surface-template was prepared by the Marxan Mapp with a resolution of 189 Km2, which is the highest resolution Marxan Mapp can give at this scale.\n\nQLD_Unit  &lt;- st_read(QLD_Unit)\nQLD_Unit  &lt;- st_simplify(QLD_Unit , dTolerance = 0.01) \n\n\n# Calculate the resolution since Marxan MaPP for visulization purpose\nareas &lt;- st_area(QLD_Unit)\nareas_numeric &lt;- as.numeric(areas)\naverage_area &lt;- mean(areas_numeric)\n\n# Convert to numeric\naverage_area_km2 &lt;- average_area / 1e6\n\n# Get the number of rows\nn_rows &lt;- nrow(QLD_Unit)\n\n# Plot the shapefile with no fill color and number of rows in the title\nggplot(data = QLD_Unit) +\n  geom_sf(fill = NA, color = \"gray\") +\n  theme_minimal() +\n  ggtitle(paste(\"QLD Planning Units:\", n_rows, \"\\n\",\n                \"Resolution of planning in square kilometers:\", round(average_area_km2)))+\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n\n\n\n\n\n\n2. I made a cost layer using the reciprocal of the distance to state-owned road as a surrogate of the cost.\nThe assumption is: the closer to the state owned road, the more expensive to purchase the unit.\n\nQLD_cost_road &lt;- st_read(\"qld_3species_Marxan/QLD_Cost/QLD_cost_road.shp\")\n\n# Plot the shapefile with continuous cost_road values\nggplot(QLD_cost_road) +\n  geom_sf(aes(fill = cost_road)) +\n  scale_fill_continuous(name = \"Cost\",\n                        low = \"lightblue\", high = \"red\",\n                        labels = c(\"0 (Low cost)\", \"1 (High cost)\"),\n                        breaks = c(0.01, 1)) +\n  theme_minimal() +\n  labs(title = \"Cost: using the distance to road of each Unit as a proxy\")+\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n\n\n\n\n\n\n3. Biodiversity features. I used EcoCommons to produce three species’ SDM to start with.\n\nSpecies 1: koala\nSpecies 2: brush tailed rock-wallaby\nSpecies 3: beach stone curlew\n\n\n# Define the folder path where the rasters are stored\nfolder_path &lt;- \"qld_3species_Marxan/QLD_feature/\"\n\n# Get a list of all .tif files in the folder\nraster_files &lt;- list.files(path = folder_path, pattern = \"\\\\.tif$\", full.names = TRUE)\n\n# Extract the species names from the file names (removing the folder path and .tif extension)\nspecies_names &lt;- tools::file_path_sans_ext(basename(raster_files))\n\n# Read all raster files in one go using lapply\nraster_list &lt;- lapply(raster_files, rast)  # Use rast() from terra for reading rasters\n\n# Using QLD_Unit as the spatial vector for masking\n\n# Transform the raster CRS to match the vector CRS and apply masking in one step\nraster_list &lt;- lapply(raster_list, function(r) {\n  r_transformed &lt;- project(r, crs(vect(QLD_Unit)))\n  mask(r_transformed, vect(QLD_Unit))\n})\n\n# Function to convert rasters to data frames and combine them\nprepare_raster_data &lt;- function(raster_list, species_names) {\n\n  # Initialize an empty data frame\n  combined_df &lt;- data.frame()\n  # Loop through each raster and combine them into one data frame\n  for (i in seq_along(raster_list)) {\n    # Convert raster to a data frame\n    raster_df &lt;- as.data.frame(raster_list[[i]], xy = TRUE)\n    # Rename the third column to 'value' or any appropriate name for the raster values\n    names(raster_df)[3] &lt;- \"value\"\n    # Add a column to identify the species name\n    raster_df$species &lt;- species_names[i]\n    # Combine the raster data with the overall data frame\n    combined_df &lt;- bind_rows(combined_df, raster_df)\n}\n  return(combined_df)\n}\n\n# Prepare the combined data frame\ncombined_raster_df &lt;- prepare_raster_data(raster_list, species_names)\n\n\n# Create the ggplot with facet_wrap to display each raster in a separate facet\nggplot(combined_raster_df, aes(x = x, y = y, fill = value)) +  # Use the correct column name for fill\n  geom_raster()+\n  facet_wrap(~ species, ncol = 3) +  # Adjust ncol to control the number of columns\n  scale_fill_viridis_c() +  # You can adjust the color scale as needed\n  labs(title = \"Species SDM\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))+\n  coord_fixed()  # Maintain the correct aspect ratio\n\n\n\n\n\n\n\n\n4. We need to turn these SDMs to binary results (shapefies).\n\n# Define the folder paths where the rasters and CSV files are stored\nfolder_path_rasters &lt;- \"qld_3species_Marxan/QLD_feature/\"\nfolder_path_csvs &lt;- \"qld_3species_Marxan/model_evaluation/\"\noutput_dir &lt;- \"qld_3species_Marxan/QLD_feature/Marxan_feature_input/\"\n\nQLD_Unit &lt;- \"qld_3species_Marxan/QLD_plannningunits/cost-surface-template.shp\"  #This cost-surface-template was prepared by the Marxan Mapp with a resolution of 189 Km2, which is the highest resolution Marxan Mapp can give at this scale.\n\nQLD_Unit  &lt;- st_read(QLD_Unit)\nQLD_Unit  &lt;- st_simplify(QLD_Unit , dTolerance = 0.01) \n\n\n# Get a list of all .tif files and CSV files in the folder\nraster_files &lt;- list.files(path = folder_path_rasters, pattern = \"\\\\.tif$\", full.names = TRUE)\ncsv_files &lt;- list.files(path = folder_path_csvs, pattern = \"\\\\.csv$\", full.names = TRUE)\n\n# Extract the species names from the file names (removing the folder path and .tif/.csv extension)\nspecies_names &lt;- tools::file_path_sans_ext(basename(raster_files))\n\n# Read all raster files in one go using lapply\nraster_list &lt;- lapply(raster_files, rast)  # Use rast() from terra for reading rasters\n\n# Transform the raster CRS to match the vector CRS and apply masking in one step\nraster_list &lt;- lapply(raster_list, function(r) {\n  r_transformed &lt;- project(r, crs(vect(QLD_Unit)))\n  mask(r_transformed, vect(QLD_Unit))\n})\n\n# Prepare a named list of rasters\nspecies_rasters &lt;- setNames(raster_list, species_names)\nspecies_csvs &lt;- setNames(csv_files, species_names)\n\n# Define UI for the application\nui &lt;- fluidPage(\n  titlePanel(\"Interactive TSS-based threshold for the probability of presence and absence of Species\"),\n  \n  # Use a loop to create a row for each species\n  lapply(species_names, function(species) {\n    fluidRow(\n      column(3, \n             h4(paste(\"Species:\", species)),\n             sliderInput(paste0(\"tss_value_\", species), \n                         \"Select TSS Value:\", \n                         min = 0, max = 1, value = 0.5, step = 0.01),\n             actionButton(paste0(\"run_analysis_\", species), \"Run Species Analysis\"),\n             br(),\n             textOutput(paste0(\"tpr_tnr_\", species))\n      ),\n      \n      column(4, \n             plotOutput(paste0(\"plot_\", species), width = \"400px\")\n      ),\n      \n      column(5, \n             plotOutput(paste0(\"species_plot_\", species))\n      )\n    )\n  })\n)\n\n# Define server logic\nserver &lt;- function(input, output, session) {\n  \n  selected_raster &lt;- function(species) {\n    species_rasters[[species]]\n  }\n  \n  species_eval_data &lt;- function(species) {\n    csv_path &lt;- species_csvs[[species]]\n    \n    if (!file.exists(csv_path)) {\n      showNotification(paste(\"CSV file for\", species, \"not found!\"), type = \"error\")\n      return(NULL)\n    }\n    \n    eval_data &lt;- read.csv(csv_path)\n    \n    if (!all(c(\"tpr\", \"tnr\", \"tpv\") %in% names(eval_data))) {\n      showNotification(paste(\"Required columns missing in CSV for\", species), type = \"error\")\n      return(NULL)\n    }\n    \n    if (nrow(eval_data) == 0) {\n      showNotification(paste(\"No data found in CSV for\", species), type = \"error\")\n      return(NULL)\n    }\n    \n    eval_data$tss &lt;- round(eval_data$tpr + eval_data$tnr - 1, 3)\n    return(eval_data)\n  }\n  \n  lapply(species_names, function(species) {\n    eval_data &lt;- species_eval_data(species)\n    \n    if (!is.null(eval_data)) {\n      min_tss &lt;- min(eval_data$tss, na.rm = TRUE)\n      max_tss &lt;- max(eval_data$tss, na.rm = TRUE)\n      \n      updateSliderInput(session, paste0(\"tss_value_\", species), \n                        min = min_tss, \n                        max = max_tss, \n                        value = max_tss,\n                        step = 0.01)\n    }\n    \n    observeEvent(input[[paste0(\"tss_value_\", species)]], {\n      if (!is.null(eval_data)) {\n        row &lt;- which.min(abs(eval_data$tss - input[[paste0(\"tss_value_\", species)]]))\n        \n        tpr &lt;- eval_data$tpr[row]\n        tnr &lt;- eval_data$tnr[row]\n        \n        output[[paste0(\"tpr_tnr_\", species)]] &lt;- renderText({\n          paste0(\"TPR (Sensitivity): \", round(tpr, 3), \n                 \", TNR (Specificity): \", round(tnr, 3))\n        })\n      }\n    })\n    \n    output[[paste0(\"plot_\", species)]] &lt;- renderPlot({\n      if (is.null(eval_data)) return(NULL)\n      \n      ggplot(eval_data, aes(x = tpv)) +\n        geom_line(aes(y = tpr, colour = \"TPR\"), linewidth = 1) +\n        geom_line(aes(y = tnr, colour = \"TNR\"), linewidth = 1) +\n        geom_line(aes(y = tss, colour = \"TSS\"), linewidth = 1) +\n        geom_vline(xintercept = eval_data$tpv[which.min(abs(eval_data$tss - input[[paste0(\"tss_value_\", species)]]))],\n                   linetype = \"dotted\", color = \"red\", linewidth = 1) +\n        labs(title = paste(\"Sensitivity, Specificity, and TSS for\", species),\n             x = \"Threshold Probability Value\",\n             y = \"Value\") +\n        scale_colour_manual(values = c(\"TPR\" = \"blue\", \"TNR\" = \"green\", \"TSS\" = \"red\")) +\n        theme_minimal()\n    })\n    \n    observeEvent(input[[paste0(\"run_analysis_\", species)]], {\n      species_shp &lt;- process_species(selected_raster(species), QLD_Unit, species, output_dir, input[[paste0(\"tss_value_\", species)]])\n      \n      output[[paste0(\"species_plot_\", species)]] &lt;- renderPlot({\n        ggplot() +\n          geom_sf(data = QLD_Unit, fill = NA, color = \"grey\") +\n          geom_sf(data = species_shp, aes(fill = feature), color = NA) +\n          scale_fill_viridis_c(option = \"plasma\") +\n          labs(title = paste(\"Species Distribution for\", species),\n               x = \"Longitude\", y = \"Latitude\") +\n          theme_minimal()\n      })\n    })\n  })\n}\n\nprocess_species &lt;- function(raster_data, planning_unit, species_name, output_dir, tss_threshold) {\n  raster_data_transformed &lt;- project(raster_data, crs(vect(planning_unit)))\n  extracted_values &lt;- extract(raster_data_transformed, vect(planning_unit), fun = mean, na.rm = TRUE)\n  names(planning_unit)[names(planning_unit) == \"cost\"] &lt;- \"feature\"\n  planning_unit$feature &lt;- extracted_values[, 2]\n  \n  QLD_species &lt;- subset(planning_unit, feature &gt;= tss_threshold)\n  shapefile_base &lt;- file.path(output_dir, species_name)\n  st_write(QLD_species, paste0(shapefile_base, \".shp\"), delete_layer = TRUE)\n  \n  return(QLD_species)\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n5. Plot species SDM binary shapefile outputs for double check\n\noutput_dir &lt;- \"qld_3species_Marxan/QLD_feature/Marxan_feature_input/\"\n\n# List all the shapefiles in the directory (assuming each species has its own shapefile)\nspecies_files &lt;- list.files(output_dir, pattern = \"\\\\.shp$\", full.names = TRUE)\n\nspecies_files\n\n# Extract species names from the filenames (you can adjust this depending on your naming conventions)\nspecies_names &lt;- tools::file_path_sans_ext(basename(species_files))\n\n# Load all species shapefiles and add a species identifier\nspecies_sf_list &lt;- lapply(seq_along(species_files), function(i) {\n  sf &lt;- st_read(species_files[i])\n  sf$species &lt;- species_names[i]  # Add species name column\n  return(sf)\n})\n\n# Combine all species into one dataset\ncombined_species_sf &lt;- do.call(rbind, species_sf_list)\n\n# Plot the unit (base map) first and overlay the species habitats without borders\ncombined_plot_with_unit &lt;- ggplot() +\n  geom_sf(data = QLD_Unit, fill = NA, color = \"grey\", linewidth = 0.01) +  # Base map (QLD Unit)\n  geom_sf(data = combined_species_sf, aes(fill = species), color = NA) +  # No borders for species\n  scale_fill_manual(values = RColorBrewer::brewer.pal(n = length(species_names), name = \"Set1\")) +  # Automatically assign colors\n  theme_minimal() +\n  labs(title = \"Species Habitats within QLD Unit\",\n       subtitle = paste(species_names, collapse = \", \")) +  # List all species in subtitle\n  theme(legend.title = element_blank())\n\n# Display the plot\nprint(combined_plot_with_unit)\n\n\n\n\n\n\n\n\n6. We can also make a species presence and absence csv table.\n\n# Function to extract presence (1) and absence (0) from raster based on a threshold (e.g., 0.5)\n\nextract_presence_absence &lt;- function(raster_data, unit) {\n  extracted_values &lt;- extract(raster_data, vect(unit), fun = mean, na.rm = TRUE)\n  presence_absence &lt;- ifelse(extracted_values[, 2] &gt;= 0.5, 1, 0)\n  return(presence_absence)\n}\n\n# Create an empty presence-absence data frame\npresence_absence_df &lt;- data.frame(puid = QLD_Unit$puid)  # Assuming 'puid' is the unique identifier\n\n# Loop through each species raster in the raster list and extract presence-absence data\nfor (i in seq_along(raster_list)) {\n  # Generate a dynamic presence column name for the current species\n  presence_col_name &lt;- paste0(species_names[i], \"_presence\")\n  \n  # Extract presence/absence data and add it to the presence-absence dataframe\n  presence_absence_df[[species_names[i]]] &lt;- extract_presence_absence(raster_list[[i]], QLD_Unit)\n}\n\n# Write the final presence-absence data frame to a CSV file\noutput_csv &lt;- file.path(output_dir, \"presence_absence_species.csv\")\nwrite.csv(presence_absence_df, output_csv, row.names = FALSE)\n\n# Check the CSV output\nprint(head(presence_absence_df))\n\n  puid beach_stone_curlew_GLM brushtailed_rockwallaby_GLM Koala_GLM\n1    1                      0                           0         0\n2    2                      0                           0         0\n3    3                      0                           0         0\n4    4                      0                           0         0\n5    5                      0                           0         0\n6    6                      0                           0         0\n\n\n\n\n\n\n\n\n\n\nEcoCommons SDMs output of three species on Marxan MaPP\n\n\n\n\n\n\n\n\nNo Costs, neither SDMs\n\n\n\n\n\n\n\n\nSDMs only\n\n\n\n\n\n\n\n\nCosts only\n\n\n\n\n\n\n\n\nCosts and SDMs"
  },
  {
    "objectID": "notebooks/sp/ecocommons-marxan-integration-poc.html#introduction",
    "href": "notebooks/sp/ecocommons-marxan-integration-poc.html#introduction",
    "title": "EcoCommons -> Marxan MaPP connection",
    "section": "",
    "text": "Using the Species distribution modeling techniques provided by the EcoCommons Platform (www.ecocommons.org.au), we produced probability distribution maps for the three Queensland endangered species: koala, brush tailed rock-wallaby, and beach stone curlew.\nThen we adjusted the probability distribution maps of these three species with the planning units shapefile prepared by the Marxan MaPP, and ran four planning scenarios with a target of expanding the coverage of protected areas in QLD to 30%."
  },
  {
    "objectID": "notebooks/sp/ecocommons-marxan-integration-poc.html#ecocommons-outputs",
    "href": "notebooks/sp/ecocommons-marxan-integration-poc.html#ecocommons-outputs",
    "title": "EcoCommons -> Marxan MaPP connection",
    "section": "",
    "text": "Species records pulled from GBIF, ALA, EcoPlots, OBIS\nSpecies distribution modelling output: Species distribution Probability maps (This is the input tested in this project)."
  },
  {
    "objectID": "notebooks/sp/ecocommons-marxan-integration-poc.html#marxan-mapp-inputs",
    "href": "notebooks/sp/ecocommons-marxan-integration-poc.html#marxan-mapp-inputs",
    "title": "EcoCommons -> Marxan MaPP connection",
    "section": "",
    "text": "Shapefile of planning area and units.\nShapefile of cost.\nShapefile and csv of biodiversity features (Where EcoCommons can help!)."
  },
  {
    "objectID": "notebooks/sp/ecocommons-marxan-integration-poc.html#ecocommons-connects-with-marxan-showcase",
    "href": "notebooks/sp/ecocommons-marxan-integration-poc.html#ecocommons-connects-with-marxan-showcase",
    "title": "EcoCommons -> Marxan MaPP connection",
    "section": "",
    "text": "Make sure you are in the directory you want\n\ngetwd()\n\n[1] \"/Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/sp\"\n\n# setwd(“/replace_this_with_your_actual_directory/ecocommons-marxan-integration-poc”)\n\n\n# Set CRAN mirror\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\n# List of packages to check, install if needed, and load\npackages &lt;- c(\"shiny\", \"sf\", \"terra\", \"ggplot2\", \"ggspatial\", \"raster\", \"dplyr\", \"httpuv\", \"rmarkdown\", \"knitr\", \"jsonlite\", \"reticulate\", \"htmltools\", \"pryr\")\n\n# Function to display a cat message\ncat_message &lt;- function(pkg, message_type) {\n  if (message_type == \"installed\") {\n    cat(paste0(pkg, \" has been installed successfully!\\n\"))\n  } else if (message_type == \"loading\") {\n    cat(paste0(pkg, \" is already installed and has been loaded!\\n\"))\n  }\n}\n\n# Install missing packages and load them\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n    cat_message(pkg, \"installed\")\n  } else {\n    cat_message(pkg, \"loading\")\n  }\n  library(pkg, character.only = TRUE)\n}\n\nshiny is already installed and has been loaded!\nsf is already installed and has been loaded!\n\n\nterra is already installed and has been loaded!\n\n\nggplot2 is already installed and has been loaded!\nggspatial is already installed and has been loaded!\nraster is already installed and has been loaded!\n\n\ndplyr is already installed and has been loaded!\n\n\nhttpuv is already installed and has been loaded!\nrmarkdown is already installed and has been loaded!\nknitr is already installed and has been loaded!\n\n\njsonlite is already installed and has been loaded!\n\n\n\nThe downloaded binary packages are in\n    /var/folders/r7/3wfwvlts0g52zbkjyt24s_nr0000gp/T//Rtmp4RTdHE/downloaded_packages\nreticulate has been installed successfully!\nhtmltools is already installed and has been loaded!\n\n\n\nThe downloaded binary packages are in\n    /var/folders/r7/3wfwvlts0g52zbkjyt24s_nr0000gp/T//Rtmp4RTdHE/downloaded_packages\npryr has been installed successfully!\n\n\n1. We get the QLD planning units from Marxan MaPP\n\nQLD_Unit &lt;- \"qld_3species_Marxan/QLD_plannningunits/cost-surface-template.shp\"  #This cost-surface-template was prepared by the Marxan Mapp with a resolution of 189 Km2, which is the highest resolution Marxan Mapp can give at this scale.\n\nQLD_Unit  &lt;- st_read(QLD_Unit)\nQLD_Unit  &lt;- st_simplify(QLD_Unit , dTolerance = 0.01) \n\n\n# Calculate the resolution since Marxan MaPP for visulization purpose\nareas &lt;- st_area(QLD_Unit)\nareas_numeric &lt;- as.numeric(areas)\naverage_area &lt;- mean(areas_numeric)\n\n# Convert to numeric\naverage_area_km2 &lt;- average_area / 1e6\n\n# Get the number of rows\nn_rows &lt;- nrow(QLD_Unit)\n\n# Plot the shapefile with no fill color and number of rows in the title\nggplot(data = QLD_Unit) +\n  geom_sf(fill = NA, color = \"gray\") +\n  theme_minimal() +\n  ggtitle(paste(\"QLD Planning Units:\", n_rows, \"\\n\",\n                \"Resolution of planning in square kilometers:\", round(average_area_km2)))+\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n\n\n\n\n\n\n2. I made a cost layer using the reciprocal of the distance to state-owned road as a surrogate of the cost.\nThe assumption is: the closer to the state owned road, the more expensive to purchase the unit.\n\nQLD_cost_road &lt;- st_read(\"qld_3species_Marxan/QLD_Cost/QLD_cost_road.shp\")\n\n# Plot the shapefile with continuous cost_road values\nggplot(QLD_cost_road) +\n  geom_sf(aes(fill = cost_road)) +\n  scale_fill_continuous(name = \"Cost\",\n                        low = \"lightblue\", high = \"red\",\n                        labels = c(\"0 (Low cost)\", \"1 (High cost)\"),\n                        breaks = c(0.01, 1)) +\n  theme_minimal() +\n  labs(title = \"Cost: using the distance to road of each Unit as a proxy\")+\n  theme(plot.title = element_text(hjust = 0.5))  # Center the title\n\n\n\n\n\n\n\n\n3. Biodiversity features. I used EcoCommons to produce three species’ SDM to start with.\n\nSpecies 1: koala\nSpecies 2: brush tailed rock-wallaby\nSpecies 3: beach stone curlew\n\n\n# Define the folder path where the rasters are stored\nfolder_path &lt;- \"qld_3species_Marxan/QLD_feature/\"\n\n# Get a list of all .tif files in the folder\nraster_files &lt;- list.files(path = folder_path, pattern = \"\\\\.tif$\", full.names = TRUE)\n\n# Extract the species names from the file names (removing the folder path and .tif extension)\nspecies_names &lt;- tools::file_path_sans_ext(basename(raster_files))\n\n# Read all raster files in one go using lapply\nraster_list &lt;- lapply(raster_files, rast)  # Use rast() from terra for reading rasters\n\n# Using QLD_Unit as the spatial vector for masking\n\n# Transform the raster CRS to match the vector CRS and apply masking in one step\nraster_list &lt;- lapply(raster_list, function(r) {\n  r_transformed &lt;- project(r, crs(vect(QLD_Unit)))\n  mask(r_transformed, vect(QLD_Unit))\n})\n\n# Function to convert rasters to data frames and combine them\nprepare_raster_data &lt;- function(raster_list, species_names) {\n\n  # Initialize an empty data frame\n  combined_df &lt;- data.frame()\n  # Loop through each raster and combine them into one data frame\n  for (i in seq_along(raster_list)) {\n    # Convert raster to a data frame\n    raster_df &lt;- as.data.frame(raster_list[[i]], xy = TRUE)\n    # Rename the third column to 'value' or any appropriate name for the raster values\n    names(raster_df)[3] &lt;- \"value\"\n    # Add a column to identify the species name\n    raster_df$species &lt;- species_names[i]\n    # Combine the raster data with the overall data frame\n    combined_df &lt;- bind_rows(combined_df, raster_df)\n}\n  return(combined_df)\n}\n\n# Prepare the combined data frame\ncombined_raster_df &lt;- prepare_raster_data(raster_list, species_names)\n\n\n# Create the ggplot with facet_wrap to display each raster in a separate facet\nggplot(combined_raster_df, aes(x = x, y = y, fill = value)) +  # Use the correct column name for fill\n  geom_raster()+\n  facet_wrap(~ species, ncol = 3) +  # Adjust ncol to control the number of columns\n  scale_fill_viridis_c() +  # You can adjust the color scale as needed\n  labs(title = \"Species SDM\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))+\n  coord_fixed()  # Maintain the correct aspect ratio\n\n\n\n\n\n\n\n\n4. We need to turn these SDMs to binary results (shapefies).\n\n# Define the folder paths where the rasters and CSV files are stored\nfolder_path_rasters &lt;- \"qld_3species_Marxan/QLD_feature/\"\nfolder_path_csvs &lt;- \"qld_3species_Marxan/model_evaluation/\"\noutput_dir &lt;- \"qld_3species_Marxan/QLD_feature/Marxan_feature_input/\"\n\nQLD_Unit &lt;- \"qld_3species_Marxan/QLD_plannningunits/cost-surface-template.shp\"  #This cost-surface-template was prepared by the Marxan Mapp with a resolution of 189 Km2, which is the highest resolution Marxan Mapp can give at this scale.\n\nQLD_Unit  &lt;- st_read(QLD_Unit)\nQLD_Unit  &lt;- st_simplify(QLD_Unit , dTolerance = 0.01) \n\n\n# Get a list of all .tif files and CSV files in the folder\nraster_files &lt;- list.files(path = folder_path_rasters, pattern = \"\\\\.tif$\", full.names = TRUE)\ncsv_files &lt;- list.files(path = folder_path_csvs, pattern = \"\\\\.csv$\", full.names = TRUE)\n\n# Extract the species names from the file names (removing the folder path and .tif/.csv extension)\nspecies_names &lt;- tools::file_path_sans_ext(basename(raster_files))\n\n# Read all raster files in one go using lapply\nraster_list &lt;- lapply(raster_files, rast)  # Use rast() from terra for reading rasters\n\n# Transform the raster CRS to match the vector CRS and apply masking in one step\nraster_list &lt;- lapply(raster_list, function(r) {\n  r_transformed &lt;- project(r, crs(vect(QLD_Unit)))\n  mask(r_transformed, vect(QLD_Unit))\n})\n\n# Prepare a named list of rasters\nspecies_rasters &lt;- setNames(raster_list, species_names)\nspecies_csvs &lt;- setNames(csv_files, species_names)\n\n# Define UI for the application\nui &lt;- fluidPage(\n  titlePanel(\"Interactive TSS-based threshold for the probability of presence and absence of Species\"),\n  \n  # Use a loop to create a row for each species\n  lapply(species_names, function(species) {\n    fluidRow(\n      column(3, \n             h4(paste(\"Species:\", species)),\n             sliderInput(paste0(\"tss_value_\", species), \n                         \"Select TSS Value:\", \n                         min = 0, max = 1, value = 0.5, step = 0.01),\n             actionButton(paste0(\"run_analysis_\", species), \"Run Species Analysis\"),\n             br(),\n             textOutput(paste0(\"tpr_tnr_\", species))\n      ),\n      \n      column(4, \n             plotOutput(paste0(\"plot_\", species), width = \"400px\")\n      ),\n      \n      column(5, \n             plotOutput(paste0(\"species_plot_\", species))\n      )\n    )\n  })\n)\n\n# Define server logic\nserver &lt;- function(input, output, session) {\n  \n  selected_raster &lt;- function(species) {\n    species_rasters[[species]]\n  }\n  \n  species_eval_data &lt;- function(species) {\n    csv_path &lt;- species_csvs[[species]]\n    \n    if (!file.exists(csv_path)) {\n      showNotification(paste(\"CSV file for\", species, \"not found!\"), type = \"error\")\n      return(NULL)\n    }\n    \n    eval_data &lt;- read.csv(csv_path)\n    \n    if (!all(c(\"tpr\", \"tnr\", \"tpv\") %in% names(eval_data))) {\n      showNotification(paste(\"Required columns missing in CSV for\", species), type = \"error\")\n      return(NULL)\n    }\n    \n    if (nrow(eval_data) == 0) {\n      showNotification(paste(\"No data found in CSV for\", species), type = \"error\")\n      return(NULL)\n    }\n    \n    eval_data$tss &lt;- round(eval_data$tpr + eval_data$tnr - 1, 3)\n    return(eval_data)\n  }\n  \n  lapply(species_names, function(species) {\n    eval_data &lt;- species_eval_data(species)\n    \n    if (!is.null(eval_data)) {\n      min_tss &lt;- min(eval_data$tss, na.rm = TRUE)\n      max_tss &lt;- max(eval_data$tss, na.rm = TRUE)\n      \n      updateSliderInput(session, paste0(\"tss_value_\", species), \n                        min = min_tss, \n                        max = max_tss, \n                        value = max_tss,\n                        step = 0.01)\n    }\n    \n    observeEvent(input[[paste0(\"tss_value_\", species)]], {\n      if (!is.null(eval_data)) {\n        row &lt;- which.min(abs(eval_data$tss - input[[paste0(\"tss_value_\", species)]]))\n        \n        tpr &lt;- eval_data$tpr[row]\n        tnr &lt;- eval_data$tnr[row]\n        \n        output[[paste0(\"tpr_tnr_\", species)]] &lt;- renderText({\n          paste0(\"TPR (Sensitivity): \", round(tpr, 3), \n                 \", TNR (Specificity): \", round(tnr, 3))\n        })\n      }\n    })\n    \n    output[[paste0(\"plot_\", species)]] &lt;- renderPlot({\n      if (is.null(eval_data)) return(NULL)\n      \n      ggplot(eval_data, aes(x = tpv)) +\n        geom_line(aes(y = tpr, colour = \"TPR\"), linewidth = 1) +\n        geom_line(aes(y = tnr, colour = \"TNR\"), linewidth = 1) +\n        geom_line(aes(y = tss, colour = \"TSS\"), linewidth = 1) +\n        geom_vline(xintercept = eval_data$tpv[which.min(abs(eval_data$tss - input[[paste0(\"tss_value_\", species)]]))],\n                   linetype = \"dotted\", color = \"red\", linewidth = 1) +\n        labs(title = paste(\"Sensitivity, Specificity, and TSS for\", species),\n             x = \"Threshold Probability Value\",\n             y = \"Value\") +\n        scale_colour_manual(values = c(\"TPR\" = \"blue\", \"TNR\" = \"green\", \"TSS\" = \"red\")) +\n        theme_minimal()\n    })\n    \n    observeEvent(input[[paste0(\"run_analysis_\", species)]], {\n      species_shp &lt;- process_species(selected_raster(species), QLD_Unit, species, output_dir, input[[paste0(\"tss_value_\", species)]])\n      \n      output[[paste0(\"species_plot_\", species)]] &lt;- renderPlot({\n        ggplot() +\n          geom_sf(data = QLD_Unit, fill = NA, color = \"grey\") +\n          geom_sf(data = species_shp, aes(fill = feature), color = NA) +\n          scale_fill_viridis_c(option = \"plasma\") +\n          labs(title = paste(\"Species Distribution for\", species),\n               x = \"Longitude\", y = \"Latitude\") +\n          theme_minimal()\n      })\n    })\n  })\n}\n\nprocess_species &lt;- function(raster_data, planning_unit, species_name, output_dir, tss_threshold) {\n  raster_data_transformed &lt;- project(raster_data, crs(vect(planning_unit)))\n  extracted_values &lt;- extract(raster_data_transformed, vect(planning_unit), fun = mean, na.rm = TRUE)\n  names(planning_unit)[names(planning_unit) == \"cost\"] &lt;- \"feature\"\n  planning_unit$feature &lt;- extracted_values[, 2]\n  \n  QLD_species &lt;- subset(planning_unit, feature &gt;= tss_threshold)\n  shapefile_base &lt;- file.path(output_dir, species_name)\n  st_write(QLD_species, paste0(shapefile_base, \".shp\"), delete_layer = TRUE)\n  \n  return(QLD_species)\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n5. Plot species SDM binary shapefile outputs for double check\n\noutput_dir &lt;- \"qld_3species_Marxan/QLD_feature/Marxan_feature_input/\"\n\n# List all the shapefiles in the directory (assuming each species has its own shapefile)\nspecies_files &lt;- list.files(output_dir, pattern = \"\\\\.shp$\", full.names = TRUE)\n\nspecies_files\n\n# Extract species names from the filenames (you can adjust this depending on your naming conventions)\nspecies_names &lt;- tools::file_path_sans_ext(basename(species_files))\n\n# Load all species shapefiles and add a species identifier\nspecies_sf_list &lt;- lapply(seq_along(species_files), function(i) {\n  sf &lt;- st_read(species_files[i])\n  sf$species &lt;- species_names[i]  # Add species name column\n  return(sf)\n})\n\n# Combine all species into one dataset\ncombined_species_sf &lt;- do.call(rbind, species_sf_list)\n\n# Plot the unit (base map) first and overlay the species habitats without borders\ncombined_plot_with_unit &lt;- ggplot() +\n  geom_sf(data = QLD_Unit, fill = NA, color = \"grey\", linewidth = 0.01) +  # Base map (QLD Unit)\n  geom_sf(data = combined_species_sf, aes(fill = species), color = NA) +  # No borders for species\n  scale_fill_manual(values = RColorBrewer::brewer.pal(n = length(species_names), name = \"Set1\")) +  # Automatically assign colors\n  theme_minimal() +\n  labs(title = \"Species Habitats within QLD Unit\",\n       subtitle = paste(species_names, collapse = \", \")) +  # List all species in subtitle\n  theme(legend.title = element_blank())\n\n# Display the plot\nprint(combined_plot_with_unit)\n\n\n\n\n\n\n\n\n6. We can also make a species presence and absence csv table.\n\n# Function to extract presence (1) and absence (0) from raster based on a threshold (e.g., 0.5)\n\nextract_presence_absence &lt;- function(raster_data, unit) {\n  extracted_values &lt;- extract(raster_data, vect(unit), fun = mean, na.rm = TRUE)\n  presence_absence &lt;- ifelse(extracted_values[, 2] &gt;= 0.5, 1, 0)\n  return(presence_absence)\n}\n\n# Create an empty presence-absence data frame\npresence_absence_df &lt;- data.frame(puid = QLD_Unit$puid)  # Assuming 'puid' is the unique identifier\n\n# Loop through each species raster in the raster list and extract presence-absence data\nfor (i in seq_along(raster_list)) {\n  # Generate a dynamic presence column name for the current species\n  presence_col_name &lt;- paste0(species_names[i], \"_presence\")\n  \n  # Extract presence/absence data and add it to the presence-absence dataframe\n  presence_absence_df[[species_names[i]]] &lt;- extract_presence_absence(raster_list[[i]], QLD_Unit)\n}\n\n# Write the final presence-absence data frame to a CSV file\noutput_csv &lt;- file.path(output_dir, \"presence_absence_species.csv\")\nwrite.csv(presence_absence_df, output_csv, row.names = FALSE)\n\n# Check the CSV output\nprint(head(presence_absence_df))\n\n  puid beach_stone_curlew_GLM brushtailed_rockwallaby_GLM Koala_GLM\n1    1                      0                           0         0\n2    2                      0                           0         0\n3    3                      0                           0         0\n4    4                      0                           0         0\n5    5                      0                           0         0\n6    6                      0                           0         0"
  },
  {
    "objectID": "notebooks/sp/ecocommons-marxan-integration-poc.html#marxan-four-scenarios-solutions",
    "href": "notebooks/sp/ecocommons-marxan-integration-poc.html#marxan-four-scenarios-solutions",
    "title": "EcoCommons -> Marxan MaPP connection",
    "section": "",
    "text": "EcoCommons SDMs output of three species on Marxan MaPP\n\n\n\n\n\n\n\n\nNo Costs, neither SDMs\n\n\n\n\n\n\n\n\nSDMs only\n\n\n\n\n\n\n\n\nCosts only\n\n\n\n\n\n\n\n\nCosts and SDMs"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "index",
    "section": "",
    "text": "EcoCommons Notebooks\nEcoCommons Notebooks represent an exciting expansion of the platform’s capabilities, offering a new layer of flexibility for advanced users. By incorporating notebooks, EcoCommons enhances its ability to serve the environmental data science community, enabling users to engage with cutting-edge methods, customize workflows, and increase reproducibility in their research. These notebooks, hosted on trusted cloud providers, provide a transparent, open-access approach to environmental data analysis while adhering to the FAIR principles. With planned monthly releases, EcoCommons Notebooks will empower users to explore complex topics such as species migration, community modeling, and data cleaning for biodiversity research."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "seabass he/him\nPrincipal Data Scientist\nSebastian is a data scientist specialising in AI, computer vision, data analytics, and developing data-intensive interactive applications for AgTech, the environment, and climate. He manages QCIF’s Sustainable Futures portfolio, a team in charge of the delivery of ARDC-supported platforms such as EcoCommons and Wildlife Observatories. From 2018 to 2024, Sebastian managed FishID, Australia’s platform for automated analysis of aquatic imagery. In his free time, you can find Seb cycling, reviewing restaurants, and fishing.\n\n\n\nCaptain Marval  (she/her)\nSenior Data Scientist\nJenna is a Conservation Scientist specialising in ecological modelling, species distribution modelling and using biodiversity data to inform policy. As an Adjunct Research Fellow at the Australian Rivers Institute, she uses spatial analyses and conservation planning to drive environmental strategies. Her work bridges science and technology, advancing biodiversity conservation and ecological research. In her free time, you can find Jenna growing food and flowers in the garden, hiking or getting crafty.\n\n\n\nIron Man (he/him)\nData Ecologist\n\nAbhimanyu is a Zoologist who metamorphosed into an Environmental Scientist focused on terrestrial ecology and species behaviour/interactions. His MS majors are in environmental protection as well as climate change adaptation. He was an Environmental Field Officer in Sydney Basin, working with local councils and national parks on threatened and invasive species management. As a Functional Analyst, he facilitates platform testing strategies, provides user support, and assists with ecological and climate change modelling-based use cases/learning material/workshops for the EcoCommons platform.\n\n\n\nBrisbane Bluey (he/him)\nSoftware Developer & R Specialist\nXiang Zhao (Zhao) joined QCIF in 2024 as a Software Developer and R Specialist, focusing on maintaining and developing ecological algorithms for the EcoCommons Platform. During his MPhil at QUT, he utilized spatial mapping, modeling, statistics, and optimization algorithms to study historical biodiversity survey patterns in terrestrial Antarctica, proposing an optimal survey design for future research. At QUT, he also tutors in GIS and Environmental Planning units, leveraging his expertise in GIS and cartography.\nZhao spent two years traveling across Australia as a backpacker. Prior to this, he worked on a conservation project in Southwest China aimed at protecting giant panda habitats from the threat of free-ranging livestock. This project earned him the BirdLife International Young Conservation Leaders Award.\n\n\n\nRyeBread (he/him)\nSoftware Developer and R Specialist\nRyan joined QCIF in 2024 as a Software Developer and R Specialist, with a focus on developing an ‘Integrated Ecological Data Service’ for State of Environment reporting in Queensland and working on the EcoCommons platform. His research background encompasses various aspects of the environmental sciences including landscape ecology, ecological network analysis, chemical ecology and pollination biology and ecology. His PhD studies focused on critical but unexplored aspects of stingless bee foraging ecology and colony propagation. When not entrenched in scientific exploration, you can find Ryan in the mountains somewhere searching for secret waterfalls and adding to his birding list."
  },
  {
    "objectID": "about.html#our-team",
    "href": "about.html#our-team",
    "title": "about",
    "section": "",
    "text": "seabass he/him\nPrincipal Data Scientist\nSebastian is a data scientist specialising in AI, computer vision, data analytics, and developing data-intensive interactive applications for AgTech, the environment, and climate. He manages QCIF’s Sustainable Futures portfolio, a team in charge of the delivery of ARDC-supported platforms such as EcoCommons and Wildlife Observatories. From 2018 to 2024, Sebastian managed FishID, Australia’s platform for automated analysis of aquatic imagery. In his free time, you can find Seb cycling, reviewing restaurants, and fishing.\n\n\n\nCaptain Marval  (she/her)\nSenior Data Scientist\nJenna is a Conservation Scientist specialising in ecological modelling, species distribution modelling and using biodiversity data to inform policy. As an Adjunct Research Fellow at the Australian Rivers Institute, she uses spatial analyses and conservation planning to drive environmental strategies. Her work bridges science and technology, advancing biodiversity conservation and ecological research. In her free time, you can find Jenna growing food and flowers in the garden, hiking or getting crafty.\n\n\n\nIron Man (he/him)\nData Ecologist\n\nAbhimanyu is a Zoologist who metamorphosed into an Environmental Scientist focused on terrestrial ecology and species behaviour/interactions. His MS majors are in environmental protection as well as climate change adaptation. He was an Environmental Field Officer in Sydney Basin, working with local councils and national parks on threatened and invasive species management. As a Functional Analyst, he facilitates platform testing strategies, provides user support, and assists with ecological and climate change modelling-based use cases/learning material/workshops for the EcoCommons platform.\n\n\n\nBrisbane Bluey (he/him)\nSoftware Developer & R Specialist\nXiang Zhao (Zhao) joined QCIF in 2024 as a Software Developer and R Specialist, focusing on maintaining and developing ecological algorithms for the EcoCommons Platform. During his MPhil at QUT, he utilized spatial mapping, modeling, statistics, and optimization algorithms to study historical biodiversity survey patterns in terrestrial Antarctica, proposing an optimal survey design for future research. At QUT, he also tutors in GIS and Environmental Planning units, leveraging his expertise in GIS and cartography.\nZhao spent two years traveling across Australia as a backpacker. Prior to this, he worked on a conservation project in Southwest China aimed at protecting giant panda habitats from the threat of free-ranging livestock. This project earned him the BirdLife International Young Conservation Leaders Award.\n\n\n\nRyeBread (he/him)\nSoftware Developer and R Specialist\nRyan joined QCIF in 2024 as a Software Developer and R Specialist, with a focus on developing an ‘Integrated Ecological Data Service’ for State of Environment reporting in Queensland and working on the EcoCommons platform. His research background encompasses various aspects of the environmental sciences including landscape ecology, ecological network analysis, chemical ecology and pollination biology and ecology. His PhD studies focused on critical but unexplored aspects of stingless bee foraging ecology and colony propagation. When not entrenched in scientific exploration, you can find Ryan in the mountains somewhere searching for secret waterfalls and adding to his birding list."
  },
  {
    "objectID": "notebooks/dataprep.html",
    "href": "notebooks/dataprep.html",
    "title": "Data Preparation Overview",
    "section": "",
    "text": "Welcome to the Data Preparation section! Here, you can find various guides and resources to help you with the preparation of data for Species Distribution Models (SDM).\n\n\nBelow is a list of specific topics related to data preparation. Click on each link to access more detailed guides.\n\n\nEnvironmental Data Preparation\nThis guide will help you prepare environmental variables such as bioclimatic data, land use, and others required for SDM.\n\n\n\nImbalanced dataset\nLearn how to deal with the issue of imbalanced dataset.\n\n\n\nRaster Processing\nStep-by-step instructions on how to handle raster data, including reprojecting, cropping, and masking.\n\n\n\nData Cleaning Techniques\nA guide on the best practices for cleaning both environmental and occurrence datasets.\n\nIf you have questions or suggestions, please contact the EcoCommons team."
  },
  {
    "objectID": "notebooks/dataprep.html#available-guides",
    "href": "notebooks/dataprep.html#available-guides",
    "title": "Data Preparation Overview",
    "section": "",
    "text": "Below is a list of specific topics related to data preparation. Click on each link to access more detailed guides.\n\n\nEnvironmental Data Preparation\nThis guide will help you prepare environmental variables such as bioclimatic data, land use, and others required for SDM.\n\n\n\nImbalanced dataset\nLearn how to deal with the issue of imbalanced dataset.\n\n\n\nRaster Processing\nStep-by-step instructions on how to handle raster data, including reprojecting, cropping, and masking.\n\n\n\nData Cleaning Techniques\nA guide on the best practices for cleaning both environmental and occurrence datasets.\n\nIf you have questions or suggestions, please contact the EcoCommons team."
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#s.1-set-the-working-directory-and-create-a-folder-for-data.",
    "href": "notebooks/data_prep/imbalanced_data.html#s.1-set-the-working-directory-and-create-a-folder-for-data.",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "S.1 Set the working directory and create a folder for data.",
    "text": "S.1 Set the working directory and create a folder for data.\nSave the Quarto Markdown file (.QMD) to a folder of your choice, and then set the path to your folder as your working directory.\n\n# Set the workspace to the current working directory\n\nworkspace &lt;- getwd()  # Get the current working directory and store it in 'workspace'\n\n# Increase the plot size by adjusting the options for plot dimensions in the notebook output\noptions(repr.plot.width = 16, repr.plot.height = 8)  # Sets width to 16 and height to 8 for larger plots\n\nIdeally, you would use the renv package to create an isolated environment for installing all the required R packages used in this notebook. However, since installing renv and its dependencies can be time-consuming, we recommend trying this after the workshop.\n\n# # Ensure \"renv\" package is installed\n# if (!requireNamespace(\"renv\", quietly = TRUE)) {\n#   install.packages(\"renv\")\n# }\n# \n# # Check if renv has been initialised in the project\n# if (!file.exists(\"renv/activate.R\")) {\n#   message(\"renv has not been initiated in this project. Initializing now...\")\n#   renv::init()  # Initialise renv if not already set up\n# } else {\n#   source(\"renv/activate.R\")  # Activate the renv environment\n#   message(\"renv is activated.\")\n# }\n# \n# # Check for the existence of renv.lock and restore the environment\n# if (file.exists(\"renv.lock\")) {\n#   message(\"Restoring renv environment from renv.lock...\")\n#   renv::restore()\n# } else {\n#   message(\"No renv.lock file found in the current directory. Skipping restore.\")\n# }"
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#s.2-install-and-load-essential-libraries.",
    "href": "notebooks/data_prep/imbalanced_data.html#s.2-install-and-load-essential-libraries.",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "S.2 Install and load essential libraries.",
    "text": "S.2 Install and load essential libraries.\nInstall and load R packages.\n\n# Set CRAN mirror\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\n# List of packages to check, install if needed, and load\npackages &lt;- c(\"dplyr\", \"reshape2\", \"terra\", \"sf\", \"googledrive\", \"ggplot2\", \"corrplot\", \n              \"pROC\", \"dismo\", \"spatstat.geom\", \"patchwork\", \"biomod2\", \"PRROC\",\n              \"leaflet\", \"car\", \"gridExtra\", \"htmltools\", \"RColorBrewer\", \"kableExtra\", \"tibble\")\n\n# Function to display a cat message\ncat_message &lt;- function(pkg, message_type) {\n  if (message_type == \"installed\") {\n    cat(paste0(pkg, \" has been installed successfully!\\n\"))\n  } else if (message_type == \"loading\") {\n    cat(paste0(pkg, \" is already installed and has been loaded!\\n\"))\n  }\n}\n\n# Install missing packages and load them\nfor (pkg in packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n    cat_message(pkg, \"installed\")\n  } else {\n    cat_message(pkg, \"loading\")\n  }\n  library(pkg, character.only = TRUE)\n}\n\ndplyr is already installed and has been loaded!\n\n\nreshape2 is already installed and has been loaded!\nterra is already installed and has been loaded!\n\n\nsf is already installed and has been loaded!\n\n\ngoogledrive is already installed and has been loaded!\nggplot2 is already installed and has been loaded!\ncorrplot is already installed and has been loaded!\n\n\npROC is already installed and has been loaded!\n\n\ndismo is already installed and has been loaded!\n\n\nspatstat.geom is already installed and has been loaded!\n\n\npatchwork is already installed and has been loaded!\n\n\nbiomod2 is already installed and has been loaded!\n\n\nPRROC is already installed and has been loaded!\nleaflet is already installed and has been loaded!\ncar is already installed and has been loaded!\n\n\ngridExtra is already installed and has been loaded!\n\n\nhtmltools is already installed and has been loaded!\nRColorBrewer is already installed and has been loaded!\nkableExtra is already installed and has been loaded!\n\n\ntibble is already installed and has been loaded!\n\n# If you are using renv, you can snapshot the renv after loading all the packages.\n\n#renv::snapshot()"
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#s.3-download-case-study-datasets",
    "href": "notebooks/data_prep/imbalanced_data.html#s.3-download-case-study-datasets",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "S.3 Download case study datasets",
    "text": "S.3 Download case study datasets\nWe have prepared the following data and uploaded them to our Google Drive for your use:\n\nSpecies occurrence data: Shapefile format (.shp)\nEnvironmental variables: Stacked Raster format (.tif)\nStudy area boundary: Shapefile format (.shp)\n\n\n# De-authenticate Google Drive to access public files\ndrive_deauth()\n\n# Define Google Drive file ID and the path for downloading\nzip_file_id &lt;- \"1GXOA330Ow2F7NqxdhuBfbz0a-CMT8DTb\" # Replace with the actual file ID of the zipped file\ndatafolder_path &lt;- file.path(workspace)\n\n# Create a local path for the zipped file\nzip_file_path &lt;- file.path(datafolder_path, \"imbalanced_data.zip\")\n\n# Function to download a file with progress messages\ndownload_zip_file &lt;- function(file_id, file_path) {\n  cat(\"Downloading zipped file...\\n\")\n  drive_download(as_id(file_id), path = file_path, overwrite = TRUE)\n  cat(\"Downloaded zipped file to:\", file_path, \"\\n\")\n}\n\n# Create local directory if it doesn't exist\nif (!dir.exists(datafolder_path)) {\n  dir.create(datafolder_path, recursive = TRUE)\n}\n\n# Download the zipped file\ncat(\"Starting to download the zipped file...\\n\")\n\nStarting to download the zipped file...\n\ndownload_zip_file(zip_file_id, zip_file_path)\n\nDownloading zipped file...\n\n\nDownloaded zipped file to: /Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/data_prep/imbalanced_data.zip \n\n# Unzip the downloaded file\ncat(\"Unzipping the file...\\n\")\n\nUnzipping the file...\n\nunzip(zip_file_path, exdir = datafolder_path)\ncat(\"Unzipped files to folder:\", datafolder_path, \"\\n\")\n\nUnzipped files to folder: /Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/data_prep"
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#imbalanced-binary-species-dataset",
    "href": "notebooks/data_prep/imbalanced_data.html#imbalanced-binary-species-dataset",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "1.1 Imbalanced binary species dataset",
    "text": "1.1 Imbalanced binary species dataset\nImbalanced datasets are not unusual. Shown as the figure below, a dataset is imbalanced if the classifications are not equally represented (Chawla et al., 2002).\n\nIn the case of binary species records, presence and absence of a species, there is a common scenario of imbalanced datasets: too many presences. The phenomenon of too many presences in a species record dataset is quite dominant. In many open access biodiversity data portals, like the Global Biodiversity Information Facility (GBIF, https://www.gbif.org/) and Atlas of Living Australia (ALA, https://www.ala.org.au/), most of species only have present records.\nUndesired outcomes of using imbalanced dataset for modelling and prediction. For example, because presences dominate the training set, the model may be inclined to predict presence more often to minimize overall errors. You may see high sensitivity (true positive rate, correctly identifying most presences) but low specificity (true negative rate) (Johnson et al., 2012). Thus, dealing with imbalanced data before fitting SDMs is a necessary step."
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#weighting-methods",
    "href": "notebooks/data_prep/imbalanced_data.html#weighting-methods",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "1.2 Weighting methods",
    "text": "1.2 Weighting methods\nMany studies show that weighting methods can improve SDM performance (Benkendorf et al., 2023; Chen et al., 2004; Zhang et al., 2020). Applying class weights to SDM algorithm so the cost of misclassifying the minority class can be elevated relative to the cost of misclassifying the majority class (Benkendorf et al., 2023).\nFor example, if a dataset contains 900 presences and 100 absences, the ratio of classes is 9:1. To compensate for this imbalance, class weights can be assigned inversely proportional to class frequencies. In this case, a weight of 1 would be assigned to the majority class (presences) and a weight of 9 to the minority class (absences). This means the model incurs a 9× cost for misclassifying an absence relative to a presence. Such reweighting has been shown to improve the tradeoff between model sensitivity and specificity. Specifically, although increases in sensitivity are often accompanied by decreases in specificity and overall accuracy, applying these class weights can help achieve a better balance.\nIn this notebook, we use a very simple weighting methods used by Elith et al. (Elith* et al., 2006) and Valavi et al. (Valavi et al., 2022). The weights are generated by giving a weight of 1 to every presence location and give the weights to the background in a way that the sum of the weights for the presence and background samples are equal.\n\n# Load necessary library\nlibrary(dplyr)\n\n# 1️⃣ Create a toy dataset\nset.seed(123)  # For reproducibility\n\n# Example data frame with presence (1) and background/absence (0)\ntrain_data &lt;- data.frame(\n  id = 1:10,\n  occrrnS = c(rep(1, 7), rep(0, 3))  # 7 presence points, 3 background points\n)\n\n# 2️⃣ Calculate weights\n# Count the number of presence and background points\nprNum &lt;- sum(train_data$occrrnS == 1)\nbgNum &lt;- sum(train_data$occrrnS == 0)\n\n# Apply weights: presence = 1, background = prNum / bgNum\nwt &lt;- ifelse(train_data$occrrnS == 1, 1, prNum / bgNum)\n\n# Add weights to the data frame\ntrain_data &lt;- train_data %&gt;%\n  mutate(weight = wt)\n\n# 3️⃣ Display the final dataset with weights\nprint(train_data)\n\n   id occrrnS   weight\n1   1       1 1.000000\n2   2       1 1.000000\n3   3       1 1.000000\n4   4       1 1.000000\n5   5       1 1.000000\n6   6       1 1.000000\n7   7       1 1.000000\n8   8       0 2.333333\n9   9       0 2.333333\n10 10       0 2.333333\n\n\nThe sum of the weights for 1 (Present) is 7 × 1 = 7, which is equal to the sum of the weights for 0 (Absent), 2.33 × 3 = 7. This weighting method will be used to address the imbalanced dataset issue in this notebook."
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#background-points-and-pseudo-absence-points",
    "href": "notebooks/data_prep/imbalanced_data.html#background-points-and-pseudo-absence-points",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "1.3 Background points and Pseudo-absence points",
    "text": "1.3 Background points and Pseudo-absence points\nWhen working with presence-only records for your study species, a common approach is to sample a relatively large number of random samples from the study area. These random samples area referred to background or pseudo-absence points (Valavi et al., 2022).\nThere are two common approaches of generating background points. The first involves randomly selecting background points from the entire extent of the study area, allowing observations to be compared against the full range of environmental conditions within that area (Elith* et al., 2006; Phillips et al., 2006). The second approach generates background points within a specified minimum-maximum radius around presence points-a method referred to as the “disk” approach in EcoCommons Platform.\nFor more information about background points, pseudo-absence data, please check out our support article Absence Data."
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#evaluation-metric",
    "href": "notebooks/data_prep/imbalanced_data.html#evaluation-metric",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "1.4 Evaluation metric",
    "text": "1.4 Evaluation metric\nSeveral studies suggest how to evaluate the model performance of imbalanced dataset (Barbet-Massin et al., 2012; Gaul et al., 2022; Johnson et al., 2012; Zhang et al., 2020). We select these to use in this notebook:\n\nWe also provide some information on model evaluation on our platform’s educational material page.\n\n\nMetric\nWhat it Tells You\nOptimized When\n\n\n\n\nAUROC (Area Under ROC Curve)\nAbility of the model to distinguish between classes\nWhen overall discriminative power is important\n\n\nCORR (Correlation between the observation in the occurrence dataset and the prediction)\nDegree of correlation between predicted and observed values\nWhen assessing the strength and direction of relationships\n\n\nPrecision\nProportion of positive predictions that are correct\nFalse positives need to be minimized\n\n\nRecall\nAbility to capture actual positives\nMissing positives has severe consequences\n\n\nSpecificity\nAbility to correctly identify negatives\nFalse positives need to be minimized\n\n\nF1-Score\nBalance between Precision and Recall\nBoth false positives and false negatives are costly\n\n\nAUPR (Area Under Precision-Recall Curve)\nPerformance on imbalanced datasets focusing on positive class\nWhen positive class is rare and critical\n\n\nTSS (True Skill Statistic)\nSkill of the model beyond random chance\nWhen evaluating presence/absence models in ecological studies"
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#model-objective",
    "href": "notebooks/data_prep/imbalanced_data.html#model-objective",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "2.1 Model objective",
    "text": "2.1 Model objective\nThis notebook focuses not on creating a perfect Species Distribution Model (SDM) for the Gang-gang Cockatoo, but on comparing different methods for addressing class imbalance in binary species datasets and evaluating their effectiveness in improving SDM performance."
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#taxon-location-data-scale-and-model-algorithm",
    "href": "notebooks/data_prep/imbalanced_data.html#taxon-location-data-scale-and-model-algorithm",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "2.2 Taxon, location, data, scale, and model algorithm",
    "text": "2.2 Taxon, location, data, scale, and model algorithm\nTaxon: Gang-gang Cockatoo (Callocephalon fimbriatum)\n\nPhotographer: Kym Nicolson, ALA\nThe Gang-gang Cockatoo (Callocephalon fimbriatum) is endemic to southeastern Australia, with its distribution primarily spanning higher elevations and southern latitudes, including regions in New South Wales, Victoria, and the Australian Capital Territory. It inhabits temperate eucalypt forests and woodlands, favoring wet sclerophyll forests with dense acacia and banksia understories during the summer months, while moving to drier, open woodlands at lower altitudes in winter. The species is well-adapted to cooler climates and often forages in parks and suburban gardens.\nHowever, Gang-gang Cockatoos face significant threats, including habitat loss due to land clearing, wildfire damage, and competition for nesting hollows with other species. Climate change, particularly increased fire frequency and altered rainfall patterns, poses additional risks to their survival. The species suffered a drastic population decline, exacerbated by the 2019/2020 bushfires, which burned approximately 28–36% of its habitat, leading to a projected long-term decline in population size.\nLocation: Australian Capital Territory (ACT, study area)\nSpatial and temporal scales: small (spatial) and static (temporal)\nmodel algorithm: Generalised linear model (GLM)"
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#imbalanced-biodiversity-data",
    "href": "notebooks/data_prep/imbalanced_data.html#imbalanced-biodiversity-data",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "2.3 Imbalanced biodiversity data",
    "text": "2.3 Imbalanced biodiversity data\nUnderstanding your species is essential. This includes knowing their common names (which may include multiple names) and scientific name to ensure you collect the most comprehensive records available in open-access biodiversity data portals, such as the Atlas of Living Australia (ALA) or the Global Biodiversity Information Facility (GBIF).\nFor this exercise, we have prepared a species occurrence data file in CSV format, which was downloaded from ALA. To make it accessible, we have stored this file in the EcoCommons Public Google Drive for you to download and use conveniently.\n\n# Read the shapefile for Gang-gang Cockatoo occurrence point dataset\ngang_gang_act &lt;- st_read(\"imbalanced_data/gang_gang_ACT.shp\")\n\nReading layer `gang_gang_ACT' from data source \n  `/Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/data_prep/imbalanced_data/gang_gang_ACT.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 918 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 148.77 ymin: -35.9 xmax: 149.39 ymax: -35.13\nGeodetic CRS:  GDA94\n\n# Read the shapefile for ACT dataset\nACT &lt;- st_read(\"imbalanced_data/ACT_4283.shp\")\n\nReading layer `ACT_4283' from data source \n  `/Users/xiangzhaoqcif/Documents/Documents - QCIF’s MacBook Pro (2)/github/notebook-blog/notebooks/data_prep/imbalanced_data/ACT_4283.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 148.7628 ymin: -35.92053 xmax: 149.3993 ymax: -35.12442\nGeodetic CRS:  GDA94\n\n# Create leaflet map\nleaflet() %&gt;%\n  addProviderTiles(providers$Esri.WorldImagery) %&gt;%\n  \n  # Add the ACT layer with a distinct color\n  addPolygons(\n    data = ACT,\n    color = \"lightblue\",         # Border color of ACT polygon\n    weight = 1,                  # Border width\n    fillColor = \"lightblue\",      # Fill color of ACT\n    fillOpacity = 0.3,           # Transparency for fill\n    group = \"ACT\"\n  ) %&gt;%\n  \n  # Add Gang-gang Cockatoo presence/absence points from the same shapefile\n  addCircleMarkers(\n    data = gang_gang_act,\n    color = ~ifelse(occrrnS == \"PRESENT\", \"#11aa96\", \"#f6aa70\"),  # Dynamically set color based on occrrnS\n    radius = 1,\n    weight = 0.5,\n    opacity = 1,\n    fillOpacity = 1,\n    group = \"Gang-gang Cockatoo Records\"\n  ) %&gt;%\n  \n  setView(lng = 149, lat = -35.5, zoom = 8) %&gt;% # Set the view to desired location\n  \n  # Add layer controls for toggling\n  addLayersControl(\n    overlayGroups = c(\"ACT\", \"Gang-gang Cockatoo Records\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) %&gt;%\n  \n  # Add a legend for the layers\n  addControl(\n    html = \"\n    &lt;div style='background-color: white; padding: 10px; border-radius: 5px;'&gt;\n      &lt;strong&gt;Legend&lt;/strong&gt;&lt;br&gt;\n      &lt;i style='background: lightblue; width: 18px; height: 18px; display: inline-block; margin-right: 8px; opacity: 0.7;'&gt;&lt;/i&gt;\n      ACT Boundary&lt;br&gt;\n      &lt;i style='background: #11aa96; width: 10px; height: 10px; border-radius: 50%; display: inline-block; margin-right: 8px;'&gt;&lt;/i&gt;\n      Gang-gang Cockatoo Presence&lt;br&gt;\n      &lt;i style='background: #f6aa70; width: 10px; height: 10px; border-radius: 50%; display: inline-block; margin-right: 8px;'&gt;&lt;/i&gt;\n      Gang-gang Cockatoo Absence\n    &lt;/div&gt;\n    \",\n    position = \"bottomright\"\n  )\n\n\n\n\n\nNow, let’s calculate the number of presences and absences in the dataset to assess the extent of imbalance.\n\n# Calculate counts of presence (PRESENT) and absence (ABSENT)\nocc_counts &lt;- table(gang_gang_act$occrrnS)\n\n# Convert to data frame for plotting\nocc_df &lt;- as.data.frame(occ_counts)\ncolnames(occ_df) &lt;- c(\"Occurrence\", \"Count\")\n\n# Create bar plot with custom colors\nggplot(occ_df, aes(x = Occurrence, y = Count, fill = Occurrence)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(label = Count),\n    vjust = -0.3,  \n    size = 3\n  ) +\n  labs(\n    title = \"Presence vs Absence Count\",\n    x = \"Occurrence\",\n    y = \"Count\"\n  ) +\n  scale_fill_manual(values = c(\"PRESENT\" = \"#11aa96\", \"ABSENT\" = \"#f6aa70\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can see the amount of Gang-gang Cockatoo absent records are way less than its present records."
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#environmental-variables",
    "href": "notebooks/data_prep/imbalanced_data.html#environmental-variables",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "2.4 Environmental variables",
    "text": "2.4 Environmental variables\nWe conducted several tests and selected seven environmental variables for this case study. For more information on preparing environmental datasets and selecting environmental variables, please refer to our notebooks: Raster-preparation and GLM (section 2.3-3.1).\n\n\n\n\n\n\n\nVariable code\nDescription\n\n\n\n\nAusClim_bioclim_03_9s_1976-2005\nIsothermality (BIO3) — Mean Diurnal Range / Annual Temp Range × 100\n\n\nAusClim_bioclim_06_9s_1976-2005\nMin Temperature of Coldest Month (BIO6)\n\n\nAusClim_bioclim_09_9s_1976-2005\nMean Temperature of Driest Quarter (BIO9)\n\n\nAusClim_bioclim_15_9s_1976-2005\nPrecipitation Seasonality (Coefficient of Variation) (BIO15)\n\n\nAusClim_bioclim_24_9s_1976-2005\nAnnual Mean Moisture Index (custom or derived bioclim variable)\n\n\nAusClim_bioclim_27_9s_1976-2005\nMean Moisture Index of Driest Quarter (custom/derived)\n\n\nAusClim_bioclim_29_9s_1976-2005\nMean Radiation of Warmest Quarter (custom/derived)\n\n\n\n\n# Load the stacked raster layers\nenv_var_stack &lt;- rast(\"imbalanced_data/ACT_raster.tif\")\n\n# Select the desired layers by their names\nselected_layers &lt;- c(\"AusClim_bioclim_03_9s_1976-2005\",\n                     \"AusClim_bioclim_06_9s_1976-2005\",\n                     \"AusClim_bioclim_09_9s_1976-2005\",\n                     \"AusClim_bioclim_15_9s_1976-2005\",\n                     \"AusClim_bioclim_24_9s_1976-2005\",\n                     \"AusClim_bioclim_27_9s_1976-2005\",\n                     \"AusClim_bioclim_29_9s_1976-2005\")\n\n# Subset the raster stack\nselected_rasters &lt;- env_var_stack[[selected_layers]]\n\n# Plot all selected layers\nplot(selected_rasters)"
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#including-true-absence-in-the-training-dataset",
    "href": "notebooks/data_prep/imbalanced_data.html#including-true-absence-in-the-training-dataset",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "3.1 Including true absence in the training dataset",
    "text": "3.1 Including true absence in the training dataset\nIncluding true-absence data in training dataset is always recommended when such data is available (Václavı́k & Meentemeyer, 2009). In this study, we aim to explore the question: ‘how much true-absence data is enough?’ ‘Is more always better?’\nTo answer this question, we allocate 90%, 70%, 50% of the true absence data, along with an equal amount of present data to the test dataset, using the rest data as training dataset.\nHere, we can make a function to split the data as we want.\n\nsplit_occurrence_data_by_absence_test &lt;- function(data, p_abs_test = 0.9, seed = 123) {\n  # Set seed for reproducibility\n  set.seed(seed)\n  \n  # Identify indices for ABSENT and PRESENT rows\n  abs_idx &lt;- which(data$occrrnS == \"ABSENT\")\n  pres_idx &lt;- which(data$occrrnS == \"PRESENT\")\n  \n  # Total number of ABSENT records\n  n_abs &lt;- length(abs_idx)\n  \n  # Number of ABSENT records to include in test set\n  n_test_abs &lt;- round(p_abs_test * n_abs)\n  \n  # Randomly sample n_test_abs indices from the ABSENT indices for the test set\n  test_abs_idx &lt;- sample(abs_idx, n_test_abs)\n  \n  # For the test set, randomly sample an equal number of PRESENT records\n  test_pres_idx &lt;- sample(pres_idx, n_test_abs)\n  \n  # Create test data: sampled ABSENT + sampled PRESENT\n  test_data &lt;- data[c(test_abs_idx, test_pres_idx), ]\n  \n  # Create training data: the remaining rows (i.e., those not selected for test)\n  train_data &lt;- data[-c(test_abs_idx, test_pres_idx), ]\n  \n  return(list(test_data = test_data, train_data = train_data))\n}\n\nNow, we can use the above function to split the data in three difference ways:\n\n90% of true absence data, along with an equal amount of present data to the test dataset, using the rest data as training dataset.\n70% of true absence data, along with an equal amount of present data to the test dataset, using the rest data as training dataset.\n50% of the true absence data, along with an equal amount of present data to the test dataset, using the rest data as training dataset.\n\n\n# For a 90% test split (of the ABSENT records)\nsplit_90_absence &lt;- split_occurrence_data_by_absence_test(gang_gang_act, p_abs_test = 0.9, seed = 123)\n\n# For a 70% test split\nsplit_70_absence &lt;- split_occurrence_data_by_absence_test(gang_gang_act, p_abs_test = 0.7, seed = 123)\n\n# For a 50% test split\nsplit_50_absence &lt;- split_occurrence_data_by_absence_test(gang_gang_act, p_abs_test = 0.5, seed = 123)\n\n# Generate counts from the split data\ntrain_90_absence &lt;- table(split_90_absence$train_data$occrrnS)\ntest_90_absence &lt;- table(split_90_absence$test_data$occrrnS)\n\ntrain_70_absence &lt;- table(split_70_absence$train_data$occrrnS)\ntest_70_absence &lt;- table(split_70_absence$test_data$occrrnS)\n\ntrain_50_absence &lt;- table(split_50_absence$train_data$occrrnS)\ntest_50_absence &lt;- table(split_50_absence$test_data$occrrnS)\n\n# Create a data frame with the extracted counts\nsplit_data &lt;- data.frame(\n  Split = c(\"90% Absence Train\", \"90% Absence Test\", \"70% Absence Train\", \"70% Absence Test\", \"50% Absence Train\", \"50% Absence Test\"),\n  ABSENT = c(train_90_absence[\"ABSENT\"], test_90_absence[\"ABSENT\"],\n             train_70_absence[\"ABSENT\"], test_70_absence[\"ABSENT\"],\n             train_50_absence[\"ABSENT\"], test_50_absence[\"ABSENT\"]),\n  PRESENT = c(train_90_absence[\"PRESENT\"], test_90_absence[\"PRESENT\"],\n              train_70_absence[\"PRESENT\"], test_70_absence[\"PRESENT\"],\n              train_50_absence[\"PRESENT\"], test_50_absence[\"PRESENT\"])\n)\n\n# Display the table\nprint(split_data)\n\n              Split ABSENT PRESENT\n1 90% Absence Train     14     660\n2  90% Absence Test    122     122\n3 70% Absence Train     41     687\n4  70% Absence Test     95      95\n5 50% Absence Train     68     714\n6  50% Absence Test     68      68\n\n\nWe can visualise above table in a bar chart.\n\n# Reshape data for ggplot using melt (or use pivot_longer)\nsplit_data_melted &lt;- melt(split_data, id.vars = \"Split\", variable.name = \"Status\", value.name = \"Count\")\n\n# Create the bar plot with labels\nggplot(split_data_melted, aes(x = Split, y = Count, fill = Status)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +  # Adjust bar positions\n  geom_text(aes(label = Count), \n            position = position_dodge(width = 0.9), \n            vjust = -0.5, size = 3) +  # Add text labels\n  labs(title = \"ABSENT and PRESENT Counts Across Data Splits\", x = \"Data Split\", y = \"Count\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"ABSENT\" = \"#f6aa70\", \"PRESENT\" = \"#11aa96\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nNow, we combine the training and test data with environmental dataset.\n\nget_occurrence_data &lt;- function(occurrence_sf, env_var_stack, drop_na = TRUE, dropCols = NULL) {\n  # Convert the occurrence sf object to a terra SpatVector\n  occ_vect &lt;- vect(occurrence_sf)\n  \n  # Extract raster values at the occurrence points\n  extracted_values &lt;- terra::extract(env_var_stack, occ_vect)\n  \n  # Combine occurrence data (as a data frame) with the extracted raster values\n  occ_data &lt;- cbind(as.data.frame(occurrence_sf), extracted_values)\n  \n  # Optionally remove rows with any NA values\n  if (drop_na) {\n    occ_data &lt;- na.omit(occ_data)\n  }\n  \n  # Optionally drop specified columns\n  if (!is.null(dropCols)) {\n    occ_data &lt;- occ_data[, -dropCols]\n  }\n  \n  return(occ_data)\n}\n\n\ncombine_with_env_data &lt;- function(split_list, env_var_stack, dropCols = c(2,3)) {\n  # Process the training data\n  train_data_env &lt;- get_occurrence_data(split_list$train_data, env_var_stack, dropCols = dropCols)\n  # Process the test data\n  test_data_env  &lt;- get_occurrence_data(split_list$test_data, env_var_stack, dropCols = dropCols)\n  \n  return(list(train_data_env = train_data_env, test_data_env = test_data_env))\n}\n\n\n# Then, combine each split with the environmental variables:\nenv_combined_90 &lt;- combine_with_env_data(split_90_absence, env_var_stack, dropCols = c(2,3))\n\nenv_combined_70 &lt;- combine_with_env_data(split_70_absence, env_var_stack, dropCols = c(2,3))\n\nenv_combined_50 &lt;- combine_with_env_data(split_50_absence, env_var_stack, dropCols = c(2,3))\n\n# Create a list of three training data data frames\ntrain_list &lt;- list(\n  \"split_90_train\" = env_combined_90$train_data_env,\n  \"split_70_train\" = env_combined_70$train_data_env,\n  \"split_50_train\" = env_combined_50$train_data_env\n)\n\n# Create a list of three training data data frames\ntest_list &lt;- list(\n  \"split_90_test\" = env_combined_90$test_data_env,\n  \"split_70_test\" = env_combined_70$test_data_env,\n  \"split_50_test\" = env_combined_50$test_data_env\n)\n\nNow, we have three sets of training and test data ready for modelling.\n\nsplit_90_train & split_90_test\nsplit_70_train & split_70_test\nsplit_50_train & split_50_test"
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#un-weighted-models",
    "href": "notebooks/data_prep/imbalanced_data.html#un-weighted-models",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "3.2 Un-weighted models",
    "text": "3.2 Un-weighted models\nIn this section, we will ignore the issue of dataset imbalance and run a GLM SDM model. We will generate all the evaluation metrics mentioned in section 1.4 Evaluation metric.\nThis is a function running GLM and producing evaluation metrics.\n\nfit_model_and_get_perf_no_wt &lt;- function(train_data_env, test_data_env, tss_step = 0.01) {\n  \n  # Load required libraries\n  library(pROC)     # For AUROC\n  library(PRROC)    # For AUPR\n  \n  # 1) Copy data locally\n  train_data &lt;- train_data_env\n  test_data  &lt;- test_data_env\n  \n  # Convert occrrnS to 0/1 numeric\n  train_data$occrrnS &lt;- ifelse(train_data$occrrnS == \"PRESENT\", 1, 0)\n  test_data$occrrnS  &lt;- ifelse(test_data$occrrnS == \"PRESENT\", 1, 0)\n  \n  # Define model formula\n  my_formula &lt;- occrrnS ~ `AusClim_bioclim_03_9s_1976-2005` + \n    `AusClim_bioclim_06_9s_1976-2005` + \n    `AusClim_bioclim_09_9s_1976-2005` + \n    `AusClim_bioclim_15_9s_1976-2005` + \n    `AusClim_bioclim_24_9s_1976-2005` + \n    `AusClim_bioclim_27_9s_1976-2005` + \n    `AusClim_bioclim_29_9s_1976-2005`\n  \n  # 2) Fit the unweighted GLM model\n  model &lt;- glm(my_formula, data = train_data, family = binomial(link = \"logit\"))\n  \n  # Predict probabilities on the test data\n  predicted_probs &lt;- predict(model, newdata = test_data, type = \"response\")\n  \n  # -----------------------------\n  # PART A: Standard metrics at threshold = 0.5\n  # -----------------------------\n  pred_class &lt;- ifelse(predicted_probs &gt;= 0.5, 1, 0)\n  \n  # Force table to have both '0' and '1' as levels\n  pred_class_factor &lt;- factor(pred_class, levels = c(0,1))\n  actual_factor     &lt;- factor(test_data$occrrnS, levels = c(0,1))\n  \n  conf_mat &lt;- table(Predicted = pred_class_factor, Actual = actual_factor)\n  \n  # Safely extract entries\n  TP &lt;- conf_mat[\"1\",\"1\"]\n  FP &lt;- conf_mat[\"1\",\"0\"]\n  FN &lt;- conf_mat[\"0\",\"1\"]\n  TN &lt;- conf_mat[\"0\",\"0\"]\n  \n  accuracy    &lt;- (TP + TN) / sum(conf_mat)\n  sensitivity &lt;- TP / (TP + FN)  # Recall\n  specificity &lt;- TN / (TN + FP)\n  precision   &lt;- TP / (TP + FP)\n  f1          &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  \n  # -----------------------------\n  # PART B: Additional Metrics\n  # -----------------------------\n  \n  # AUROC\n  roc_curve &lt;- pROC::roc(response = test_data$occrrnS, predictor = predicted_probs)\n  auc_value &lt;- pROC::auc(roc_curve)\n  \n  # AUPR (Area Under Precision-Recall Curve)\n  pr &lt;- pr.curve(scores.class0 = predicted_probs[test_data$occrrnS == 1],\n                 scores.class1 = predicted_probs[test_data$occrrnS == 0],\n                 curve = TRUE)\n  aupr_value &lt;- pr$auc.integral\n  \n  # Correlation between observed and predicted\n  corr_value &lt;- cor(test_data$occrrnS, predicted_probs, method = \"pearson\")\n  \n  # -----------------------------\n  # PART C: Sweep thresholds to find max TSS\n  # -----------------------------\n  \n  # Helper function to safely compute TSS elements\n  compute_tss &lt;- function(obs, preds, threshold) {\n    pred_bin &lt;- ifelse(preds &gt;= threshold, 1, 0)\n    TP &lt;- sum(pred_bin == 1 & obs == 1)\n    TN &lt;- sum(pred_bin == 0 & obs == 0)\n    FP &lt;- sum(pred_bin == 1 & obs == 0)\n    FN &lt;- sum(pred_bin == 0 & obs == 1)\n    \n    sens &lt;- if ((TP + FN) &gt; 0) TP / (TP + FN) else NA\n    spec &lt;- if ((TN + FP) &gt; 0) TN / (TN + FP) else NA\n    tss_val &lt;- sens + spec - 1\n    return(list(sensitivity = sens, specificity = spec, TSS = tss_val))\n  }\n  \n  thresholds &lt;- seq(0, 1, by = tss_step)\n  tss_df &lt;- data.frame(\n    threshold   = thresholds,\n    sensitivity = NA,\n    specificity = NA,\n    TSS         = NA\n  )\n  \n  for (i in seq_along(thresholds)) {\n    res_tss &lt;- compute_tss(test_data$occrrnS, predicted_probs, thresholds[i])\n    tss_df$sensitivity[i] &lt;- res_tss$sensitivity\n    tss_df$specificity[i] &lt;- res_tss$specificity\n    tss_df$TSS[i]         &lt;- res_tss$TSS\n  }\n  \n  # Identify the threshold that yields the maximum TSS\n  max_idx &lt;- which.max(tss_df$TSS)\n  best_threshold &lt;- tss_df$threshold[max_idx]\n  best_tss       &lt;- tss_df$TSS[max_idx]\n  best_sens      &lt;- tss_df$sensitivity[max_idx]\n  best_spec      &lt;- tss_df$specificity[max_idx]\n  \n  # -----------------------------\n  # PART D: Return all metrics\n  # -----------------------------\n  return(list(\n    # The fitted model + raw predictions\n    model             = model,\n    predicted_probs   = predicted_probs,\n    roc_obj           = roc_curve,\n    auc               = as.numeric(auc_value),\n    aupr              = as.numeric(aupr_value),\n    corr              = corr_value,\n    confusion_matrix_0.5  = conf_mat,\n    metrics_0.5 = data.frame(\n      Accuracy     = accuracy,\n      Sensitivity  = sensitivity,  # Recall\n      Specificity  = specificity,\n      Precision    = precision,\n      F1           = f1\n    ),\n    # TSS sweep results\n    tss_table      = tss_df,\n    best_threshold = best_threshold,\n    max_tss        = best_tss,\n    best_sensitivity = best_sens,\n    best_specificity = best_spec\n  ))\n}\n\nRun models with our three pairs of data and extract the evaluation metrics of these models.\n\nSplit_90_absence_unwt: split_90_train & split_90_test\nSplit_70_absence_unwt: split_70_train & split_70_test\nSplit_50_absence_unwt: split_50_train & split_50_test\n\n‘unwt’ means unweighted.\n\n# Apply the fit_model_and_get_perf_no_wt function to train and test lists\nmodel_including_true_absence_no_wt &lt;- mapply(fit_model_and_get_perf_no_wt, \n                   train_data_env = train_list, \n                   test_data_env = test_list,\n                   SIMPLIFY = FALSE)\n\n# Build a data frame by extracting each metric from the results\nresults_including_true_absence_table_no_wt &lt;- data.frame(\n  AUC              = sapply(model_including_true_absence_no_wt, function(x) x$auc),\n  AUPR             = sapply(model_including_true_absence_no_wt, function(x) x$aupr),\n  CORR             = sapply(model_including_true_absence_no_wt, function(x) x$corr),\n  Accuracy         = sapply(model_including_true_absence_no_wt, function(x) x$metrics_0.5$Accuracy),\n  Sensitivity      = sapply(model_including_true_absence_no_wt, function(x) x$metrics_0.5$Sensitivity),  # Recall\n  Specificity      = sapply(model_including_true_absence_no_wt, function(x) x$metrics_0.5$Specificity),\n  Precision        = sapply(model_including_true_absence_no_wt, function(x) x$metrics_0.5$Precision),\n  F1               = sapply(model_including_true_absence_no_wt, function(x) x$metrics_0.5$F1),\n  Best_Threshold   = sapply(model_including_true_absence_no_wt, function(x) x$best_threshold),\n  Max_TSS          = sapply(model_including_true_absence_no_wt, function(x) x$max_tss),\n  Best_Sensitivity = sapply(model_including_true_absence_no_wt, function(x) x$best_sensitivity),\n  Best_Specificity = sapply(model_including_true_absence_no_wt, function(x) x$best_specificity)\n)\n\nWe can print the evaluation metrics of all three models in one table.\n\n# Round numeric values to 3 decimal places\nresults_including_true_absence_table_no_wt &lt;- results_including_true_absence_table_no_wt %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 3)))\n\n# Create the table with smaller font\nresults_including_true_absence_table_no_wt %&gt;%\n  kable(\"html\", caption = \"Model Evaluation Metrics\") %&gt;%\n  kable_styling(font_size = 12, full_width = FALSE)\n\n\nModel Evaluation Metrics\n\n\n\nAUC\nAUPR\nCORR\nAccuracy\nSensitivity\nSpecificity\nPrecision\nF1\nBest_Threshold\nMax_TSS\nBest_Sensitivity\nBest_Specificity\n\n\n\n\nsplit_90_train\n0.661\n0.645\n0.143\n0.5\n1\n0\n0.5\n0.667\n0.97\n0.172\n0.852\n0.320\n\n\nsplit_70_train\n0.695\n0.688\n0.325\n0.5\n1\n0\n0.5\n0.667\n0.94\n0.284\n0.716\n0.568\n\n\nsplit_50_train\n0.664\n0.643\n0.317\n0.5\n1\n0\n0.5\n0.667\n0.89\n0.309\n0.882\n0.426\n\n\n\n\n\n\n\n\n3.2.1 Which model is better?\nsplit_70_train shows better overall performance based on AUC, AUPR, CORR, and Best_Specificity.\nsplit_50_train leads in Max_TSS and Best_Sensitivity, indicating it may perform slightly better in balancing true positive and true negative rates.\nsplit_90_train consistently underperforms in most metrics.\n\n\n3.2.2 Is more always better?\nNot necessarily — more true-absence data can help, but only up to a point. The results show that increasing the training data (from 50% to 90%) does not consistently improve the model’s performance. Here’s why:\nPerformance Improvement with More Absences:\n\nAs we increased the proportion of true-absence data from 10% (split_90) to 50% (split_50), there was a clear improvement in several metrics:\nAUC rose from 0.661 (split_90) to 0.695 (split_70), showing better discriminatory power with more absences.\nAUPR improved similarly, reflecting better precision-recall balance.\nMax_TSS increased from 0.172 (split_90) to 0.309 (split_50), indicating a better trade-off between sensitivity and specificity.\n\nDiminishing Returns Beyond a Point:\nWhile moving from 10% to 30% absences (split_90 to split_70) resulted in noticeable improvements, the jump from 30% to 50% (split_70 to split_50) offered smaller gains in AUC and AUPR, suggesting diminishing returns.\n\n\n3.2.3 How much true-absence data is enough?\n30% (split_70) seems to strike a good balance:\n\nIt achieved the highest AUC (0.695) and AUPR (0.688).\nBest_Specificity (0.568) was highest in split_70, showing better negative prediction performance.\nMax_TSS (0.284) is respectable and close to the best value (0.309 in split_50).\n\n50% (split_50) provides the highest TSS (0.309) and Best_Sensitivity (0.882) but doesn’t outperform split_70 on AUC and AUPR. This suggests that too much absence data may dilute the signal and introduce noise.\nPlease note that these are the primary results and comparisons of models without addressing the imbalanced data issue. Next, we will investigate whether applying weights improves the models and leads to different performance outcomes."
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#weighted-models",
    "href": "notebooks/data_prep/imbalanced_data.html#weighted-models",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "3.3 Weighted models",
    "text": "3.3 Weighted models\nAs explained in Section 1.2 Weighting methods,, we will use a weighting approach to assign more importance to the minority class. To implement this, we adjusted the model code by adding a ‘wt’ parameter and incorporated it into the GLM model.\n\nfit_model_and_get_perf_wt &lt;- function(train_data_env, test_data_env, tss_step = 0.01) {\n  \n  # Load required libraries\n  library(pROC)    # For AUROC\n  library(PRROC)   # For AUPR\n  \n  # 1) Copy data\n  train_data &lt;- train_data_env\n  test_data  &lt;- test_data_env\n  \n  # Convert occrrnS to 0/1 numeric\n  train_data$occrrnS &lt;- ifelse(train_data$occrrnS == \"PRESENT\", 1, 0)\n  test_data$occrrnS  &lt;- ifelse(test_data$occrrnS == \"PRESENT\", 1, 0)\n  \n  # 2) Calculate weights: presence gets 1; background gets (prNum / bgNum)\n  prNum &lt;- sum(train_data$occrrnS == 1)\n  bgNum &lt;- sum(train_data$occrrnS == 0)\n  wt &lt;- ifelse(train_data$occrrnS == 1, 1, prNum / bgNum)\n  \n  # 3) Fit the weighted GLM\n  my_formula &lt;- occrrnS ~ \n    `AusClim_bioclim_03_9s_1976-2005` + \n    `AusClim_bioclim_06_9s_1976-2005` + \n    `AusClim_bioclim_09_9s_1976-2005` + \n    `AusClim_bioclim_15_9s_1976-2005` + \n    `AusClim_bioclim_24_9s_1976-2005` + \n    `AusClim_bioclim_27_9s_1976-2005` + \n    `AusClim_bioclim_29_9s_1976-2005`\n  \n  model &lt;- glm(my_formula, data = train_data, weights = wt, family = binomial(link = \"logit\"))\n  \n  # Predict probabilities\n  predicted_probs &lt;- predict(model, newdata = test_data, type = \"response\")\n  \n  # -----------------------------\n  # PART A: Standard metrics at threshold = 0.5\n  # -----------------------------\n  pred_class &lt;- ifelse(predicted_probs &gt;= 0.5, 1, 0)\n  \n  # Force table to have both '0' and '1' levels\n  pred_class_factor &lt;- factor(pred_class, levels = c(0,1))\n  actual_factor     &lt;- factor(test_data$occrrnS, levels = c(0,1))\n  \n  conf_mat &lt;- table(Predicted = pred_class_factor, Actual = actual_factor)\n  \n  # Safely extract entries\n  TP &lt;- conf_mat[\"1\",\"1\"]\n  FP &lt;- conf_mat[\"1\",\"0\"]\n  FN &lt;- conf_mat[\"0\",\"1\"]\n  TN &lt;- conf_mat[\"0\",\"0\"]\n  \n  # Compute standard metrics\n  accuracy    &lt;- (TP + TN) / sum(conf_mat)\n  sensitivity &lt;- TP / (TP + FN)  # Recall\n  specificity &lt;- TN / (TN + FP)\n  precision   &lt;- TP / (TP + FP)\n  f1          &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  \n  # -----------------------------\n  # PART B: Additional Metrics\n  # -----------------------------\n  \n  # AUROC\n  roc_curve &lt;- pROC::roc(response = test_data$occrrnS, predictor = predicted_probs)\n  auc_value &lt;- pROC::auc(roc_curve)\n  \n  # AUPR (Area Under Precision-Recall Curve)\n  pr &lt;- pr.curve(scores.class0 = predicted_probs[test_data$occrrnS == 1],\n                 scores.class1 = predicted_probs[test_data$occrrnS == 0],\n                 curve = TRUE)\n  aupr_value &lt;- pr$auc.integral\n  \n  # Correlation between observed and predicted\n  corr_value &lt;- cor(test_data$occrrnS, predicted_probs, method = \"pearson\")\n  \n  # -----------------------------\n  # PART C: Sweep thresholds to find max TSS\n  # -----------------------------\n  compute_tss &lt;- function(obs, preds, threshold) {\n    pred_bin &lt;- ifelse(preds &gt;= threshold, 1, 0)\n    TP &lt;- sum(pred_bin == 1 & obs == 1)\n    TN &lt;- sum(pred_bin == 0 & obs == 0)\n    FP &lt;- sum(pred_bin == 1 & obs == 0)\n    FN &lt;- sum(pred_bin == 0 & obs == 1)\n    \n    sens &lt;- if ((TP + FN) &gt; 0) TP / (TP + FN) else NA\n    spec &lt;- if ((TN + FP) &gt; 0) TN / (TN + FP) else NA\n    tss_val &lt;- sens + spec - 1\n    return(list(sensitivity = sens, specificity = spec, TSS = tss_val))\n  }\n  \n  thresholds &lt;- seq(0, 1, by = tss_step)\n  tss_df &lt;- data.frame(\n    threshold   = thresholds,\n    sensitivity = NA,\n    specificity = NA,\n    TSS         = NA\n  )\n  \n  for (i in seq_along(thresholds)) {\n    out_tss &lt;- compute_tss(test_data$occrrnS, predicted_probs, thresholds[i])\n    tss_df$sensitivity[i] &lt;- out_tss$sensitivity\n    tss_df$specificity[i] &lt;- out_tss$specificity\n    tss_df$TSS[i]         &lt;- out_tss$TSS\n  }\n  \n  # Identify threshold that yields the maximum TSS\n  max_idx &lt;- which.max(tss_df$TSS)\n  best_threshold &lt;- tss_df$threshold[max_idx]\n  best_tss       &lt;- tss_df$TSS[max_idx]\n  best_sens      &lt;- tss_df$sensitivity[max_idx]\n  best_spec      &lt;- tss_df$specificity[max_idx]\n  \n  # -----------------------------\n  # PART D: Return all metrics\n  # -----------------------------\n  return(list(\n    # Model and predictions\n    model              = model,\n    predicted_probs    = predicted_probs,\n    roc_obj            = roc_curve,\n    \n    # Evaluation Metrics\n    auc                = as.numeric(auc_value),   # AUROC\n    aupr               = as.numeric(aupr_value),  # AUPR\n    corr               = corr_value,              # Correlation\n    \n    # Confusion Matrix at threshold 0.5\n    confusion_matrix_0.5 = conf_mat,\n    \n    # Metrics at threshold 0.5\n    metrics_0.5 = data.frame(\n      Accuracy     = accuracy,\n      Sensitivity  = sensitivity,  # Recall\n      Specificity  = specificity,\n      Precision    = precision,\n      F1           = f1\n    ),\n    \n    # TSS sweep results\n    tss_table      = tss_df,\n    best_threshold = best_threshold,\n    max_tss        = best_tss,\n    best_sensitivity = best_sens,\n    best_specificity = best_spec\n  ))\n}\n\nRun models with our three pairs of data and extract the evaluation metrics of these models.\n\nSplit_90_absence_wt: split_90_train & split_90_test\nSplit_70_absence_wt: split_70_train & split_70_test\nSplit_50_absence_wt: split_50_train & split_50_test\n\n‘wt’ means weighted.\n\n# 1) Fit weighted models for each train/test pair\nmodel_including_true_absence_wt &lt;- mapply(\n  fit_model_and_get_perf_wt,\n  train_data_env = train_list,\n  test_data_env  = test_list,\n  SIMPLIFY       = FALSE\n)\n\n# 2) Build a data frame by extracting each metric from the results\n#    Note: 'metrics_0.5' is where threshold=0.5 metrics live.\nresults_including_true_absence_table_wt &lt;- data.frame(\n  AUC              = sapply(model_including_true_absence_wt, function(x) x$auc),                   # AUROC\n  AUPR             = sapply(model_including_true_absence_wt, function(x) x$aupr),                  # AUPR\n  CORR             = sapply(model_including_true_absence_wt, function(x) x$corr),                  # Correlation\n  Accuracy         = sapply(model_including_true_absence_wt, function(x) x$`metrics_0.5`$Accuracy),\n  Sensitivity      = sapply(model_including_true_absence_wt, function(x) x$`metrics_0.5`$Sensitivity),  # Recall\n  Specificity      = sapply(model_including_true_absence_wt, function(x) x$`metrics_0.5`$Specificity),\n  Precision        = sapply(model_including_true_absence_wt, function(x) x$`metrics_0.5`$Precision),\n  F1               = sapply(model_including_true_absence_wt, function(x) x$`metrics_0.5`$F1),\n  Best_Threshold   = sapply(model_including_true_absence_wt, function(x) x$best_threshold),\n  Max_TSS          = sapply(model_including_true_absence_wt, function(x) x$max_tss),\n  Best_Sensitivity = sapply(model_including_true_absence_wt, function(x) x$best_sensitivity),\n  Best_Specificity = sapply(model_including_true_absence_wt, function(x) x$best_specificity)\n)\n\nWe can print the evaluation metrics of all three models in one table.\n\n# Round numeric values to 3 decimal places\nresults_including_true_absence_table_wt &lt;- results_including_true_absence_table_wt %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 3)))\n\n# Create the table with smaller font\nresults_including_true_absence_table_wt %&gt;%\n  kable(\"html\", caption = \"Model Evaluation Metrics\") %&gt;%\n  kable_styling(font_size = 12, full_width = FALSE)\n\n\nModel Evaluation Metrics\n\n\n\nAUC\nAUPR\nCORR\nAccuracy\nSensitivity\nSpecificity\nPrecision\nF1\nBest_Threshold\nMax_TSS\nBest_Sensitivity\nBest_Specificity\n\n\n\n\nsplit_90_train\n0.679\n0.678\n0.281\n0.607\n0.779\n0.434\n0.579\n0.664\n0.65\n0.295\n0.557\n0.738\n\n\nsplit_70_train\n0.697\n0.690\n0.336\n0.647\n0.674\n0.621\n0.640\n0.656\n0.49\n0.295\n0.705\n0.589\n\n\nsplit_50_train\n0.670\n0.647\n0.297\n0.588\n0.588\n0.588\n0.588\n0.588\n0.45\n0.324\n0.838\n0.485\n\n\n\n\n\n\n\n\n3.3.1 General Trends Across Weighted Models\nBest Overall Performer:\n\nsplit_70_train consistently shows the strongest performance across most metrics, striking a balance between sensitivity and specificity while achieving the highest AUC (0.697) and AUPR (0.690).\n\nEffect of Increasing True-Absence Data:\nIncreasing the proportion of true-absence data from 10% (split_90) to 50% (split_50) shows mixed effects:\n\nDiscrimination metrics (AUC, AUPR) improve from split_90 to split_70 but decline slightly in split_50.\nSpecificity improves as more absence data is included, while sensitivity tends to decrease."
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#comparing-un-weighted-and-weighted-methods.",
    "href": "notebooks/data_prep/imbalanced_data.html#comparing-un-weighted-and-weighted-methods.",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "3.4 Comparing un-weighted and weighted methods.",
    "text": "3.4 Comparing un-weighted and weighted methods.\nWe combine the performance tables of un-weighted and weighted models to see the differences between these two methods.\n\n#combine two evaluation tables\ncombined_df &lt;- bind_rows(\n  results_including_true_absence_table_no_wt %&gt;%\n    mutate(Source = \"No_WT\"),\n  results_including_true_absence_table_wt %&gt;%\n    mutate(Source = \"WT\")\n) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 3)))\n\n# Create the table using row names\nkable(combined_df, \"html\", caption = \"Model Evaluation Metrics\", rownames = TRUE) %&gt;%\n  kable_styling(font_size = 12, full_width = FALSE)\n\n\nModel Evaluation Metrics\n\n\n\nAUC\nAUPR\nCORR\nAccuracy\nSensitivity\nSpecificity\nPrecision\nF1\nBest_Threshold\nMax_TSS\nBest_Sensitivity\nBest_Specificity\nSource\n\n\n\n\nsplit_90_train...1\n0.661\n0.645\n0.143\n0.500\n1.000\n0.000\n0.500\n0.667\n0.97\n0.172\n0.852\n0.320\nNo_WT\n\n\nsplit_70_train...2\n0.695\n0.688\n0.325\n0.500\n1.000\n0.000\n0.500\n0.667\n0.94\n0.284\n0.716\n0.568\nNo_WT\n\n\nsplit_50_train...3\n0.664\n0.643\n0.317\n0.500\n1.000\n0.000\n0.500\n0.667\n0.89\n0.309\n0.882\n0.426\nNo_WT\n\n\nsplit_90_train...4\n0.679\n0.678\n0.281\n0.607\n0.779\n0.434\n0.579\n0.664\n0.65\n0.295\n0.557\n0.738\nWT\n\n\nsplit_70_train...5\n0.697\n0.690\n0.336\n0.647\n0.674\n0.621\n0.640\n0.656\n0.49\n0.295\n0.705\n0.589\nWT\n\n\nsplit_50_train...6\n0.670\n0.647\n0.297\n0.588\n0.588\n0.588\n0.588\n0.588\n0.45\n0.324\n0.838\n0.485\nWT\n\n\n\n\n\n\n\n\n3.4.1 Results of comparison\nBy comparing the performance of unweighted and weighted models, we find:\n(1) The weighted models (WT) significantly outperform the unweighted models (No_WT) in terms of balanced classification, offering better specificity, F1 scores, and TSS — all while maintaining similar AUC and AUPR.\n(2) Weighted models improved discrimination (AUC, AUPR) and provided more balanced classification by reducing the bias toward positive predictions.\n(3) Specificity improved significantly, correcting the unweighted models’ tendency to overpredict presences (Sensitivity = 1 in unweighted models).\n(4) split_70_train (WT) provided the most balanced results:\n\nHighest AUC (0.697) and AUPR (0.690)\nImproved Accuracy (0.647) and TSS (0.295)\nBalanced Sensitivity (0.674) and Specificity (0.621)\n\n(5) Weighting corrected imbalanced data issues:\n\nReduced false positives and improved the model’s ability to correctly classify absences.\nOptimal thresholds shifted lower, indicating improved model calibration.\n\n\n\n\n\n\n\n\n\nMetric\nUnweighted\nWeighted\n\n\n\n\nSensitivity\nVery high (1.0) but at the expense of specificity.\nBalanced (~0.58 - 0.77).\n\n\nSpecificity\nExtremely poor (0.0).\nImproved (~0.43 - 0.62).\n\n\nAUC / AUPR\nModerate (0.66 - 0.69).\nSimilar or slightly better (0.67 - 0.70).\n\n\nF1 Score\nInflated due to perfect recall but low precision.\nMore balanced (~0.59 - 0.66).\n\n\nTSS\nLow (~0.17 - 0.31), poor model skill.\nHigher (~0.29 - 0.32), indicating improved skill.\n\n\n\n\n\n3.4.2 Summary\nWeighting improved overall model performance, especially in addressing the imbalance between presences and absences.\nThe split_70_train (WT) appears to be the most balanced and effective model.\nWhile sensitivity decreased, the gains in specificity, AUC, and TSS suggest more reliable and generalizable \nWe should always consider weighting methods when the species binary dataset has an imbalance issue."
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#random-background-points",
    "href": "notebooks/data_prep/imbalanced_data.html#random-background-points",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "4.1 Random background points",
    "text": "4.1 Random background points\nMany studies suggest that the number of background points should be sufficient to comprehensively sample and represent all environments in the study area, or at least a range of background point sizes should be tested. For example, Valavi et al. (Valavi et al., 2022) tested their models with 100 to 100,000 background points. Barbet-Massin et al. (Barbet-Massin et al., 2012) tested 100, 300, 1000, 3000 or 10000 background points.\nWe here will also test the effect of different sizes of background points on the performance of models. We will test the ratio of background points to true presences from 1:1, 5:1, 10:1, 15:1, 20:1, 30:1, and 50:1 with and without weighting methods.\nThis is the function generating required number of random background points within the extent of the study area.\n\n# Convert SpatRaster to Raster\nstudyarea &lt;- raster(env_var_stack$`AusClim_bioclim_01_9s_1976-2005`)\n\n# Count the number of presence points in train_data (assuming presence is coded as 1)\nn_pres &lt;- nrow(train_data)\n\n# Set seed for reproducibility\nset.seed(1963)\n\n# Generate background points with a 1:1 ratio\nrandom_bg_1to1 &lt;- randomPoints(studyarea, n_pres)\n\n# Generate background points with a 5:1 ratio\nrandom_bg_5to1 &lt;- randomPoints(studyarea, n_pres * 5)\n\n# Generate background points with a 10:1 ratio\nrandom_bg_10to1 &lt;- randomPoints(studyarea, n_pres * 10)\n\n# Generate background points with a 15:1 ratio\nrandom_bg_15to1 &lt;- randomPoints(studyarea, n_pres * 15)\n\n# Generate background points with a 20:1 ratio\nrandom_bg_20to1 &lt;- randomPoints(studyarea, n_pres * 20)\n\n# Generate background points with a 30:1 ratio\nrandom_bg_30to1 &lt;- randomPoints(studyarea, n_pres * 30)\n\n# Generate background points with a 100:1 ratio\nrandom_bg_50to1 &lt;- randomPoints(studyarea, n_pres * 50)\n\n\ncombine_bg_with_presence &lt;- function(bg_points, train_data, raster_obj, target_crs = 4283, occ_label = \"bg\") {\n  # Convert the background points matrix to a data frame\n  bg_df &lt;- as.data.frame(bg_points)\n  \n  # Add the occrrnS column with the specified label (e.g., \"bg\")\n  bg_df$occrrnS &lt;- occ_label\n  \n  # Convert the data frame to an sf object using the raster's projection\n  bg_sf &lt;- st_as_sf(bg_df, coords = c(\"x\", \"y\"), crs = projection(raster_obj))\n  \n  # Transform the sf object to the target CRS (default is EPSG:4283)\n  bg_sf_target &lt;- st_transform(bg_sf, crs = target_crs)\n  \n  # Combine the original (presence) data with the background points\n  combined &lt;- rbind(train_data, bg_sf_target)\n  \n  return(combined)\n}\n\nWe then combine the generated background points with true present data.\n\ncombined_random_bg_1to1 &lt;- combine_bg_with_presence(random_bg_1to1, train_data, studyarea)\ncombined_random_bg_5to1 &lt;- combine_bg_with_presence(random_bg_5to1, train_data, studyarea)\ncombined_random_bg_10to1 &lt;- combine_bg_with_presence(random_bg_10to1, train_data, studyarea)\ncombined_random_bg_15to1 &lt;- combine_bg_with_presence(random_bg_15to1, train_data, studyarea)\ncombined_random_bg_20to1 &lt;- combine_bg_with_presence(random_bg_20to1, train_data, studyarea)\ncombined_random_bg_30to1 &lt;- combine_bg_with_presence(random_bg_30to1, train_data, studyarea)\ncombined_random_bg_50to1 &lt;- combine_bg_with_presence(random_bg_50to1, train_data, studyarea)\n\n\ncreate_map_plot &lt;- function(bg_sf, title, point_size = 0.5) {\n  # Ensure correct factor levels\n  bg_sf$occrrnS &lt;- factor(bg_sf$occrrnS, levels = c(\"PRESENT\", \"bg\"))\n  \n  # Split the data into background and present points\n  bg_only &lt;- bg_sf[bg_sf$occrrnS == \"bg\", ]\n  present_only &lt;- bg_sf[bg_sf$occrrnS == \"PRESENT\", ]\n  \n  ggplot() +\n    # Add the ACT polygon layer (without including it in the legend)\n    geom_sf(data = ACT, fill = \"lightblue\", color = \"black\", alpha = 0.3, show.legend = FALSE) +\n    # Plot background points first with mapped color\n    geom_sf(data = bg_only, aes(color = \"Background Points\"), size = point_size, show.legend = TRUE) +\n    # Plot present points on top with mapped color\n    geom_sf(data = present_only, aes(color = \"Present\"), size = point_size, show.legend = TRUE) +\n    # Manual color mapping\n    scale_color_manual(\n      name = \"Group\",\n      values = c(\"Present\" = \"#11aa96\", \"Background Points\" = \"grey\")\n    ) +\n    # Legend with larger points\n    guides(color = guide_legend(override.aes = list(size = 3))) +\n    ggtitle(title) +\n    coord_sf(lims_method = \"geometry_bbox\") +\n    theme_minimal() +\n    theme(\n      legend.position = \"right\",\n      plot.title = element_text(hjust = 0.5, size = 10, face = \"bold\"),\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank()\n    )\n}\n\n# Create plots for each background ratio\np1 &lt;- create_map_plot(combined_random_bg_1to1, \"1:1\", point_size = 0.01)\np2 &lt;- create_map_plot(combined_random_bg_5to1, \"5:1\", point_size = 0.01)\np3 &lt;- create_map_plot(combined_random_bg_10to1, \"10:1\", point_size = 0.01)\np4 &lt;- create_map_plot(combined_random_bg_15to1, \"15:1\", point_size = 0.01)\np5 &lt;- create_map_plot(combined_random_bg_20to1, \"20:1\", point_size = 0.01)\np6 &lt;- create_map_plot(combined_random_bg_30to1, \"30:1\", point_size = 0.01)\np7 &lt;- create_map_plot(combined_random_bg_50to1, \"50:1\", point_size = 0.01)\n\n# Combine the plots with a shared legend at the bottom\ncombined_plot &lt;- (p1 + p2 + p3 + p4 + p5 + p6 + p7) +\n  plot_layout(ncol = 4, guides = \"collect\") +  # Collects the legend into one\n  plot_annotation(theme = theme(legend.position = \"bottom\"))\n\ncombined_plot\n\n\n\n\n\n\n\n\nThen, we combine occurrence data with environmental variables.\n\ntrain_data_list &lt;- list(\n  combined_random_bg_1to1_env = get_occurrence_data(combined_random_bg_1to1, env_var_stack, dropCols = c(2,3)),\n  combined_random_bg_5to1_env = get_occurrence_data(combined_random_bg_1to1, env_var_stack, dropCols = c(2,3)),\n  combined_random_bg_10to1_env = get_occurrence_data(combined_random_bg_1to1, env_var_stack, dropCols = c(2,3)),\n  combined_random_bg_15to1_env = get_occurrence_data(combined_random_bg_10to1, env_var_stack, dropCols = c(2,3)),\n  combined_random_bg_20to1_env = get_occurrence_data(combined_random_bg_1to1, env_var_stack, dropCols = c(2,3)),\n  combined_random_bg_30to1_env = get_occurrence_data(combined_random_bg_1to1, env_var_stack, dropCols = c(2,3)),\n  combined_random_bg_50to1_env = get_occurrence_data(combined_random_bg_50to1, env_var_stack, dropCols = c(2,3))\n)\n\ntest_data_env &lt;- get_occurrence_data(test_data, env_var_stack, dropCols = c(2,3))\n\nWhen the ratio of background points to true presences exceeds 1:1—such as 5:1, 10:1, 15:1, 20:1, 30:1, and 50:1—an imbalanced dataset issue is introduced. To address this imbalance, we need to use weighting methods.\n\n# 1) Fit weighted models and extract results\nresults_list_wt &lt;- lapply(train_data_list, function(train_data) {\n  fit_model_and_get_perf_wt(train_data, test_data_env)\n})\n\n# 2) Build a data frame by extracting each metric from the results_list_wt\nresults_table_wt_random &lt;- data.frame(\n  AUC              = sapply(results_list_wt, function(x) x$auc),                    # AUROC\n  AUPR             = sapply(results_list_wt, function(x) x$aupr),                   # Area Under Precision-Recall Curve\n  CORR             = sapply(results_list_wt, function(x) x$corr),                   # Correlation\n  Accuracy         = sapply(results_list_wt, function(x) x$metrics_0.5$Accuracy),   # Accuracy at threshold 0.5\n  Sensitivity      = sapply(results_list_wt, function(x) x$metrics_0.5$Sensitivity),# Sensitivity (Recall)\n  Specificity      = sapply(results_list_wt, function(x) x$metrics_0.5$Specificity),# Specificity\n  Precision        = sapply(results_list_wt, function(x) x$metrics_0.5$Precision),  # Precision\n  F1               = sapply(results_list_wt, function(x) x$metrics_0.5$F1),         # F1 Score\n  Best_Threshold   = sapply(results_list_wt, function(x) x$best_threshold),         # Best threshold based on TSS\n  Max_TSS          = sapply(results_list_wt, function(x) x$max_tss),                # Maximum TSS\n  Best_Sensitivity = sapply(results_list_wt, function(x) x$best_sensitivity),       # Sensitivity at Best Threshold\n  Best_Specificity = sapply(results_list_wt, function(x) x$best_specificity)        # Specificity at Best Threshold\n)\n\nPrint the results.\n\n# Round numeric values to 3 decimal places\nresults_table_wt_random &lt;- results_table_wt_random %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 3)))\n\n# Create the table with smaller font\nresults_table_wt_random %&gt;%\n  kable(\"html\", caption = \"Model Evaluation Metrics\") %&gt;%\n  kable_styling(font_size = 12, full_width = FALSE)\n\n\nModel Evaluation Metrics\n\n\n\nAUC\nAUPR\nCORR\nAccuracy\nSensitivity\nSpecificity\nPrecision\nF1\nBest_Threshold\nMax_TSS\nBest_Sensitivity\nBest_Specificity\n\n\n\n\ncombined_random_bg_1to1_env\n0.580\n0.569\n0.116\n0.562\n0.713\n0.412\n0.548\n0.620\n0.61\n0.154\n0.456\n0.699\n\n\ncombined_random_bg_5to1_env\n0.580\n0.569\n0.116\n0.562\n0.713\n0.412\n0.548\n0.620\n0.61\n0.154\n0.456\n0.699\n\n\ncombined_random_bg_10to1_env\n0.580\n0.569\n0.116\n0.562\n0.713\n0.412\n0.548\n0.620\n0.61\n0.154\n0.456\n0.699\n\n\ncombined_random_bg_15to1_env\n0.568\n0.563\n0.090\n0.537\n0.669\n0.404\n0.529\n0.591\n0.65\n0.176\n0.324\n0.853\n\n\ncombined_random_bg_20to1_env\n0.580\n0.569\n0.116\n0.562\n0.713\n0.412\n0.548\n0.620\n0.61\n0.154\n0.456\n0.699\n\n\ncombined_random_bg_30to1_env\n0.580\n0.569\n0.116\n0.562\n0.713\n0.412\n0.548\n0.620\n0.61\n0.154\n0.456\n0.699\n\n\ncombined_random_bg_50to1_env\n0.571\n0.559\n0.097\n0.533\n0.662\n0.404\n0.526\n0.586\n0.64\n0.162\n0.331\n0.831\n\n\n\n\n\n\n\n\n4.1.1 Results\n\nBackground Ratios up to 10:1\n\n\nMetrics remain stable with minimal changes, indicating the model is robust to moderate increases in background points.\n\n(2) High Background Ratios (15:1 and 50:1)\n\nSensitivity and precision decline, leading to lower F1 scores.\nSpecificity improves, but at the cost of missing more true positives.\n\n(3) Overall Performance:\n\nIncreasing background points beyond 10:1 introduces diminishing returns and eventually harms model balance.\nA ratio of 5:1 to 10:1 appears to be a sweet spot, maintaining good sensitivity and specificity without introducing severe imbalances.\n\n\n\n4.1.2 Summary\n(1) Increasing background points beyond a 10:1 ratio does not consistently improve model performance and may even reduce sensitivity and overall balance.\n(2) Moderate background ratios (5:1 to 10:1) provide a balance between sufficient background sampling and maintaining model sensitivity.\n(3) At very high ratios (15:1 and 50:1), the model starts favoring background points, leading to poorer detection of presences."
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#the-disk-approach",
    "href": "notebooks/data_prep/imbalanced_data.html#the-disk-approach",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "4.2 The ‘disk’ approach",
    "text": "4.2 The ‘disk’ approach\nBesides completely random background points, we will also test a ‘disk’ approach to generate random background point methods within a defined range around present records. As the below figure shown, these rings indicate potential areas where pseudo-absence points can be sampled, simulating absence data when real absences are unavailable.\n\nFirst, we extract present data from the training dataset.\n\npresence_in_train_data &lt;- train_data[train_data$occrrnS == \"PRESENT\", ]\n\nThe function generating background points in a defined disk round the presence data.\n\n# 1) Generate random-buffer pseudo-absences within a study area\ngenerate_pseudo_absences_random_buffer &lt;- function(presence_in_train_data, \n                                                   study_area,\n                                                   min_buffer, max_buffer, \n                                                   n_points_per_presence, \n                                                   seed = 123) {\n  set.seed(seed)\n  \n  pseudo_list &lt;- lapply(seq_len(nrow(presence_in_train_data)), function(i) {\n    # Random distance for this presence point\n    random_buffer &lt;- runif(1, min = min_buffer, max = max_buffer)\n    \n    # Create the buffer polygon\n    buffer_poly &lt;- st_buffer(presence_in_train_data[i, ], dist = random_buffer)\n    \n    # Intersect buffer with study_area, so we only sample inside it\n    intersection_poly &lt;- st_intersection(buffer_poly, study_area)\n    \n    # If there's no overlap, return NULL\n    if (st_is_empty(intersection_poly)) {\n      return(NULL)\n    } else {\n      # Sample points inside the intersection\n      st_sample(intersection_poly, size = n_points_per_presence, type = \"random\")\n    }\n  })\n  \n  # Combine into an sfc\n  pseudo_sfc &lt;- do.call(c, pseudo_list)\n  \n  # If all overlaps were empty, pseudo_sfc is NULL\n  if (is.null(pseudo_sfc)) {\n    return(NULL)\n  }\n  \n  # Convert sfc to sf\n  pseudo_sf &lt;- st_sf(geometry = pseudo_sfc)\n  \n  # Label them as \"bg\"\n  pseudo_sf$occrrnS &lt;- \"bg\"\n  \n  # Keep only the occrrnS column\n  pseudo_sf &lt;- pseudo_sf[, \"occrrnS\", drop = FALSE]\n  return(pseudo_sf)\n}\n\n# 2) Combine pseudo-absences with original data\ngenerate_and_combine_pseudo_absences &lt;- function(presence_in_train_data, \n                                                 train_data,\n                                                 study_area,\n                                                 min_buffer, max_buffer, \n                                                 n_points_per_presence, \n                                                 seed = 123, \n                                                 target_crs = 4283) {\n  # Make sure everything is in the same CRS before buffering/intersecting\n  presence_in_train_data &lt;- st_transform(presence_in_train_data, target_crs)\n  study_area &lt;- st_transform(study_area, target_crs)\n  \n  # Generate pseudo-absence points\n  pseudo_sf &lt;- generate_pseudo_absences_random_buffer(\n    presence_in_train_data, \n    study_area,\n    min_buffer, max_buffer, \n    n_points_per_presence, \n    seed\n  )\n  \n  # Convert and combine\n  original_data &lt;- st_transform(train_data, target_crs)\n  \n  # If pseudo_sf is NULL (no overlap found), just return original\n  if (is.null(pseudo_sf)) {\n    message(\"No pseudo-absence points generated (buffers might lie outside the study area).\")\n    return(original_data)\n  }\n  \n  combined &lt;- rbind(original_data, pseudo_sf)\n  return(combined)\n}\n\nGenerate background points within these ranges around present records of Gang-gang Cockatoo in the training dataset:\n\nWithin 500m\n500 - 1000m\n1000 - 1500m\n1500 - 2000m\nMore than 2000m\n\n\nsuppressWarnings({ # We want to suppress some irrelevant warnings\n\n  disk_bg_500 &lt;- generate_and_combine_pseudo_absences(\n    presence_in_train_data,\n    train_data, \n    min_buffer = 0, \n    max_buffer = 500, \n    n_points_per_presence = 1,\n    seed = 123,\n    study_area = ACT,\n    target_crs = 4283\n  )\n\n  disk_bg_500_1000 &lt;- generate_and_combine_pseudo_absences(\n    presence_in_train_data,\n    train_data, \n    min_buffer = 500, \n    max_buffer = 1000, \n    n_points_per_presence = 1,\n    seed = 123,\n    study_area = ACT,\n    target_crs = 4283\n  )\n\n  disk_bg_1000_1500 &lt;- generate_and_combine_pseudo_absences(\n    presence_in_train_data,\n    train_data, \n    min_buffer = 1000, \n    max_buffer = 1500, \n    n_points_per_presence = 1,\n    seed = 123,\n    study_area = ACT,\n    target_crs = 4283\n  )\n\n  disk_bg_1500_2000 &lt;- generate_and_combine_pseudo_absences(\n    presence_in_train_data,\n    train_data, \n    min_buffer = 1500, \n    max_buffer = 2000,\n    n_points_per_presence = 1, \n    seed = 123, \n    study_area = ACT,\n    target_crs = 4283\n  )\n\n  disk_bg_2000 &lt;- generate_and_combine_pseudo_absences(\n    presence_in_train_data,\n    train_data, \n    min_buffer = 2000, \n    max_buffer = 10000,\n    n_points_per_presence = 1, \n    seed = 123, \n    study_area = ACT,\n    target_crs = 4283\n  )\n\n})\n\nLet’s plot the generated disk background points.\n\n# Create plots for each background ratio with smaller points\np1 &lt;- create_map_plot(disk_bg_500, \"Buffe &lt; 500m\", point_size = 0.01)\np2 &lt;- create_map_plot(disk_bg_500_1000, \"Buffer 500-1000m\", point_size = 0.01)\np3 &lt;- create_map_plot(disk_bg_1000_1500, \"Buffer 1000-1500m\", point_size = 0.01)\np4 &lt;- create_map_plot(disk_bg_1500_2000, \"Buffer 1500-2000m\", point_size = 0.01)\np5 &lt;- create_map_plot(disk_bg_2000, \"Buffer &gt; 2000m\", point_size = 0.01)\n\n# Combine the three plots in one row and collect a common legend at the bottom\ncombined_plot &lt;- (p1 + p2 + p3+ p4 + p5) +\n  plot_layout(ncol = 5, guides = \"collect\") +\n  plot_annotation(theme = theme(legend.position = \"bottom\"))\n\ncombined_plot\n\n\n\n\n\n\n\n\nFrom the above figure, we can see that as the range increases, the grey points (background points) move farther away from the present points.\nNow, let’s combine the occurrence data with environmental variables.\n\ntrain_data_list &lt;- list(\n  disk_bg_500_env = get_occurrence_data(disk_bg_500, env_var_stack, dropCols = c(2,3)),\n  disk_bg_500_1000_env = get_occurrence_data(disk_bg_500_1000, env_var_stack, dropCols = c(2,3)),\n  disk_bg_1000_1500_env = get_occurrence_data(disk_bg_1000_1500, env_var_stack, dropCols = c(2,3)),\n  disk_bg_1500_2000_env = get_occurrence_data(disk_bg_1500_2000, env_var_stack, dropCols = c(2,3)),\n  disk_bg_2000_env = get_occurrence_data(disk_bg_2000, env_var_stack, dropCols = c(2,3))\n)\n\ntest_data_env &lt;- get_occurrence_data(test_data, env_var_stack, dropCols = c(2,3))\n\nWe can now run the GLM models with these datasets and generate the evaluation metrics. Please note that we created an equal number of background points and present points, so there is no imbalance issue in this case. Thus, we are not using ‘wt’ in the GLM function.\n\nresults_list_no_wt &lt;- lapply(train_data_list, function(train_data) {\n  fit_model_and_get_perf_no_wt(train_data, test_data_env)\n})\n\n\n# 2) Build a data frame by extracting each metric from the results_list_wt\nresults_table_no_wt_disk &lt;- data.frame(\n  AUC              = sapply(results_list_no_wt, function(x) x$auc),                    # AUROC\n  AUPR             = sapply(results_list_no_wt, function(x) x$aupr),                   # Area Under Precision-Recall Curve\n  CORR             = sapply(results_list_no_wt, function(x) x$corr),                   # Correlation\n  Accuracy         = sapply(results_list_no_wt, function(x) x$metrics_0.5$Accuracy),   # Accuracy at threshold 0.5\n  Sensitivity      = sapply(results_list_no_wt, function(x) x$metrics_0.5$Sensitivity),# Sensitivity (Recall)\n  Specificity      = sapply(results_list_no_wt, function(x) x$metrics_0.5$Specificity),# Specificity\n  Precision        = sapply(results_list_no_wt, function(x) x$metrics_0.5$Precision),  # Precision\n  F1               = sapply(results_list_no_wt, function(x) x$metrics_0.5$F1),         # F1 Score\n  Best_Threshold   = sapply(results_list_no_wt, function(x) x$best_threshold),         # Best threshold based on TSS\n  Max_TSS          = sapply(results_list_no_wt, function(x) x$max_tss),                # Maximum TSS\n  Best_Sensitivity = sapply(results_list_no_wt, function(x) x$best_sensitivity),       # Sensitivity at Best Threshold\n  Best_Specificity = sapply(results_list_no_wt, function(x) x$best_specificity)        # Specificity at Best Threshold\n)\n\nPrint the results of models.\n\n# Round numeric values to 3 decimal places\nresults_table_no_wt_disk &lt;- results_table_no_wt_disk %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 3)))\n\n# Create the table with smaller font\nresults_table_no_wt_disk %&gt;%\n  kable(\"html\", caption = \"Model Evaluation Metrics\") %&gt;%\n  kable_styling(font_size = 12, full_width = FALSE)\n\n\nModel Evaluation Metrics\n\n\n\nAUC\nAUPR\nCORR\nAccuracy\nSensitivity\nSpecificity\nPrecision\nF1\nBest_Threshold\nMax_TSS\nBest_Sensitivity\nBest_Specificity\n\n\n\n\ndisk_bg_500_env\n0.536\n0.541\n0.083\n0.529\n0.471\n0.588\n0.533\n0.500\n0.50\n0.059\n0.471\n0.588\n\n\ndisk_bg_500_1000_env\n0.582\n0.542\n0.156\n0.574\n0.625\n0.522\n0.567\n0.594\n0.50\n0.147\n0.625\n0.522\n\n\ndisk_bg_1000_1500_env\n0.560\n0.526\n0.096\n0.540\n0.471\n0.610\n0.547\n0.506\n0.49\n0.088\n0.801\n0.287\n\n\ndisk_bg_1500_2000_env\n0.542\n0.495\n0.071\n0.507\n0.471\n0.544\n0.508\n0.489\n0.49\n0.184\n0.838\n0.346\n\n\ndisk_bg_2000_env\n0.538\n0.453\n-0.049\n0.478\n0.463\n0.493\n0.477\n0.470\n0.49\n0.044\n0.735\n0.309\n\n\n\n\n\n\n\n\n4.2.1 Results\n(1) Moderate Buffers Perform Best:\n\n500 - 1000m buffer had the best AUC (0.582) and F1 (0.594), indicating a good balance between ecological similarity and separation.\n\n(2) Too Close or Too Far Decreases Performance:\n\nWithin 500m: Pseudo-absences too close to presences make it hard for the model to distinguish the two.\nBeyond 2000m: Pseudo-absences too far may not represent relevant environmental contrasts, leading to poor predictions.\n\n(3) Sensitivity vs. Specificity Trade-off:\n\nAs background points move farther away, Specificity tends to improve, but Sensitivity decreases, leading to a trade-off between detecting presences and absences.\n\n(4) Correlation Decreases With Distance:\n\nThe CORR metric peaks around 500 - 1000m and then declines, even becoming negative beyond 2000m, indicating the model struggles with very distant background points.\n\n\n\n4.2.3 Summary\n(1) Optimal Buffer Distance:\n\nA moderate buffer (e.g., 500 - 1000m) seems to strike the best balance for pseudo-absence sampling in this context.\n\n(2) Avoid Extremes:\n\nSampling too close or too far from presence points degrades model performance."
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#comparison-across-all-models",
    "href": "notebooks/data_prep/imbalanced_data.html#comparison-across-all-models",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "5.1 Comparison across all models",
    "text": "5.1 Comparison across all models\nLet’s also compare the performance of all good models\n\n#Combine three tables\ncombined_results_table &lt;- bind_rows(results_including_true_absence_table_wt, results_table_no_wt_disk, results_table_wt_random)\n\n# Step 3: Display the combined table with kable\ncombined_results_table %&gt;%\n  kable(\"html\", caption = \"Combined Model Evaluation Metrics\") %&gt;%\n  kable_styling(font_size = 12, full_width = FALSE) %&gt;%\n  column_spec(ncol(combined_results_table), bold = TRUE)  # Highlight the Source column\n\n\nCombined Model Evaluation Metrics\n\n\n\nAUC\nAUPR\nCORR\nAccuracy\nSensitivity\nSpecificity\nPrecision\nF1\nBest_Threshold\nMax_TSS\nBest_Sensitivity\nBest_Specificity\n\n\n\n\nsplit_90_train\n0.679\n0.678\n0.281\n0.607\n0.779\n0.434\n0.579\n0.664\n0.65\n0.295\n0.557\n0.738\n\n\nsplit_70_train\n0.697\n0.690\n0.336\n0.647\n0.674\n0.621\n0.640\n0.656\n0.49\n0.295\n0.705\n0.589\n\n\nsplit_50_train\n0.670\n0.647\n0.297\n0.588\n0.588\n0.588\n0.588\n0.588\n0.45\n0.324\n0.838\n0.485\n\n\ndisk_bg_500_env\n0.536\n0.541\n0.083\n0.529\n0.471\n0.588\n0.533\n0.500\n0.50\n0.059\n0.471\n0.588\n\n\ndisk_bg_500_1000_env\n0.582\n0.542\n0.156\n0.574\n0.625\n0.522\n0.567\n0.594\n0.50\n0.147\n0.625\n0.522\n\n\ndisk_bg_1000_1500_env\n0.560\n0.526\n0.096\n0.540\n0.471\n0.610\n0.547\n0.506\n0.49\n0.088\n0.801\n0.287\n\n\ndisk_bg_1500_2000_env\n0.542\n0.495\n0.071\n0.507\n0.471\n0.544\n0.508\n0.489\n0.49\n0.184\n0.838\n0.346\n\n\ndisk_bg_2000_env\n0.538\n0.453\n-0.049\n0.478\n0.463\n0.493\n0.477\n0.470\n0.49\n0.044\n0.735\n0.309\n\n\ncombined_random_bg_1to1_env\n0.580\n0.569\n0.116\n0.562\n0.713\n0.412\n0.548\n0.620\n0.61\n0.154\n0.456\n0.699\n\n\ncombined_random_bg_5to1_env\n0.580\n0.569\n0.116\n0.562\n0.713\n0.412\n0.548\n0.620\n0.61\n0.154\n0.456\n0.699\n\n\ncombined_random_bg_10to1_env\n0.580\n0.569\n0.116\n0.562\n0.713\n0.412\n0.548\n0.620\n0.61\n0.154\n0.456\n0.699\n\n\ncombined_random_bg_15to1_env\n0.568\n0.563\n0.090\n0.537\n0.669\n0.404\n0.529\n0.591\n0.65\n0.176\n0.324\n0.853\n\n\ncombined_random_bg_20to1_env\n0.580\n0.569\n0.116\n0.562\n0.713\n0.412\n0.548\n0.620\n0.61\n0.154\n0.456\n0.699\n\n\ncombined_random_bg_30to1_env\n0.580\n0.569\n0.116\n0.562\n0.713\n0.412\n0.548\n0.620\n0.61\n0.154\n0.456\n0.699\n\n\ncombined_random_bg_50to1_env\n0.571\n0.559\n0.097\n0.533\n0.662\n0.404\n0.526\n0.586\n0.64\n0.162\n0.331\n0.831\n\n\n\n\n\n\n\n(1) Best Overall Performance:\n\nSplit Models show the highest AUC scores.\nsplit_70_train has the best AUC (0.697) and CORR (0.336), indicating strong predictive performance.\nsplit_90_train and split_50_train follow closely with AUCs of 0.679 and 0.670, respectively.\n\n(2) Disk Strategy Performance:\n\nBest AUC (0.582) achieved with 500–1000m buffer.\nCloser buffers (500m) result in weaker performance (AUC = 0.536), likely due to environmental similarity between presence and background points.\nFarther buffers (&gt;2000m) degrade performance (AUC = 0.538) due to irrelevant background sampling.\n\n(3) Random Strategy Performance:\n\nStable AUC (~0.580) across most background-to-presence ratios (1:1 to 30:1).\nSlight performance drop at 50:1 ratio (AUC = 0.571) suggests diminishing returns from adding more background points.\nSensitivity remains high (~0.713) across most ratios but at the expense of Specificity (~0.412).\n\n(4) Best Trade-offs (Max_TSS):\n\nsplit_50_train shows the highest Max_TSS (0.324), indicating the best balance between Sensitivity and Specificity.\nIn Random Strategy, 15:1 ratio yields the highest Max_TSS (0.176).\n\n(5) Key Insights:\n\nSplit models outperform both Disk and Random background strategies in terms of AUC and overall predictive ability.\nDisk strategy performs best at moderate buffer distances (500–1000m), balancing environmental similarity and separation.\nRandom strategy shows stable performance across most ratios, but excessive background points (50:1) can reduce effectiveness.5.2"
  },
  {
    "objectID": "notebooks/data_prep/imbalanced_data.html#summary-3",
    "href": "notebooks/data_prep/imbalanced_data.html#summary-3",
    "title": "Imbalanced species data, background points, and weighting methods",
    "section": "5.2 Summary",
    "text": "5.2 Summary\nAddressing imbalanced species data is crucial for producing reliable and accurate Species Distribution Models. Through this study, we demonstrated how resampling methods, background point generation techniques, and weighting strategies can significantly impact model performance. Weighted models and carefully selected background points help to balance the influence of presence and absence data, reducing bias and improving metrics like sensitivity, specificity, and AUROC. The Gang-gang Cockatoo case study highlighted the practical implications of these approaches, showing that even simple weighting methods can enhance model outputs. Ultimately, integrating these techniques into ecological modeling workflows supports better conservation planning and decision-making, ensuring more robust and interpretable SDMs within platforms like EcoCommons."
  }
]